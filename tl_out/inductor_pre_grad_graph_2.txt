class GraphModule(torch.nn.Module):
    def forward(self, cat_36: "f16[s0, 3890][3890, 1]cuda:0", cat_37: "f16[s0, 2688][2688, 1]cuda:0", tbe_lookup_nro_weighted_1: "f16[s0, 384][384, 1]cuda:0", tbe_lookup_nro_weighted_0: "f16[s0, 384][384, 1]cuda:0", tbe_lookup_nro_unweighted_0: "f16[s0, 17932][17932, 1]cuda:0", tbe_lookup_nro_unweighted_1: "f16[s0, 17584][17584, 1]cuda:0", tbe_lookup_ro_0: "f16[s0, 30516][30516, 1]cuda:0", pack_segments_1: "f16[s0, 200, 64][12800, 64, 1]cuda:0", pack_segments: "f16[s0, 200, 64][12800, 64, 1]cuda:0"):
        # No stacktrace found for following nodes
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 240][240, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 240][240, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 192][192, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 192][192, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 72][72, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 144][144, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 144][144, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 64][64, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[192, 96][96, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3.submodules, "1").scale
        submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_feature_arch.embedding_projection_arch.F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3.submodules, "1").bias
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[214, 6052][6052, 1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[214][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_w: "f16[42, 6052][6052, 1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").arch.submodules, "0").submodules, "0").shards, "1").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_b: "f16[42][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").arch.submodules, "0").submodules, "0").shards, "1").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale: "f16[256][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").arch.submodules, "0").submodules, "1").norm.submodules, "0").scale
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias: "f16[256][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").arch.submodules, "0").submodules, "1").norm.submodules, "0").bias
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_w: "f16[3026, 256][256, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "1").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b: "f16[3026][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "1").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w: "f16[256, 3026][3026, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.linear_archs, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b: "f16[256][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.linear_archs, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale: "f16[3282][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.activations, "0").norm.submodules, "0").scale
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias: "f16[3282][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_projection_arch.activations, "0").norm.submodules, "0").bias
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_w: "f16[192, 3282][3282, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_0.submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_0.submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_w: "f16[192, 3282][3282, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_1.submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_1.submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_w: "f16[192, 3282][3282, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_2.submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_2.submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_w: "f16[192, 3282][3282, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_3.submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_3.submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_w: "f16[192, 3282][3282, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_4.submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_4.submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_w: "f16[192, 3282][3282, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_5.submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_b: "f16[192][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.dense_arch.dense_embedding_archs, "0").submodules.dense_embedding_5.submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[256, 700][700, 1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[256][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale: "f16[256][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "0").submodules, "1").norm.submodules, "0").scale
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias: "f16[256][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "0").submodules, "1").norm.submodules, "0").bias
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_w: "f16[1024, 256][256, 1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "1").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b: "f16[1024][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "1").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale: "f16[1024][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "1").submodules, "1").norm.submodules, "0").scale
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias: "f16[1024][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch.submodules, "0").arch.submodules, "1").submodules, "1").norm.submodules, "0").bias
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_w: "f16[256, 700][700, 1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch_to_dot.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_b: "f16[256][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch_to_dot.submodules, "0").arch.submodules, "0").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_w: "f16[960, 256][256, 1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch_to_dot.submodules, "0").arch.submodules, "1").submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b: "f16[960][1]cuda:0" = getattr(getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch_to_dot.submodules, "0").arch.submodules, "1").submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_w: "f16[1536, 960][960, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch_to_dot.submodules, "1").linear_arch.shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.specialized_arch.specialized_module_list, "0").specialized_arch_to_dot.submodules, "1").linear_arch.shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb: "f16[1, 200, 64][12800, 64, 1]cuda:0" = self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.pos_emb
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb: "f16[1, 32, 64][2048, 64, 1]cuda:0" = self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.seed_emb
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.ln_y, "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.ln_y, "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight: "f16[64, 64][64, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.attns, "0").k_proj.weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight: "f16[64, 64][64, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.attns, "0").q_proj.weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.attns, "0").q_proj.bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.ln_x, "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.ln_x, "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.ln_ffn, "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.ln_ffn, "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_weight: "f16[128, 64][64, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.mlps, "0"), "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias: "f16[128][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.mlps, "0"), "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_weight: "f16[64, 128][128, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.mlps, "0"), "2").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias: "f16[64][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.iaw.relevance_model.pre_norm_pma.mab.mlps, "0"), "2").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb: "f16[1, 200, 64][12800, 64, 1]cuda:0" = self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.pos_emb
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb: "f16[1, 32, 64][2048, 64, 1]cuda:0" = self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.seed_emb
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.ln_y, "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.ln_y, "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight: "f16[64, 64][64, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.attns, "0").k_proj.weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight: "f16[64, 64][64, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.attns, "0").q_proj.weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.attns, "0").q_proj.bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.ln_x, "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.ln_x, "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.ln_ffn, "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias: "f16[64][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.ln_ffn, "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_weight: "f16[128, 64][64, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.mlps, "0"), "0").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias: "f16[128][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.mlps, "0"), "0").bias
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_weight: "f16[64, 128][128, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.mlps, "0"), "2").weight
        submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias: "f16[64][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.event_submodels_dict.user_conv_ads_event.relevance_model.pre_norm_pma.mab.mlps, "0"), "2").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias: "f16[64, 1, 192][192, 192, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_projection_arch.first_fused_mlp, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight: "f16[64, 64, 192][12288, 192, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.embedding_projection_arch.first_fused_mlp, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w: "f16[174, 458][458, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._fused_lce_module._compression_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b: "f16[174, 1][1, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._fused_lce_module._compression_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._fused_lce_module._ln_lce._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._fused_lce_module._ln_lce._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight: "f16[21984, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias: "f16[21984][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w: "f16[21984][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b: "f16[21984][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight: "f16[9216, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp.compressed_tensor_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcpp.compressed_tensor_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w: "f16[21984][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b: "f16[21984][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_weight: "f16[2048, 21984][21984, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_fc._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_fc._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._ln_on_dsi._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._ln_on_dsi._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight: "f16[384, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_weight: "f16[6354, 384][384, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_match_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_match_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcn_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._post_dcn_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_weight: "f16[3072, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "1").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "1").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "1").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "1").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "2").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "2").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "2").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "2").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "3").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "3").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "3").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "3").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "4").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "4").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "4").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._mlps, "4").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._residual_activation, "2").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._residual_activation, "2").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._residual_activation, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._residual_mlp._residual_activation, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_weight: "f16[9216, 3072][3072, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._snn_projection.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._snn_projection.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._ln_on_dhen_layer._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "0")._ln_on_dhen_layer._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w: "f16[72, 102][102, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._fused_lce_module._compression_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b: "f16[72, 1][1, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._fused_lce_module._compression_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._fused_lce_module._ln_lce._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._fused_lce_module._ln_lce._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight: "f16[4896, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight: "f16[9216, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp.compressed_tensor_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcpp.compressed_tensor_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w: "f16[4896][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b: "f16[4896][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_weight: "f16[2048, 4896][4896, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_fc._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_fc._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._ln_on_dsi._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._ln_on_dsi._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight: "f16[384, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_weight: "f16[6354, 384][384, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_match_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_match_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcn_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._post_dcn_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_weight: "f16[3072, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "1").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "1").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "1").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "1").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "2").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "2").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "2").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "2").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "3").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "3").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "3").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "3").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "4").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "4").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "4").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._mlps, "4").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._residual_activation, "2").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._residual_activation, "2").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._residual_activation, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._residual_mlp._residual_activation, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_weight: "f16[9216, 3072][3072, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._snn_projection.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._snn_projection.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._ln_on_dhen_layer._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "1")._ln_on_dhen_layer._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w: "f16[72, 102][102, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._fused_lce_module._compression_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b: "f16[72, 1][1, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._fused_lce_module._compression_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._fused_lce_module._ln_lce._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._fused_lce_module._ln_lce._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight: "f16[4896, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight: "f16[9216, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp.compressed_tensor_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcpp.compressed_tensor_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w: "f16[4896][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b: "f16[4896][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_weight: "f16[2048, 4896][4896, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_fc._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_fc._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._ln_on_dsi._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._ln_on_dsi._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight: "f16[384, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_weight: "f16[6354, 384][384, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_match_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_match_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcn_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._post_dcn_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_weight: "f16[3072, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "1").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "1").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "1").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "1").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "2").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "2").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "2").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "2").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "3").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "3").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "3").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "3").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "4").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "4").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "4").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._mlps, "4").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._residual_activation, "2").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._residual_activation, "2").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._residual_activation, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._residual_mlp._residual_activation, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_weight: "f16[9216, 3072][3072, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._snn_projection.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._snn_projection.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._ln_on_dhen_layer._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "2")._ln_on_dhen_layer._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w: "f16[24, 102][102, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._input_compression._compression_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b: "f16[24, 1][1, 1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._input_compression._compression_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._input_compression._ln_lce._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b: "f16[192][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._input_compression._ln_lce._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_weight: "f16[768, 4608][4608, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_weight: "f16[768, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "3").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "3").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias: "f16[768][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b: "f16[768][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_mlp.mlp_net, "5")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight: "f16[4896, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b: "f16[4896][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._weight_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight: "f16[9216, 768][768, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_post_match_mlp.mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b: "f16[9216][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp._resnet_arch_post_match_mlp.mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp.compressed_tensor_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b: "f16[48][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcpp.compressed_tensor_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w: "f16[4896][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b: "f16[4896][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_weight: "f16[2048, 4896][4896, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_fc._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_fc._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias: "f16[2048][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_fc._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b: "f16[2048][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcpp_fc._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._ln_on_dsi._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._ln_on_dsi._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight: "f16[384, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_low_rank_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b: "f16[384][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_low_rank_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_weight: "f16[6354, 384][384, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_match_mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_match_mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b: "f16[6354][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._dcn._dcn_match_mlps, "0").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcn_ln._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b: "f16[6354][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._post_dcn_ln._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_weight: "f16[3072, 6354][6354, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "0").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "0").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "0").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "0").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "0").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "1").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "1").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "1").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "1").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "1").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "2").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "2").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "2").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "2").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_weight: "f16[1536, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "3").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "3").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias: "f16[1536][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "3").mlp_net, "1").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "3").mlp_net, "2")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b: "f16[1536][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "3").mlp_net, "2")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_weight: "f16[3072, 1536][1536, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "4").mlp_net, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "4").mlp_net, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "4").mlp_net, "1")._init_w
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._mlps, "4").mlp_net, "1")._init_b
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._residual_activation, "2").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._residual_activation, "2").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._residual_activation, "4").norm, "0").weight
        submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias: "f16[3072][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.pytorch_dhen.dhen_arch.layers, "3")._residual_mlp._residual_activation, "4").norm, "0").bias
        submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_w: "f16[512, 3072][3072, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.encoder.linear_archs, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b: "f16[512][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.encoder.linear_archs, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_w: "f16[3072, 512][512, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.encoder.linear_archs, "1").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b: "f16[3072][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.encoder.linear_archs, "1").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_w: "f16[512, 3072][3072, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.decoder.linear_archs, "0").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b: "f16[512][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.decoder.linear_archs, "0").shards, "0").b
        submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_w: "f16[3072, 512][512, 1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.decoder.linear_archs, "1").shards, "0").w
        submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b: "f16[3072][1]cuda:0" = getattr(getattr(self.submod_0.main_module.impl.impl.shared_arch.cyclegan.decoder.linear_archs, "1").shards, "0").b
        submod_0_main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias: "f16[1][1]cuda:0" = getattr(self.submod_0.main_module.impl.impl.dependent_tasks, "1:SALR_STANDALONE").aggregator_module.task_arch.sparse_aggregates_logistic_regression.global_bias
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_w: "f16[3072, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").shards, "0").w
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b: "f16[3072][1]cuda:0" = getattr(getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.gating_archs, "0").gn_arch.submodules, "0").shards, "0").b
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w: "f16[512, 3072][3072, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.linear_archs, "0").shards, "0").w
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b: "f16[512][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.linear_archs, "0").shards, "0").b
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_w: "f16[1, 512][512, 1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.linear_archs, "1").shards, "0").w
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b: "f16[1][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.linear_archs, "1").shards, "0").b
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale: "f16[512][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.activations, "0").norm.submodules, "0").scale
        submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias: "f16[512][1]cuda:0" = getattr(getattr(getattr(self.submod_0.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.dense_arch.dense_projection_arch.activations, "0").norm.submodules, "0").bias
        _tensor_constant2: "f16[1][1]cuda:0" = self._tensor_constant2
        submod_0_cat_fusion_gpu__offset_dim_list: "i64[277][1]cuda:0" = self.submod_0.cat_fusion_gpu._offset_dim_list
        submod_0_cat_fusion_gpu__permute: "i64[276][1]cuda:0" = self.submod_0.cat_fusion_gpu._permute
        submod_0_cat_fusion_gpu__inv_permute: "i64[276][1]cuda:0" = self.submod_0.cat_fusion_gpu._inv_permute
        submod_0_cat_fusion_gpu__inv_offset_dim_list: "i64[277][1]cuda:0" = self.submod_0.cat_fusion_gpu._inv_offset_dim_list
        submod_0_cat_fusion_cpu__offset_dim_list: "i64[183][1]cuda:0" = self.submod_0.cat_fusion_cpu._offset_dim_list
        submod_0_cat_fusion_cpu__permute: "i64[182][1]cuda:0" = self.submod_0.cat_fusion_cpu._permute
        submod_0_cat_fusion_cpu__inv_permute: "i64[182][1]cuda:0" = self.submod_0.cat_fusion_cpu._inv_permute
        submod_0_cat_fusion_cpu__inv_offset_dim_list: "i64[183][1]cuda:0" = self.submod_0.cat_fusion_cpu._inv_offset_dim_list
        submod_1__tensor_constant1: "f16[][]cuda:0" = self.submod_1._tensor_constant1
        submod_1_main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_positive_weight_calibration_bias: "f16[1][1]cuda:0" = getattr(self.submod_1.main_module.impl.impl.task_archs, "1:Optimized").prediction_arch.calibration.positive_weight_calibration_bias
        
         # 
        sym_size_int_12: "Sym(s0)" = torch.ops.aten.sym_size.int(cat_36, 0)
        
         # File: <torch_package_1>.caffe2/torch/fb/predictor/modules/tensors_to_device_module.py:57 in forward, code: return [
        _to_copy: "f16[s0, 3890][3890, 1]cuda:0" = torch.ops.aten._to_copy.default(cat_36, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  cat_36 = None
        _to_copy_1: "f16[s0, 2688][2688, 1]cuda:0" = torch.ops.aten._to_copy.default(cat_37, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  cat_37 = None
        _to_copy_2: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten._to_copy.default(tbe_lookup_nro_weighted_1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  tbe_lookup_nro_weighted_1 = None
        _to_copy_3: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten._to_copy.default(tbe_lookup_nro_weighted_0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  tbe_lookup_nro_weighted_0 = None
        _to_copy_4: "f16[s0, 17932][17932, 1]cuda:0" = torch.ops.aten._to_copy.default(tbe_lookup_nro_unweighted_0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  tbe_lookup_nro_unweighted_0 = None
        _to_copy_5: "f16[s0, 17584][17584, 1]cuda:0" = torch.ops.aten._to_copy.default(tbe_lookup_nro_unweighted_1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  tbe_lookup_nro_unweighted_1 = None
        _to_copy_6: "f16[s0, 30516][30516, 1]cuda:0" = torch.ops.aten._to_copy.default(tbe_lookup_ro_0, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  tbe_lookup_ro_0 = None
        _to_copy_7: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten._to_copy.default(pack_segments_1, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  pack_segments_1 = None
        _to_copy_8: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten._to_copy.default(pack_segments, dtype = torch.float16, layout = torch.strided, device = device(type='cuda', index=0), non_blocking = True);  pack_segments = None
        
         # File: <torch_package_1>.caffe2/torch/fb/model_transform/serving_paradigm/split_concat_transform_structs.py:132 in forward, code: catted = torch.cat(unpadded_tensors, dim=1)
        cat: "f16[s0, 36284][36284, 1]cuda:0" = torch.ops.aten.cat.default([_to_copy_2, _to_copy_3, _to_copy_4, _to_copy_5], 1);  _to_copy_2 = _to_copy_3 = _to_copy_4 = _to_copy_5 = None
        
         # File: <torch_package_1>.caffe2/torch/fb/model_transform/serving_paradigm/split_concat_transform_structs.py:143 in forward, code: permuted = torch.ops.fbgemm.permute_pooled_embs_auto_grad(
        permute_pooled_embs_auto_grad: "f16[s0, 36284][36284, 1]cuda:0" = torch.ops.fbgemm.permute_pooled_embs_auto_grad.default(cat, submod_0_cat_fusion_gpu__offset_dim_list, submod_0_cat_fusion_gpu__permute, submod_0_cat_fusion_gpu__inv_offset_dim_list, submod_0_cat_fusion_gpu__inv_permute);  cat = submod_0_cat_fusion_gpu__offset_dim_list = submod_0_cat_fusion_gpu__permute = submod_0_cat_fusion_gpu__inv_offset_dim_list = submod_0_cat_fusion_gpu__inv_permute = None
        
         # File: <torch_package_1>.caffe2/torch/fb/model_transform/serving_paradigm/split_concat_transform_structs.py:155 in forward, code: res = permuted.split_with_sizes(self.res_dim_list, dim=1)
        split_with_sizes = torch.ops.aten.split_with_sizes.default(permute_pooled_embs_auto_grad, [36, 228, 32, 32, 80, 32, 16, 72, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2880, 3648, 768, 192, 1152, 960, 576, 192, 768, 384, 384, 1152, 768, 384, 192, 384, 192, 384, 384, 768, 1152, 576, 2880, 384, 192, 1920, 9792, 192, 576, 192, 384, 576, 192, 192], 1);  permute_pooled_embs_auto_grad = None
        getitem: "f16[s0, 36][36284, 1]cuda:0" = split_with_sizes[0]
        getitem_1: "f16[s0, 228][36284, 1]cuda:0" = split_with_sizes[1]
        getitem_2: "f16[s0, 32][36284, 1]cuda:0" = split_with_sizes[2]
        getitem_3: "f16[s0, 32][36284, 1]cuda:0" = split_with_sizes[3]
        getitem_4: "f16[s0, 80][36284, 1]cuda:0" = split_with_sizes[4]
        getitem_5: "f16[s0, 32][36284, 1]cuda:0" = split_with_sizes[5]
        getitem_6: "f16[s0, 16][36284, 1]cuda:0" = split_with_sizes[6]
        getitem_7: "f16[s0, 72][36284, 1]cuda:0" = split_with_sizes[7]
        getitem_8: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[8]
        getitem_9: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[9]
        getitem_10: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[10]
        getitem_11: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[11]
        getitem_12: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[12]
        getitem_13: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[13]
        getitem_14: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[14]
        getitem_15: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[15]
        getitem_16: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[16]
        getitem_17: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[17]
        getitem_18: "f16[s0, 4][36284, 1]cuda:0" = split_with_sizes[18]
        getitem_19: "f16[s0, 2880][36284, 1]cuda:0" = split_with_sizes[19]
        getitem_20: "f16[s0, 3648][36284, 1]cuda:0" = split_with_sizes[20]
        getitem_21: "f16[s0, 768][36284, 1]cuda:0" = split_with_sizes[21]
        getitem_22: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[22]
        getitem_23: "f16[s0, 1152][36284, 1]cuda:0" = split_with_sizes[23]
        getitem_24: "f16[s0, 960][36284, 1]cuda:0" = split_with_sizes[24]
        getitem_25: "f16[s0, 576][36284, 1]cuda:0" = split_with_sizes[25]
        getitem_26: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[26]
        getitem_27: "f16[s0, 768][36284, 1]cuda:0" = split_with_sizes[27]
        getitem_28: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[28]
        getitem_29: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[29]
        getitem_30: "f16[s0, 1152][36284, 1]cuda:0" = split_with_sizes[30]
        getitem_31: "f16[s0, 768][36284, 1]cuda:0" = split_with_sizes[31]
        getitem_32: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[32]
        getitem_33: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[33]
        getitem_34: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[34]
        getitem_35: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[35]
        getitem_36: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[36]
        getitem_37: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[37]
        getitem_38: "f16[s0, 768][36284, 1]cuda:0" = split_with_sizes[38]
        getitem_39: "f16[s0, 1152][36284, 1]cuda:0" = split_with_sizes[39]
        getitem_40: "f16[s0, 576][36284, 1]cuda:0" = split_with_sizes[40]
        getitem_41: "f16[s0, 2880][36284, 1]cuda:0" = split_with_sizes[41]
        getitem_42: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[42]
        getitem_43: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[43]
        getitem_44: "f16[s0, 1920][36284, 1]cuda:0" = split_with_sizes[44]
        getitem_45: "f16[s0, 9792][36284, 1]cuda:0" = split_with_sizes[45]
        getitem_46: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[46]
        getitem_47: "f16[s0, 576][36284, 1]cuda:0" = split_with_sizes[47]
        getitem_48: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[48]
        getitem_49: "f16[s0, 384][36284, 1]cuda:0" = split_with_sizes[49]
        getitem_50: "f16[s0, 576][36284, 1]cuda:0" = split_with_sizes[50]
        getitem_51: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[51]
        getitem_52: "f16[s0, 192][36284, 1]cuda:0" = split_with_sizes[52];  split_with_sizes = None
        
         # File: <torch_package_1>.caffe2/torch/fb/model_transform/serving_paradigm/split_concat_transform_structs.py:143 in forward, code: permuted = torch.ops.fbgemm.permute_pooled_embs_auto_grad(
        permute_pooled_embs_auto_grad_1: "f16[s0, 30516][30516, 1]cuda:0" = torch.ops.fbgemm.permute_pooled_embs_auto_grad.default(_to_copy_6, submod_0_cat_fusion_cpu__offset_dim_list, submod_0_cat_fusion_cpu__permute, submod_0_cat_fusion_cpu__inv_offset_dim_list, submod_0_cat_fusion_cpu__inv_permute);  _to_copy_6 = submod_0_cat_fusion_cpu__offset_dim_list = submod_0_cat_fusion_cpu__permute = submod_0_cat_fusion_cpu__inv_offset_dim_list = submod_0_cat_fusion_cpu__inv_permute = None
        
         # File: <torch_package_1>.caffe2/torch/fb/model_transform/serving_paradigm/split_concat_transform_structs.py:155 in forward, code: res = permuted.split_with_sizes(self.res_dim_list, dim=1)
        split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(permute_pooled_embs_auto_grad_1, [28, 16, 32, 32, 16, 32, 16, 4, 192, 192, 192, 384, 192, 192, 768, 192, 384, 192, 192, 192, 192, 192, 384, 192, 192, 384, 13056, 192, 192, 192, 384, 192, 192, 384, 192, 768, 192, 384, 2496, 192, 768, 5760, 4], 1);  permute_pooled_embs_auto_grad_1 = None
        getitem_53: "f16[s0, 28][30516, 1]cuda:0" = split_with_sizes_1[0]
        getitem_54: "f16[s0, 16][30516, 1]cuda:0" = split_with_sizes_1[1]
        getitem_55: "f16[s0, 32][30516, 1]cuda:0" = split_with_sizes_1[2]
        getitem_56: "f16[s0, 32][30516, 1]cuda:0" = split_with_sizes_1[3]
        getitem_57: "f16[s0, 16][30516, 1]cuda:0" = split_with_sizes_1[4]
        getitem_58: "f16[s0, 32][30516, 1]cuda:0" = split_with_sizes_1[5]
        getitem_59: "f16[s0, 16][30516, 1]cuda:0" = split_with_sizes_1[6]
        getitem_60: "f16[s0, 4][30516, 1]cuda:0" = split_with_sizes_1[7]
        getitem_61: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[8]
        getitem_62: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[9]
        getitem_63: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[10]
        getitem_64: "f16[s0, 384][30516, 1]cuda:0" = split_with_sizes_1[11]
        getitem_65: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[12]
        getitem_66: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[13]
        getitem_67: "f16[s0, 768][30516, 1]cuda:0" = split_with_sizes_1[14]
        getitem_68: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[15]
        getitem_69: "f16[s0, 384][30516, 1]cuda:0" = split_with_sizes_1[16]
        getitem_70: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[17]
        getitem_71: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[18]
        getitem_72: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[19]
        getitem_73: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[20]
        getitem_74: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[21]
        getitem_75: "f16[s0, 384][30516, 1]cuda:0" = split_with_sizes_1[22]
        getitem_76: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[23]
        getitem_77: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[24]
        getitem_78: "f16[s0, 384][30516, 1]cuda:0" = split_with_sizes_1[25]
        getitem_79: "f16[s0, 13056][30516, 1]cuda:0" = split_with_sizes_1[26]
        getitem_80: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[27]
        getitem_81: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[28]
        getitem_82: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[29]
        getitem_83: "f16[s0, 384][30516, 1]cuda:0" = split_with_sizes_1[30]
        getitem_84: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[31]
        getitem_85: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[32]
        getitem_86: "f16[s0, 384][30516, 1]cuda:0" = split_with_sizes_1[33]
        getitem_87: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[34]
        getitem_88: "f16[s0, 768][30516, 1]cuda:0" = split_with_sizes_1[35]
        getitem_89: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[36]
        getitem_90: "f16[s0, 384][30516, 1]cuda:0" = split_with_sizes_1[37]
        getitem_91: "f16[s0, 2496][30516, 1]cuda:0" = split_with_sizes_1[38]
        getitem_92: "f16[s0, 192][30516, 1]cuda:0" = split_with_sizes_1[39]
        getitem_93: "f16[s0, 768][30516, 1]cuda:0" = split_with_sizes_1[40]
        getitem_94: "f16[s0, 5760][30516, 1]cuda:0" = split_with_sizes_1[41];  split_with_sizes_1 = None
        
         # File: <eval_with_key>.27:103 in forward, code: split_with_sizes = torch.split_with_sizes(input = getitem_8, split_sizes = [3026, 240, 240, 192, 192], dim = 1);  getitem_8 = None
        split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(_to_copy, [3026, 240, 240, 192, 192], 1);  _to_copy = None
        getitem_96: "f16[s0, 3026][3890, 1]cuda:0" = split_with_sizes_2[0]
        getitem_97: "f16[s0, 240][3890, 1]cuda:0" = split_with_sizes_2[1]
        getitem_98: "f16[s0, 240][3890, 1]cuda:0" = split_with_sizes_2[2]
        getitem_99: "f16[s0, 192][3890, 1]cuda:0" = split_with_sizes_2[3]
        getitem_100: "f16[s0, 192][3890, 1]cuda:0" = split_with_sizes_2[4];  split_with_sizes_2 = None
        
         # File: <eval_with_key>.27:109 in forward, code: split_with_sizes_1 = torch.split_with_sizes(input = getitem_7, split_sizes = [96, 72, 96, 96, 96, 96, 72, 96, 96, 96, 72, 96, 64, 96, 64, 72, 72, 96, 64, 72, 72, 72, 64, 64, 64, 144, 144, 64, 64, 64, 96, 96], dim = 1);  getitem_7 = None
        split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(_to_copy_1, [96, 72, 96, 96, 96, 96, 72, 96, 96, 96, 72, 96, 64, 96, 64, 72, 72, 96, 64, 72, 72, 72, 64, 64, 64, 144, 144, 64, 64, 64, 96, 96], 1);  _to_copy_1 = None
        getitem_101: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[0]
        getitem_102: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[1]
        getitem_103: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[2]
        getitem_104: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[3]
        getitem_105: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[4]
        getitem_106: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[5]
        getitem_107: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[6]
        getitem_108: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[7]
        getitem_109: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[8]
        getitem_110: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[9]
        getitem_111: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[10]
        getitem_112: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[11]
        getitem_113: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[12]
        getitem_114: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[13]
        getitem_115: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[14]
        getitem_116: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[15]
        getitem_117: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[16]
        getitem_118: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[17]
        getitem_119: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[18]
        getitem_120: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[19]
        getitem_121: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[20]
        getitem_122: "f16[s0, 72][2688, 1]cuda:0" = split_with_sizes_3[21]
        getitem_123: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[22]
        getitem_124: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[23]
        getitem_125: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[24]
        getitem_126: "f16[s0, 144][2688, 1]cuda:0" = split_with_sizes_3[25]
        getitem_127: "f16[s0, 144][2688, 1]cuda:0" = split_with_sizes_3[26]
        getitem_128: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[27]
        getitem_129: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[28]
        getitem_130: "f16[s0, 64][2688, 1]cuda:0" = split_with_sizes_3[29]
        getitem_131: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[30]
        getitem_132: "f16[s0, 96][2688, 1]cuda:0" = split_with_sizes_3[31];  split_with_sizes_3 = None
        
         # File: <eval_with_key>.27:110 in forward, code: pow_1 = torch.pow(getitem_104, 2)
        pow_1: "f16[s0, 3026][3026, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(getitem_96, 2)
        
         # File: <eval_with_key>.27:113 in forward, code: linear = torch._C._nn.linear(getitem_105, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_105 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_97, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_97 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:116 in forward, code: linear_1 = torch._C._nn.linear(getitem_106, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_106 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_1: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_98, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_98 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:119 in forward, code: linear_2 = torch._C._nn.linear(getitem_107, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_107 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_2: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_99, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_99 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:122 in forward, code: linear_3 = torch._C._nn.linear(getitem_108, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_108 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_3: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_100, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_100 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:155 in forward, code: cat = torch.cat(tensors = [getitem_104, pow_1], dim = 1);  pow_1 = None
        cat_1: "f16[s0, 6052][6052, 1]cuda:0" = torch.ops.aten.cat.default([getitem_96, pow_1], 1);  pow_1 = None
        
         # File: <eval_with_key>.27:158 in forward, code: linear_4 = torch._C._nn.linear(getitem_109, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_109 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_4: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_101, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_101 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:161 in forward, code: linear_5 = torch._C._nn.linear(getitem_110, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_110 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_5: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_102, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_102 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:164 in forward, code: linear_6 = torch._C._nn.linear(getitem_111, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_111 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_6: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_103, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_103 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:167 in forward, code: linear_7 = torch._C._nn.linear(getitem_112, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_112 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_7: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_104, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_104 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:170 in forward, code: linear_8 = torch._C._nn.linear(getitem_113, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_113 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_8: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_105, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_105 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:173 in forward, code: linear_9 = torch._C._nn.linear(getitem_114, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_114 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_9: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_106, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_106 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:176 in forward, code: linear_10 = torch._C._nn.linear(getitem_115, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_115 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_10: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_107, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_107 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:179 in forward, code: linear_11 = torch._C._nn.linear(getitem_116, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_116 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_11: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_108, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_108 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:182 in forward, code: linear_12 = torch._C._nn.linear(getitem_117, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_117 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_12: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_109, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_109 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:185 in forward, code: linear_13 = torch._C._nn.linear(getitem_118, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_118 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_13: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_110, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_110 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:188 in forward, code: linear_14 = torch._C._nn.linear(getitem_119, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_119 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_14: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_111, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_111 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:191 in forward, code: linear_15 = torch._C._nn.linear(getitem_120, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_120 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_15: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_112, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_112 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:194 in forward, code: linear_16 = torch._C._nn.linear(getitem_121, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_121 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_16: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_113, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_113 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:197 in forward, code: linear_17 = torch._C._nn.linear(getitem_122, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_122 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_17: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_114, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_114 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:200 in forward, code: linear_18 = torch._C._nn.linear(getitem_123, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_123 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_18: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_115, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_115 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:203 in forward, code: linear_19 = torch._C._nn.linear(getitem_124, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_124 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_19: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_116, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_116 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:206 in forward, code: linear_20 = torch._C._nn.linear(getitem_125, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_125 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_20: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_117, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_117 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:209 in forward, code: linear_21 = torch._C._nn.linear(getitem_126, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_126 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_21: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_118, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_118 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:212 in forward, code: linear_22 = torch._C._nn.linear(getitem_127, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_127 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_22: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_119, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_119 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:215 in forward, code: linear_23 = torch._C._nn.linear(getitem_128, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_128 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_23: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_120, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_120 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:218 in forward, code: linear_24 = torch._C._nn.linear(getitem_129, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_129 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_24: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_121, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_121 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:221 in forward, code: linear_25 = torch._C._nn.linear(getitem_130, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_130 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_25: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_122, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_122 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:224 in forward, code: linear_26 = torch._C._nn.linear(getitem_131, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_131 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_26: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_123, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_123 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:227 in forward, code: linear_27 = torch._C._nn.linear(getitem_132, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_132 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_27: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_124, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_124 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:230 in forward, code: linear_28 = torch._C._nn.linear(getitem_133, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_133 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_28: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_125, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_125 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:233 in forward, code: linear_29 = torch._C._nn.linear(getitem_134, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_134 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_29: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_126, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_126 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:236 in forward, code: linear_30 = torch._C._nn.linear(getitem_135, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_135 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_30: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_127, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_127 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:239 in forward, code: linear_31 = torch._C._nn.linear(getitem_136, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_136 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_31: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_128, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_128 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:242 in forward, code: linear_32 = torch._C._nn.linear(getitem_137, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_137 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_32: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_129, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_129 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:245 in forward, code: linear_33 = torch._C._nn.linear(getitem_138, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_138 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_33: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_130, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_130 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:248 in forward, code: linear_34 = torch._C._nn.linear(getitem_139, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_139 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_34: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_131, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_131 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:251 in forward, code: linear_35 = torch._C._nn.linear(getitem_140, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_140 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_35: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(getitem_132, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  getitem_132 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:254 in forward, code: linear_36 = torch._C._nn.linear(cat, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_36: "f16[s0, 214][214, 1]cuda:0" = torch.ops.aten.linear.default(cat_1, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:257 in forward, code: linear_37 = torch._C._nn.linear(cat, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_w, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_b);  cat = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_w = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_b = None
        linear_37: "f16[s0, 42][42, 1]cuda:0" = torch.ops.aten.linear.default(cat_1, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_b);  cat_1 = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_b = None
        
         # File: <eval_with_key>.27:266 in forward, code: sum_1 = torch.sum(getitem_53, [1], keepdim = True);  getitem_53 = None
        sum_1: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_8, [1], True);  getitem_8 = None
        
         # File: <eval_with_key>.27:267 in forward, code: sum_2 = torch.sum(getitem_52, [1], keepdim = True);  getitem_52 = None
        sum_2: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_9, [1], True);  getitem_9 = None
        
         # File: <eval_with_key>.27:268 in forward, code: sum_3 = torch.sum(getitem_51, [1], keepdim = True);  getitem_51 = None
        sum_3: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_10, [1], True);  getitem_10 = None
        
         # File: <eval_with_key>.27:269 in forward, code: sum_4 = torch.sum(getitem_50, [1], keepdim = True);  getitem_50 = None
        sum_4: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_11, [1], True);  getitem_11 = None
        
         # File: <eval_with_key>.27:270 in forward, code: sum_5 = torch.sum(getitem_49, [1], keepdim = True);  getitem_49 = None
        sum_5: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_12, [1], True);  getitem_12 = None
        
         # File: <eval_with_key>.27:271 in forward, code: sum_6 = torch.sum(getitem_48, [1], keepdim = True);  getitem_48 = None
        sum_6: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_13, [1], True);  getitem_13 = None
        
         # File: <eval_with_key>.27:272 in forward, code: sum_7 = torch.sum(getitem_47, [1], keepdim = True);  getitem_47 = None
        sum_7: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_14, [1], True);  getitem_14 = None
        
         # File: <eval_with_key>.27:273 in forward, code: sum_8 = torch.sum(getitem_46, [1], keepdim = True);  getitem_46 = None
        sum_8: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_15, [1], True);  getitem_15 = None
        
         # File: <eval_with_key>.27:274 in forward, code: sum_9 = torch.sum(getitem_45, [1], keepdim = True);  getitem_45 = None
        sum_9: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_16, [1], True);  getitem_16 = None
        
         # File: <eval_with_key>.27:275 in forward, code: sum_10 = torch.sum(getitem_44, [1], keepdim = True);  getitem_44 = None
        sum_10: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_17, [1], True);  getitem_17 = None
        
         # File: <eval_with_key>.27:276 in forward, code: sum_11 = torch.sum(getitem_43, [1], keepdim = True);  getitem_43 = None
        sum_11: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_18, [1], True);  getitem_18 = None
        
         # File: <eval_with_key>.27:277 in forward, code: cat_38 = torch.cat(tensors = [linear_36, linear_37], dim = 1);  linear_36 = linear_37 = None
        cat_2: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.cat.default([linear_36, linear_37], 1);  linear_36 = linear_37 = None
        
         # File: <eval_with_key>.27:280 in forward, code: layer_norm = torch.nn.functional.layer_norm(linear, getitem_141, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias, eps = 1e-05);  linear = getitem_141 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias = None
        layer_norm: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias);  linear = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:283 in forward, code: layer_norm_1 = torch.nn.functional.layer_norm(linear_1, getitem_142, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias, eps = 1e-05);  linear_1 = getitem_142 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias = None
        layer_norm_1: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_1, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias);  linear_1 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias = None
        
         # File: <eval_with_key>.27:286 in forward, code: layer_norm_2 = torch.nn.functional.layer_norm(linear_2, getitem_143, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias, eps = 1e-05);  linear_2 = getitem_143 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias = None
        layer_norm_2: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_2, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias);  linear_2 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias = None
        
         # File: <eval_with_key>.27:289 in forward, code: layer_norm_3 = torch.nn.functional.layer_norm(linear_3, getitem_144, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias, eps = 1e-05);  linear_3 = getitem_144 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias = None
        layer_norm_3: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_3, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias);  linear_3 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias = None
        
         # File: <eval_with_key>.27:354 in forward, code: add = torch.add(sum_3, sum_5);  sum_3 = sum_5 = None
        add_605: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(sum_3, sum_5);  sum_3 = sum_5 = None
        
         # File: <eval_with_key>.27:355 in forward, code: sum_12 = torch.sum(getitem_96, [1], keepdim = True);  getitem_96 = None
        sum_12: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(getitem_60, [1], True);  getitem_60 = None
        
         # File: <eval_with_key>.27:358 in forward, code: layer_norm_4 = torch.nn.functional.layer_norm(linear_4, getitem_145, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_4 = getitem_145 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_4: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_4, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias);  linear_4 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:361 in forward, code: layer_norm_5 = torch.nn.functional.layer_norm(linear_5, getitem_146, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_5 = getitem_146 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_5: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_5, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias);  linear_5 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:364 in forward, code: layer_norm_6 = torch.nn.functional.layer_norm(linear_6, getitem_147, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_6 = getitem_147 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_6: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_6, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias);  linear_6 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:367 in forward, code: layer_norm_7 = torch.nn.functional.layer_norm(linear_7, getitem_148, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_7 = getitem_148 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_7: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_7, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias);  linear_7 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:370 in forward, code: layer_norm_8 = torch.nn.functional.layer_norm(linear_8, getitem_149, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias, eps = 1e-05);  linear_8 = getitem_149 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias = None
        layer_norm_8: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_8, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias);  linear_8 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:373 in forward, code: layer_norm_9 = torch.nn.functional.layer_norm(linear_9, getitem_150, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_9 = getitem_150 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_9: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_9, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias);  linear_9 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:376 in forward, code: layer_norm_10 = torch.nn.functional.layer_norm(linear_10, getitem_151, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_10 = getitem_151 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_10: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_10, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias);  linear_10 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:379 in forward, code: layer_norm_11 = torch.nn.functional.layer_norm(linear_11, getitem_152, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_11 = getitem_152 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_11: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_11, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias);  linear_11 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:382 in forward, code: layer_norm_12 = torch.nn.functional.layer_norm(linear_12, getitem_153, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias, eps = 1e-05);  linear_12 = getitem_153 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias = None
        layer_norm_12: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_12, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias);  linear_12 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:385 in forward, code: layer_norm_13 = torch.nn.functional.layer_norm(linear_13, getitem_154, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_13 = getitem_154 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_13: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_13, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias);  linear_13 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:388 in forward, code: layer_norm_14 = torch.nn.functional.layer_norm(linear_14, getitem_155, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_14 = getitem_155 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_14: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_14, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias);  linear_14 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:391 in forward, code: layer_norm_15 = torch.nn.functional.layer_norm(linear_15, getitem_156, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_15 = getitem_156 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_15: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_15, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias);  linear_15 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:394 in forward, code: layer_norm_16 = torch.nn.functional.layer_norm(linear_16, getitem_157, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_16 = getitem_157 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_16: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_16, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias);  linear_16 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:397 in forward, code: layer_norm_17 = torch.nn.functional.layer_norm(linear_17, getitem_158, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_17 = getitem_158 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_17: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_17, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias);  linear_17 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:400 in forward, code: layer_norm_18 = torch.nn.functional.layer_norm(linear_18, getitem_159, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_18 = getitem_159 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_18: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_18, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias);  linear_18 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:403 in forward, code: layer_norm_19 = torch.nn.functional.layer_norm(linear_19, getitem_160, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_19 = getitem_160 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_19: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_19, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias);  linear_19 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:406 in forward, code: layer_norm_20 = torch.nn.functional.layer_norm(linear_20, getitem_161, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_20 = getitem_161 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_20: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_20, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias);  linear_20 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:409 in forward, code: layer_norm_21 = torch.nn.functional.layer_norm(linear_21, getitem_162, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_21 = getitem_162 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_21: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_21, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias);  linear_21 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:412 in forward, code: layer_norm_22 = torch.nn.functional.layer_norm(linear_22, getitem_163, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias, eps = 1e-05);  linear_22 = getitem_163 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias = None
        layer_norm_22: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_22, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias);  linear_22 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias = None
        
         # File: <eval_with_key>.27:415 in forward, code: layer_norm_23 = torch.nn.functional.layer_norm(linear_23, getitem_164, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_23 = getitem_164 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_23: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_23, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias);  linear_23 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:418 in forward, code: layer_norm_24 = torch.nn.functional.layer_norm(linear_24, getitem_165, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias, eps = 1e-05);  linear_24 = getitem_165 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias = None
        layer_norm_24: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_24, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias);  linear_24 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias = None
        
         # File: <eval_with_key>.27:421 in forward, code: layer_norm_25 = torch.nn.functional.layer_norm(linear_25, getitem_166, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_25 = getitem_166 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_25: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_25, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias);  linear_25 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:424 in forward, code: layer_norm_26 = torch.nn.functional.layer_norm(linear_26, getitem_167, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_26 = getitem_167 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_26: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_26, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias);  linear_26 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:427 in forward, code: layer_norm_27 = torch.nn.functional.layer_norm(linear_27, getitem_168, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_27 = getitem_168 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_27: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_27, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias);  linear_27 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:430 in forward, code: layer_norm_28 = torch.nn.functional.layer_norm(linear_28, getitem_169, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_28 = getitem_169 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_28: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_28, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias);  linear_28 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:433 in forward, code: layer_norm_29 = torch.nn.functional.layer_norm(linear_29, getitem_170, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias, eps = 1e-05);  linear_29 = getitem_170 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias = None
        layer_norm_29: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_29, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias);  linear_29 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias = None
        
         # File: <eval_with_key>.27:436 in forward, code: layer_norm_30 = torch.nn.functional.layer_norm(linear_30, getitem_171, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias, eps = 1e-05);  linear_30 = getitem_171 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias = None
        layer_norm_30: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_30, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias);  linear_30 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias = None
        
         # File: <eval_with_key>.27:439 in forward, code: layer_norm_31 = torch.nn.functional.layer_norm(linear_31, getitem_172, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_31 = getitem_172 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_31: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_31, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias);  linear_31 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:442 in forward, code: layer_norm_32 = torch.nn.functional.layer_norm(linear_32, getitem_173, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_32 = getitem_173 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_32: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_32, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias);  linear_32 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:445 in forward, code: layer_norm_33 = torch.nn.functional.layer_norm(linear_33, getitem_174, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_33 = getitem_174 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_33: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_33, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias);  linear_33 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:448 in forward, code: layer_norm_34 = torch.nn.functional.layer_norm(linear_34, getitem_175, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias, eps = 1e-05);  linear_34 = getitem_175 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias = None
        layer_norm_34: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_34, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias);  linear_34 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:451 in forward, code: layer_norm_35 = torch.nn.functional.layer_norm(linear_35, getitem_176, weight = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale, bias = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias, eps = 1e-05);  linear_35 = getitem_176 = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale = main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias = None
        layer_norm_35: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_35, [192], submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale, submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias);  linear_35 = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale = submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias = None
        
         # File: <eval_with_key>.27:452 in forward, code: add_1 = torch.add(add, sum_10);  add = sum_10 = None
        add_708: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_605, sum_10);  add_605 = sum_10 = None
        
         # File: <eval_with_key>.27:458 in forward, code: add_2 = torch.add(add_1, sum_11);  add_1 = sum_11 = None
        add_712: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_708, sum_11);  add_708 = sum_11 = None
        
         # File: <eval_with_key>.27:461 in forward, code: layer_norm_36 = torch.nn.functional.layer_norm(cat_38, getitem_177, weight = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale, bias = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias, eps = 1e-05);  getitem_177 = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = None
        layer_norm_36: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.layer_norm.default(cat_2, [256], submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = None
        
         # File: <eval_with_key>.27:462 in forward, code: cat_39 = torch.cat(tensors = [getitem_61, getitem_103, getitem_60, getitem_102, getitem_59, getitem_101, getitem_58, getitem_100, getitem_57, getitem_99, getitem_56, getitem_98, getitem_55, getitem_97, getitem_54], dim = 1);  getitem_61 = getitem_103 = getitem_60 = getitem_102 = getitem_59 = getitem_101 = getitem_58 = getitem_100 = getitem_57 = getitem_99 = getitem_56 = getitem_98 = getitem_55 = getitem_97 = getitem_54 = None
        cat_3: "f16[s0, 700][700, 1]cuda:0" = torch.ops.aten.cat.default([getitem, getitem_53, getitem_1, getitem_54, getitem_2, getitem_55, getitem_3, getitem_56, getitem_4, getitem_57, getitem_5, getitem_58, getitem_6, getitem_59, getitem_7], 1);  getitem = getitem_53 = getitem_1 = getitem_54 = getitem_2 = getitem_55 = getitem_3 = getitem_56 = getitem_4 = getitem_57 = getitem_5 = getitem_58 = getitem_6 = getitem_59 = getitem_7 = None
        
         # File: <eval_with_key>.27:463 in forward, code: add_3 = torch.add(add_2, sum_9);  add_2 = sum_9 = None
        add_722: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_712, sum_9);  add_712 = sum_9 = None
        
         # File: <eval_with_key>.27:464 in forward, code: sigmoid = torch.sigmoid(layer_norm_36);  layer_norm_36 = None
        sigmoid: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_36);  layer_norm_36 = None
        
         # File: <eval_with_key>.27:467 in forward, code: linear_38 = torch._C._nn.linear(cat_39, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_38: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.linear.default(cat_3, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:470 in forward, code: linear_39 = torch._C._nn.linear(cat_39, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  cat_39 = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        linear_39: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.linear.default(cat_3, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_b);  cat_3 = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:471 in forward, code: add_4 = torch.add(add_3, sum_7);  add_3 = sum_7 = None
        add_735: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_722, sum_7);  add_722 = sum_7 = None
        
         # File: <eval_with_key>.27:472 in forward, code: relu = torch.nn.functional.relu(linear_39, inplace = False);  linear_39 = None
        relu: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.relu.default(linear_39);  linear_39 = None
        
         # File: <eval_with_key>.27:473 in forward, code: add_5 = torch.add(add_4, sum_6);  add_4 = sum_6 = None
        add_742: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_735, sum_6);  add_735 = sum_6 = None
        
         # File: <eval_with_key>.27:487 in forward, code: linear_40 = torch._C._nn.linear(relu, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b);  relu = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b = None
        linear_40: "f16[s0, 960][960, 1]cuda:0" = torch.ops.aten.linear.default(relu, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b);  relu = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:488 in forward, code: add_6 = torch.add(add_5, sum_4);  add_5 = sum_4 = None
        add_749: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_742, sum_4);  add_742 = sum_4 = None
        
         # File: <eval_with_key>.27:490 in forward, code: mul = cat_38 * unsqueeze_n_times;  cat_38 = unsqueeze_n_times = None
        mul_477: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_2, sigmoid);  cat_2 = sigmoid = None
        
         # File: <eval_with_key>.27:493 in forward, code: layer_norm_37 = torch.nn.functional.layer_norm(linear_38, getitem_178, weight = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale, bias = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias, eps = 1e-05);  getitem_178 = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = None
        layer_norm_37: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_38, [256], submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias);  submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = None
        
         # File: <eval_with_key>.27:496 in forward, code: linear_41 = torch._C._nn.linear(linear_40, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_w, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b);  linear_40 = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_w = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b = None
        linear_41: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(linear_40, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_w, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b);  linear_40 = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_w = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b = None
        
         # File: <eval_with_key>.27:497 in forward, code: add_7 = torch.add(add_6, sum_8);  add_6 = sum_8 = None
        add_762: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_749, sum_8);  add_749 = sum_8 = None
        
         # File: <eval_with_key>.27:500 in forward, code: linear_42 = torch._C._nn.linear(mul, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b);  mul = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b = None
        linear_42: "f16[s0, 3026][3026, 1]cuda:0" = torch.ops.aten.linear.default(mul_477, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b);  mul_477 = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b = None
        
         # File: <eval_with_key>.27:501 in forward, code: sigmoid_1 = torch.sigmoid(layer_norm_37);  layer_norm_37 = None
        sigmoid_1: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_37);  layer_norm_37 = None
        
         # File: <eval_with_key>.27:502 in forward, code: add_8 = torch.add(add_7, sum_12);  add_7 = sum_12 = None
        add_772: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_762, sum_12);  add_762 = sum_12 = None
        
         # File: <eval_with_key>.27:503 in forward, code: sigmoid_2 = torch.sigmoid(linear_42);  linear_42 = None
        sigmoid_2: "f16[s0, 3026][3026, 1]cuda:0" = torch.ops.aten.sigmoid.default(linear_42);  linear_42 = None
        
         # File: <eval_with_key>.27:505 in forward, code: add_9 = getitem_1 + main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb;  main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb = None
        add_779: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten.add.Tensor(_to_copy_7, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb);  _to_copy_7 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb = None
        
         # File: <eval_with_key>.27:507 in forward, code: add_10 = getitem + main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb;  main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb = None
        add_784: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten.add.Tensor(_to_copy_8, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb);  _to_copy_8 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb = None
        
         # File: <eval_with_key>.27:508 in forward, code: add_11 = torch.add(add_8, sum_2);  add_8 = sum_2 = None
        add_789: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_772, sum_2);  add_772 = sum_2 = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        mul_497: "f16[s0, 3026][3026, 1]cuda:0" = torch.ops.aten.mul.Tensor(getitem_96, sigmoid_2);  getitem_96 = sigmoid_2 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_38: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_779, [64], submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias);  add_779 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_39: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_784, [64], submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias);  add_784 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias = None
        
         # File: <eval_with_key>.27:518 in forward, code: add_12 = torch.add(add_11, sum_1);  add_11 = sum_1 = None
        add_804: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_789, sum_1);  add_789 = sum_1 = None
        
         # File: <eval_with_key>.27:520 in forward, code: add_13 = add_12 + main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias;  add_12 = main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias = None
        add_808: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(add_804, submod_0_main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias);  add_804 = submod_0_main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias = None
        
         # File: <eval_with_key>.27:523 in forward, code: linear_43 = torch._C._nn.linear(main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_mul_module, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = None
        linear_43: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.linear.default(mul_497, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = None
        
         # File: <eval_with_key>.27:525 in forward, code: mul_1 = linear_38 * unsqueeze_n_times_1;  linear_38 = unsqueeze_n_times_1 = None
        mul_508: "f16[s0, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_38, sigmoid_1);  linear_38 = sigmoid_1 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_44: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_38, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_45: "f16[s0, 200, 64][12800, 64, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_39, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight = None
        
         # File: <eval_with_key>.27:532 in forward, code: cat_40 = torch.cat(tensors = [linear_43, main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_mul_module], dim = 1);  linear_43 = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_mul_module = None
        cat_4: "f16[s0, 3282][3282, 1]cuda:0" = torch.ops.aten.cat.default([linear_43, mul_497], 1);  linear_43 = mul_497 = None
        
         # File: <eval_with_key>.27:535 in forward, code: linear_44 = torch._C._nn.linear(mul_1, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b);  mul_1 = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b = None
        linear_46: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.linear.default(mul_508, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b);  mul_508 = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:537 in forward, code: repeat = main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb.repeat(getitem_179, 1, 1);  main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb = getitem_179 = None
        repeat: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.repeat.default(submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb, [sym_size_int_12, 1, 1]);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb = None
        
         # File: <eval_with_key>.27:541 in forward, code: repeat_1 = main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb.repeat(getitem_180, 1, 1);  main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb = getitem_180 = None
        repeat_1: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.repeat.default(submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb, [sym_size_int_12, 1, 1]);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_40: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.layer_norm.default(repeat, [64], submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_41: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.layer_norm.default(repeat_1, [64], submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_47: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_40, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias);  layer_norm_40 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_48: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_41, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias);  layer_norm_41 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias = None
        
         # File: <eval_with_key>.27:560 in forward, code: layer_norm_38 = torch.nn.functional.layer_norm(cat_40, getitem_183, weight = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale, bias = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias, eps = 1e-05);  getitem_183 = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = None
        layer_norm_42: "f16[s0, 3282][3282, 1]cuda:0" = torch.ops.aten.layer_norm.default(cat_4, [3282], submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = None
        
         # File: <eval_with_key>.27:563 in forward, code: layer_norm_39 = torch.nn.functional.layer_norm(linear_44, getitem_184, weight = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale, bias = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias, eps = 1e-05);  getitem_184 = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale = main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias = None
        layer_norm_43: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_46, [1024], submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale, submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias);  submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale = submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias = None
        
         # File: <eval_with_key>.27:572 in forward, code: sigmoid_3 = torch.sigmoid(layer_norm_38);  layer_norm_38 = None
        sigmoid_3: "f16[s0, 3282][3282, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_42);  layer_norm_42 = None
        
         # File: <eval_with_key>.27:573 in forward, code: sigmoid_4 = torch.sigmoid(layer_norm_39);  layer_norm_39 = None
        sigmoid_4: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_43);  layer_norm_43 = None
        
         # File: <eval_with_key>.27:574 in forward, code: view = main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_k_proj.view(getitem_185, getitem_181, 1, 64);  main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_k_proj = None
        view: "f16[s0, 200, 1, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.view.default(linear_44, [sym_size_int_12, 200, 1, 64]);  linear_44 = None
        
         # File: <eval_with_key>.27:575 in forward, code: view_1 = main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0.view(getitem_185, getitem_181, 1, 64);  main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0 = getitem_181 = None
        view_1: "f16[s0, 200, 1, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_38, [sym_size_int_12, 200, 1, 64]);  layer_norm_38 = None
        
         # File: <eval_with_key>.27:576 in forward, code: view_2 = main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj.view(getitem_185, getitem_186, 1, 64);  main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj = None
        view_2: "f16[s0, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.view.default(linear_47, [sym_size_int_12, 32, 1, 64]);  linear_47 = None
        
         # File: <eval_with_key>.27:577 in forward, code: view_3 = main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_k_proj.view(getitem_188, getitem_182, 1, 64);  main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_k_proj = None
        view_3: "f16[s0, 200, 1, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.view.default(linear_45, [sym_size_int_12, 200, 1, 64]);  linear_45 = None
        
         # File: <eval_with_key>.27:578 in forward, code: view_4 = main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0.view(getitem_188, getitem_182, 1, 64);  main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0 = getitem_182 = None
        view_4: "f16[s0, 200, 1, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_39, [sym_size_int_12, 200, 1, 64]);  layer_norm_39 = None
        
         # File: <eval_with_key>.27:579 in forward, code: view_5 = main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj.view(getitem_188, getitem_189, 1, 64);  main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj = None
        view_5: "f16[s0, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.view.default(linear_48, [sym_size_int_12, 32, 1, 64]);  linear_48 = None
        
         # File: <eval_with_key>.27:580 in forward, code: permute = view.permute(0, 2, 1, 3);  view = None
        permute: "f16[s0, 1, 200, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(view, [0, 2, 1, 3]);  view = None
        
         # File: <eval_with_key>.27:581 in forward, code: permute_1 = view_1.permute(0, 2, 1, 3);  view_1 = None
        permute_1: "f16[s0, 1, 200, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_1, [0, 2, 1, 3]);  view_1 = None
        
         # File: <eval_with_key>.27:582 in forward, code: permute_2 = view_2.permute(0, 2, 1, 3);  view_2 = None
        permute_2: "f16[s0, 1, 32, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None
        
         # File: <eval_with_key>.27:583 in forward, code: permute_3 = view_3.permute(0, 2, 1, 3);  view_3 = None
        permute_3: "f16[s0, 1, 200, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_3, [0, 2, 1, 3]);  view_3 = None
        
         # File: <eval_with_key>.27:584 in forward, code: permute_4 = view_4.permute(0, 2, 1, 3);  view_4 = None
        permute_4: "f16[s0, 1, 200, 64][12800, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_4, [0, 2, 1, 3]);  view_4 = None
        
         # File: <eval_with_key>.27:585 in forward, code: permute_5 = view_5.permute(0, 2, 1, 3);  view_5 = None
        permute_5: "f16[s0, 1, 32, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None
        
         # File: <eval_with_key>.27:598 in forward, code: scaled_dot_product_attention = torch._C._nn.scaled_dot_product_attention(permute_2, permute, permute_1, attn_mask = None, dropout_p = 0.0, is_causal = False);  permute_2 = permute = permute_1 = None
        scaled_dot_product_attention: "f16[s0, 1, 32, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.scaled_dot_product_attention.default(permute_2, permute, permute_1);  permute_2 = permute = permute_1 = None
        
         # File: <eval_with_key>.27:599 in forward, code: scaled_dot_product_attention_1 = torch._C._nn.scaled_dot_product_attention(permute_5, permute_3, permute_4, attn_mask = None, dropout_p = 0.0, is_causal = False);  permute_5 = permute_3 = permute_4 = None
        scaled_dot_product_attention_1: "f16[s0, 1, 32, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.scaled_dot_product_attention.default(permute_5, permute_3, permute_4);  permute_5 = permute_3 = permute_4 = None
        
         # File: <eval_with_key>.27:601 in forward, code: mul_2 = cat_40 * unsqueeze_n_times_2;  cat_40 = unsqueeze_n_times_2 = None
        mul_567: "f16[s0, 3282][3282, 1]cuda:0" = torch.ops.aten.mul.Tensor(cat_4, sigmoid_3);  cat_4 = sigmoid_3 = None
        
         # File: <eval_with_key>.27:603 in forward, code: mul_3 = linear_44 * unsqueeze_n_times_3;  linear_44 = unsqueeze_n_times_3 = None
        mul_570: "f16[s0, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_46, sigmoid_4);  linear_46 = sigmoid_4 = None
        
         # File: <eval_with_key>.27:604 in forward, code: permute_6 = scaled_dot_product_attention.permute(0, 2, 1, 3);  scaled_dot_product_attention = None
        permute_6: "f16[s0, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(scaled_dot_product_attention, [0, 2, 1, 3]);  scaled_dot_product_attention = None
        
         # File: <eval_with_key>.27:605 in forward, code: permute_7 = scaled_dot_product_attention_1.permute(0, 2, 1, 3);  scaled_dot_product_attention_1 = None
        permute_7: "f16[s0, 32, 1, 64][2048, 64, 64, 1]cuda:0" = torch.ops.aten.permute.default(scaled_dot_product_attention_1, [0, 2, 1, 3]);  scaled_dot_product_attention_1 = None
        
         # File: <eval_with_key>.27:608 in forward, code: linear_45 = torch._C._nn.linear(mul_2, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_b = None
        linear_49: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(mul_567, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:611 in forward, code: linear_46 = torch._C._nn.linear(mul_2, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_b = None
        linear_50: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(mul_567, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:614 in forward, code: linear_47 = torch._C._nn.linear(mul_2, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_b = None
        linear_51: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(mul_567, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:617 in forward, code: linear_48 = torch._C._nn.linear(mul_2, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_b = None
        linear_52: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(mul_567, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:620 in forward, code: linear_49 = torch._C._nn.linear(mul_2, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_b = None
        linear_53: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(mul_567, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:623 in forward, code: linear_50 = torch._C._nn.linear(mul_2, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_w, main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_b);  main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_w = main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_b = None
        linear_54: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.linear.default(mul_567, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:626 in forward, code: view_6 = contiguous.view(getitem_185, getitem_186, getitem_187);  contiguous = getitem_185 = getitem_186 = getitem_187 = None
        view_6: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(permute_6, [sym_size_int_12, 32, 64]);  permute_6 = None
        
         # File: <eval_with_key>.27:627 in forward, code: add_14 = view_6 + repeat;  view_6 = repeat = None
        add_976: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.add.Tensor(view_6, repeat);  view_6 = repeat = None
        
         # File: <eval_with_key>.27:628 in forward, code: view_7 = contiguous_1.view(getitem_188, getitem_189, getitem_190);  contiguous_1 = getitem_188 = getitem_189 = getitem_190 = None
        view_7: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(permute_7, [sym_size_int_12, 32, 64]);  permute_7 = None
        
         # File: <eval_with_key>.27:629 in forward, code: add_15 = view_7 + repeat_1;  view_7 = repeat_1 = None
        add_985: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.add.Tensor(view_7, repeat_1);  view_7 = repeat_1 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_44: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_976, [64], submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_45: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_985, [64], submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias);  submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_55: "f16[s0, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_44, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias);  layer_norm_44 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_56: "f16[s0, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_45, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias);  layer_norm_45 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
        gelu: "f16[s0, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.gelu.default(linear_55);  linear_55 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
        gelu_1: "f16[s0, 32, 128][4096, 128, 1]cuda:0" = torch.ops.aten.gelu.default(linear_56);  linear_56 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_57: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.linear.default(gelu, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias);  gelu = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias = None
        
         # File: <eval_with_key>.27:637 in forward, code: add_16 = add_14 + main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2;  add_14 = main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2 = None
        add_1018: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.add.Tensor(add_976, linear_57);  add_976 = linear_57 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_58: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.linear.default(gelu_1, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_weight, submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias);  gelu_1 = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_weight = submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias = None
        
         # File: <eval_with_key>.27:639 in forward, code: add_17 = add_15 + main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2;  add_15 = main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2 = None
        add_1027: "f16[s0, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.add.Tensor(add_985, linear_58);  add_985 = linear_58 = None
        
         # File: <eval_with_key>.27:640 in forward, code: equally_split = torch.ops.fb.equally_split(add_16, 32, 1);  add_16 = None
        split = torch.ops.aten.split.Tensor(add_1018, 1, 1);  add_1018 = None
        getitem_133: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[0]
        getitem_134: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[1]
        getitem_135: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[2]
        getitem_136: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[3]
        getitem_137: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[4]
        getitem_138: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[5]
        getitem_139: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[6]
        getitem_140: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[7]
        getitem_141: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[8]
        getitem_142: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[9]
        getitem_143: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[10]
        getitem_144: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[11]
        getitem_145: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[12]
        getitem_146: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[13]
        getitem_147: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[14]
        getitem_148: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[15]
        getitem_149: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[16]
        getitem_150: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[17]
        getitem_151: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[18]
        getitem_152: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[19]
        getitem_153: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[20]
        getitem_154: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[21]
        getitem_155: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[22]
        getitem_156: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[23]
        getitem_157: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[24]
        getitem_158: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[25]
        getitem_159: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[26]
        getitem_160: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[27]
        getitem_161: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[28]
        getitem_162: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[29]
        getitem_163: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[30]
        getitem_164: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split[31];  split = None
        
         # File: <eval_with_key>.27:641 in forward, code: equally_split_1 = torch.ops.fb.equally_split(add_17, 32, 1);  add_17 = None
        split_1 = torch.ops.aten.split.Tensor(add_1027, 1, 1);  add_1027 = None
        getitem_165: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[0]
        getitem_166: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[1]
        getitem_167: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[2]
        getitem_168: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[3]
        getitem_169: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[4]
        getitem_170: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[5]
        getitem_171: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[6]
        getitem_172: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[7]
        getitem_173: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[8]
        getitem_174: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[9]
        getitem_175: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[10]
        getitem_176: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[11]
        getitem_177: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[12]
        getitem_178: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[13]
        getitem_179: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[14]
        getitem_180: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[15]
        getitem_181: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[16]
        getitem_182: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[17]
        getitem_183: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[18]
        getitem_184: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[19]
        getitem_185: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[20]
        getitem_186: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[21]
        getitem_187: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[22]
        getitem_188: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[23]
        getitem_189: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[24]
        getitem_190: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[25]
        getitem_191: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[26]
        getitem_192: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[27]
        getitem_193: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[28]
        getitem_194: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[29]
        getitem_195: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[30]
        getitem_196: "f16[s0, 1, 64][2048, 64, 1]cuda:0" = split_1[31];  split_1 = None
        
         # File: <eval_with_key>.27:706 in forward, code: reshape = getitem_191.reshape(-1, 64);  getitem_191 = None
        view_8: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_133, [sym_size_int_12, 64]);  getitem_133 = None
        
         # File: <eval_with_key>.27:707 in forward, code: reshape_1 = getitem_192.reshape(-1, 64);  getitem_192 = None
        view_9: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_134, [sym_size_int_12, 64]);  getitem_134 = None
        
         # File: <eval_with_key>.27:708 in forward, code: reshape_2 = getitem_193.reshape(-1, 64);  getitem_193 = None
        view_10: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_135, [sym_size_int_12, 64]);  getitem_135 = None
        
         # File: <eval_with_key>.27:709 in forward, code: reshape_3 = getitem_194.reshape(-1, 64);  getitem_194 = None
        view_11: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_136, [sym_size_int_12, 64]);  getitem_136 = None
        
         # File: <eval_with_key>.27:710 in forward, code: reshape_4 = getitem_195.reshape(-1, 64);  getitem_195 = None
        view_12: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_137, [sym_size_int_12, 64]);  getitem_137 = None
        
         # File: <eval_with_key>.27:711 in forward, code: reshape_5 = getitem_196.reshape(-1, 64);  getitem_196 = None
        view_13: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_138, [sym_size_int_12, 64]);  getitem_138 = None
        
         # File: <eval_with_key>.27:712 in forward, code: reshape_6 = getitem_197.reshape(-1, 64);  getitem_197 = None
        view_14: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_139, [sym_size_int_12, 64]);  getitem_139 = None
        
         # File: <eval_with_key>.27:713 in forward, code: reshape_7 = getitem_198.reshape(-1, 64);  getitem_198 = None
        view_15: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_140, [sym_size_int_12, 64]);  getitem_140 = None
        
         # File: <eval_with_key>.27:714 in forward, code: reshape_8 = getitem_199.reshape(-1, 64);  getitem_199 = None
        view_16: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_141, [sym_size_int_12, 64]);  getitem_141 = None
        
         # File: <eval_with_key>.27:715 in forward, code: reshape_9 = getitem_200.reshape(-1, 64);  getitem_200 = None
        view_17: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_142, [sym_size_int_12, 64]);  getitem_142 = None
        
         # File: <eval_with_key>.27:716 in forward, code: reshape_10 = getitem_201.reshape(-1, 64);  getitem_201 = None
        view_18: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_143, [sym_size_int_12, 64]);  getitem_143 = None
        
         # File: <eval_with_key>.27:717 in forward, code: reshape_11 = getitem_202.reshape(-1, 64);  getitem_202 = None
        view_19: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_144, [sym_size_int_12, 64]);  getitem_144 = None
        
         # File: <eval_with_key>.27:718 in forward, code: reshape_12 = getitem_203.reshape(-1, 64);  getitem_203 = None
        view_20: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_145, [sym_size_int_12, 64]);  getitem_145 = None
        
         # File: <eval_with_key>.27:719 in forward, code: reshape_13 = getitem_204.reshape(-1, 64);  getitem_204 = None
        view_21: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_146, [sym_size_int_12, 64]);  getitem_146 = None
        
         # File: <eval_with_key>.27:720 in forward, code: reshape_14 = getitem_205.reshape(-1, 64);  getitem_205 = None
        view_22: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_147, [sym_size_int_12, 64]);  getitem_147 = None
        
         # File: <eval_with_key>.27:721 in forward, code: reshape_15 = getitem_206.reshape(-1, 64);  getitem_206 = None
        view_23: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_148, [sym_size_int_12, 64]);  getitem_148 = None
        
         # File: <eval_with_key>.27:722 in forward, code: reshape_16 = getitem_207.reshape(-1, 64);  getitem_207 = None
        view_24: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_149, [sym_size_int_12, 64]);  getitem_149 = None
        
         # File: <eval_with_key>.27:723 in forward, code: reshape_17 = getitem_208.reshape(-1, 64);  getitem_208 = None
        view_25: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_150, [sym_size_int_12, 64]);  getitem_150 = None
        
         # File: <eval_with_key>.27:724 in forward, code: reshape_18 = getitem_209.reshape(-1, 64);  getitem_209 = None
        view_26: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_151, [sym_size_int_12, 64]);  getitem_151 = None
        
         # File: <eval_with_key>.27:725 in forward, code: reshape_19 = getitem_210.reshape(-1, 64);  getitem_210 = None
        view_27: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_152, [sym_size_int_12, 64]);  getitem_152 = None
        
         # File: <eval_with_key>.27:726 in forward, code: reshape_20 = getitem_211.reshape(-1, 64);  getitem_211 = None
        view_28: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_153, [sym_size_int_12, 64]);  getitem_153 = None
        
         # File: <eval_with_key>.27:727 in forward, code: reshape_21 = getitem_212.reshape(-1, 64);  getitem_212 = None
        view_29: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_154, [sym_size_int_12, 64]);  getitem_154 = None
        
         # File: <eval_with_key>.27:728 in forward, code: reshape_22 = getitem_213.reshape(-1, 64);  getitem_213 = None
        view_30: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_155, [sym_size_int_12, 64]);  getitem_155 = None
        
         # File: <eval_with_key>.27:729 in forward, code: reshape_23 = getitem_214.reshape(-1, 64);  getitem_214 = None
        view_31: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_156, [sym_size_int_12, 64]);  getitem_156 = None
        
         # File: <eval_with_key>.27:730 in forward, code: reshape_24 = getitem_215.reshape(-1, 64);  getitem_215 = None
        view_32: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_157, [sym_size_int_12, 64]);  getitem_157 = None
        
         # File: <eval_with_key>.27:731 in forward, code: reshape_25 = getitem_216.reshape(-1, 64);  getitem_216 = None
        view_33: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_158, [sym_size_int_12, 64]);  getitem_158 = None
        
         # File: <eval_with_key>.27:732 in forward, code: reshape_26 = getitem_217.reshape(-1, 64);  getitem_217 = None
        view_34: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_159, [sym_size_int_12, 64]);  getitem_159 = None
        
         # File: <eval_with_key>.27:733 in forward, code: reshape_27 = getitem_218.reshape(-1, 64);  getitem_218 = None
        view_35: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_160, [sym_size_int_12, 64]);  getitem_160 = None
        
         # File: <eval_with_key>.27:734 in forward, code: reshape_28 = getitem_219.reshape(-1, 64);  getitem_219 = None
        view_36: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_161, [sym_size_int_12, 64]);  getitem_161 = None
        
         # File: <eval_with_key>.27:735 in forward, code: reshape_29 = getitem_220.reshape(-1, 64);  getitem_220 = None
        view_37: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_162, [sym_size_int_12, 64]);  getitem_162 = None
        
         # File: <eval_with_key>.27:736 in forward, code: reshape_30 = getitem_221.reshape(-1, 64);  getitem_221 = None
        view_38: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_163, [sym_size_int_12, 64]);  getitem_163 = None
        
         # File: <eval_with_key>.27:737 in forward, code: reshape_31 = getitem_222.reshape(-1, 64);  getitem_222 = None
        view_39: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_164, [sym_size_int_12, 64]);  getitem_164 = None
        
         # File: <eval_with_key>.27:738 in forward, code: reshape_32 = getitem_223.reshape(-1, 64);  getitem_223 = None
        view_40: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_165, [sym_size_int_12, 64]);  getitem_165 = None
        
         # File: <eval_with_key>.27:739 in forward, code: reshape_33 = getitem_224.reshape(-1, 64);  getitem_224 = None
        view_41: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_166, [sym_size_int_12, 64]);  getitem_166 = None
        
         # File: <eval_with_key>.27:740 in forward, code: reshape_34 = getitem_225.reshape(-1, 64);  getitem_225 = None
        view_42: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_167, [sym_size_int_12, 64]);  getitem_167 = None
        
         # File: <eval_with_key>.27:741 in forward, code: reshape_35 = getitem_226.reshape(-1, 64);  getitem_226 = None
        view_43: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_168, [sym_size_int_12, 64]);  getitem_168 = None
        
         # File: <eval_with_key>.27:742 in forward, code: reshape_36 = getitem_227.reshape(-1, 64);  getitem_227 = None
        view_44: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_169, [sym_size_int_12, 64]);  getitem_169 = None
        
         # File: <eval_with_key>.27:743 in forward, code: reshape_37 = getitem_228.reshape(-1, 64);  getitem_228 = None
        view_45: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_170, [sym_size_int_12, 64]);  getitem_170 = None
        
         # File: <eval_with_key>.27:744 in forward, code: reshape_38 = getitem_229.reshape(-1, 64);  getitem_229 = None
        view_46: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_171, [sym_size_int_12, 64]);  getitem_171 = None
        
         # File: <eval_with_key>.27:745 in forward, code: reshape_39 = getitem_230.reshape(-1, 64);  getitem_230 = None
        view_47: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_172, [sym_size_int_12, 64]);  getitem_172 = None
        
         # File: <eval_with_key>.27:746 in forward, code: reshape_40 = getitem_231.reshape(-1, 64);  getitem_231 = None
        view_48: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_173, [sym_size_int_12, 64]);  getitem_173 = None
        
         # File: <eval_with_key>.27:747 in forward, code: reshape_41 = getitem_232.reshape(-1, 64);  getitem_232 = None
        view_49: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_174, [sym_size_int_12, 64]);  getitem_174 = None
        
         # File: <eval_with_key>.27:748 in forward, code: reshape_42 = getitem_233.reshape(-1, 64);  getitem_233 = None
        view_50: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_175, [sym_size_int_12, 64]);  getitem_175 = None
        
         # File: <eval_with_key>.27:749 in forward, code: reshape_43 = getitem_234.reshape(-1, 64);  getitem_234 = None
        view_51: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_176, [sym_size_int_12, 64]);  getitem_176 = None
        
         # File: <eval_with_key>.27:750 in forward, code: reshape_44 = getitem_235.reshape(-1, 64);  getitem_235 = None
        view_52: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_177, [sym_size_int_12, 64]);  getitem_177 = None
        
         # File: <eval_with_key>.27:751 in forward, code: reshape_45 = getitem_236.reshape(-1, 64);  getitem_236 = None
        view_53: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_178, [sym_size_int_12, 64]);  getitem_178 = None
        
         # File: <eval_with_key>.27:752 in forward, code: reshape_46 = getitem_237.reshape(-1, 64);  getitem_237 = None
        view_54: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_179, [sym_size_int_12, 64]);  getitem_179 = None
        
         # File: <eval_with_key>.27:753 in forward, code: reshape_47 = getitem_238.reshape(-1, 64);  getitem_238 = None
        view_55: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_180, [sym_size_int_12, 64]);  getitem_180 = None
        
         # File: <eval_with_key>.27:754 in forward, code: reshape_48 = getitem_239.reshape(-1, 64);  getitem_239 = None
        view_56: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_181, [sym_size_int_12, 64]);  getitem_181 = None
        
         # File: <eval_with_key>.27:755 in forward, code: reshape_49 = getitem_240.reshape(-1, 64);  getitem_240 = None
        view_57: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_182, [sym_size_int_12, 64]);  getitem_182 = None
        
         # File: <eval_with_key>.27:756 in forward, code: reshape_50 = getitem_241.reshape(-1, 64);  getitem_241 = None
        view_58: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_183, [sym_size_int_12, 64]);  getitem_183 = None
        
         # File: <eval_with_key>.27:757 in forward, code: reshape_51 = getitem_242.reshape(-1, 64);  getitem_242 = None
        view_59: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_184, [sym_size_int_12, 64]);  getitem_184 = None
        
         # File: <eval_with_key>.27:758 in forward, code: reshape_52 = getitem_243.reshape(-1, 64);  getitem_243 = None
        view_60: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_185, [sym_size_int_12, 64]);  getitem_185 = None
        
         # File: <eval_with_key>.27:759 in forward, code: reshape_53 = getitem_244.reshape(-1, 64);  getitem_244 = None
        view_61: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_186, [sym_size_int_12, 64]);  getitem_186 = None
        
         # File: <eval_with_key>.27:760 in forward, code: reshape_54 = getitem_245.reshape(-1, 64);  getitem_245 = None
        view_62: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_187, [sym_size_int_12, 64]);  getitem_187 = None
        
         # File: <eval_with_key>.27:761 in forward, code: reshape_55 = getitem_246.reshape(-1, 64);  getitem_246 = None
        view_63: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_188, [sym_size_int_12, 64]);  getitem_188 = None
        
         # File: <eval_with_key>.27:762 in forward, code: reshape_56 = getitem_247.reshape(-1, 64);  getitem_247 = None
        view_64: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_189, [sym_size_int_12, 64]);  getitem_189 = None
        
         # File: <eval_with_key>.27:763 in forward, code: reshape_57 = getitem_248.reshape(-1, 64);  getitem_248 = None
        view_65: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_190, [sym_size_int_12, 64]);  getitem_190 = None
        
         # File: <eval_with_key>.27:764 in forward, code: reshape_58 = getitem_249.reshape(-1, 64);  getitem_249 = None
        view_66: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_191, [sym_size_int_12, 64]);  getitem_191 = None
        
         # File: <eval_with_key>.27:765 in forward, code: reshape_59 = getitem_250.reshape(-1, 64);  getitem_250 = None
        view_67: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_192, [sym_size_int_12, 64]);  getitem_192 = None
        
         # File: <eval_with_key>.27:766 in forward, code: reshape_60 = getitem_251.reshape(-1, 64);  getitem_251 = None
        view_68: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_193, [sym_size_int_12, 64]);  getitem_193 = None
        
         # File: <eval_with_key>.27:767 in forward, code: reshape_61 = getitem_252.reshape(-1, 64);  getitem_252 = None
        view_69: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_194, [sym_size_int_12, 64]);  getitem_194 = None
        
         # File: <eval_with_key>.27:768 in forward, code: reshape_62 = getitem_253.reshape(-1, 64);  getitem_253 = None
        view_70: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_195, [sym_size_int_12, 64]);  getitem_195 = None
        
         # File: <eval_with_key>.27:769 in forward, code: reshape_63 = getitem_254.reshape(-1, 64);  getitem_254 = None
        view_71: "f16[s0, 64][2048, 1]cuda:0" = torch.ops.aten.view.default(getitem_196, [sym_size_int_12, 64]);  getitem_196 = None
        
         # File: <eval_with_key>.27:770 in forward, code: stack = torch.stack(tensors = [reshape_32, reshape_33, reshape_34, reshape_35, reshape_36, reshape_37, reshape_38, reshape_39, reshape_40, reshape_41, reshape_42, reshape_43, reshape_44, reshape_45, reshape_46, reshape_47, reshape_48, reshape_49, reshape_50, reshape_51, reshape_52, reshape_53, reshape_54, reshape_55, reshape_56, reshape_57, reshape_58, reshape_59, reshape_60, reshape_61, reshape_62, reshape_63, reshape, reshape_1, reshape_2, reshape_3, reshape_4, reshape_5, reshape_6, reshape_7, reshape_8, reshape_9, reshape_10, reshape_11, reshape_12, reshape_13, reshape_14, reshape_15, reshape_16, reshape_17, reshape_18, reshape_19, reshape_20, reshape_21, reshape_22, reshape_23, reshape_24, reshape_25, reshape_26, reshape_27, reshape_28, reshape_29, reshape_30, reshape_31], dim = 0);  reshape_32 = reshape_33 = reshape_34 = reshape_35 = reshape_36 = reshape_37 = reshape_38 = reshape_39 = reshape_40 = reshape_41 = reshape_42 = reshape_43 = reshape_44 = reshape_45 = reshape_46 = reshape_47 = reshape_48 = reshape_49 = reshape_50 = reshape_51 = reshape_52 = reshape_53 = reshape_54 = reshape_55 = reshape_56 = reshape_57 = reshape_58 = reshape_59 = reshape_60 = reshape_61 = reshape_62 = reshape_63 = reshape = reshape_1 = reshape_2 = reshape_3 = reshape_4 = reshape_5 = reshape_6 = reshape_7 = reshape_8 = reshape_9 = reshape_10 = reshape_11 = reshape_12 = reshape_13 = reshape_14 = reshape_15 = reshape_16 = reshape_17 = reshape_18 = reshape_19 = reshape_20 = reshape_21 = reshape_22 = reshape_23 = reshape_24 = reshape_25 = reshape_26 = reshape_27 = reshape_28 = reshape_29 = reshape_30 = reshape_31 = None
        stack: "f16[64, s0, 64][64*s0, 64, 1]cuda:0" = torch.ops.aten.stack.default([view_40, view_41, view_42, view_43, view_44, view_45, view_46, view_47, view_48, view_49, view_50, view_51, view_52, view_53, view_54, view_55, view_56, view_57, view_58, view_59, view_60, view_61, view_62, view_63, view_64, view_65, view_66, view_67, view_68, view_69, view_70, view_71, view_8, view_9, view_10, view_11, view_12, view_13, view_14, view_15, view_16, view_17, view_18, view_19, view_20, view_21, view_22, view_23, view_24, view_25, view_26, view_27, view_28, view_29, view_30, view_31, view_32, view_33, view_34, view_35, view_36, view_37, view_38, view_39]);  view_40 = view_41 = view_42 = view_43 = view_44 = view_45 = view_46 = view_47 = view_48 = view_49 = view_50 = view_51 = view_52 = view_53 = view_54 = view_55 = view_56 = view_57 = view_58 = view_59 = view_60 = view_61 = view_62 = view_63 = view_64 = view_65 = view_66 = view_67 = view_68 = view_69 = view_70 = view_71 = view_8 = view_9 = view_10 = view_11 = view_12 = view_13 = view_14 = view_15 = view_16 = view_17 = view_18 = view_19 = view_20 = view_21 = view_22 = view_23 = view_24 = view_25 = view_26 = view_27 = view_28 = view_29 = view_30 = view_31 = view_32 = view_33 = view_34 = view_35 = view_36 = view_37 = view_38 = view_39 = None
        
         # File: <eval_with_key>.27:771 in forward, code: nan_to_num = torch.nan_to_num(stack, 0.0);  stack = None
        nan_to_num: "f16[64, s0, 64][64*s0, 64, 1]cuda:0" = torch.ops.aten.nan_to_num.default(stack, 0.0);  stack = None
        
         # File: <eval_with_key>.27:772 in forward, code: clamp = nan_to_num.clamp(min = -1000.1, max = 1000.1);  nan_to_num = None
        clamp: "f16[64, s0, 64][64*s0, 64, 1]cuda:0" = torch.ops.aten.clamp.default(nan_to_num, -1000.1, 1000.1);  nan_to_num = None
        
         # File: <eval_with_key>.27:775 in forward, code: baddbmm = torch.baddbmm(main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias, clamp, main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight);  main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias = clamp = main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight = None
        baddbmm: "f16[64, s0, 192][192*s0, 192, 1]cuda:0" = torch.ops.aten.baddbmm.default(submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias, clamp, submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight);  submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias = clamp = submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight = None
        
         # File: <eval_with_key>.27:777 in forward, code: equally_split_2 = torch.ops.fb.equally_split(main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_1, 64, 0);  main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_1 = None
        split_2 = torch.ops.aten.split.Tensor(baddbmm, 1);  baddbmm = None
        getitem_197: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[0]
        getitem_198: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[1]
        getitem_199: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[2]
        getitem_200: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[3]
        getitem_201: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[4]
        getitem_202: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[5]
        getitem_203: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[6]
        getitem_204: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[7]
        getitem_205: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[8]
        getitem_206: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[9]
        getitem_207: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[10]
        getitem_208: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[11]
        getitem_209: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[12]
        getitem_210: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[13]
        getitem_211: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[14]
        getitem_212: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[15]
        getitem_213: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[16]
        getitem_214: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[17]
        getitem_215: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[18]
        getitem_216: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[19]
        getitem_217: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[20]
        getitem_218: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[21]
        getitem_219: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[22]
        getitem_220: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[23]
        getitem_221: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[24]
        getitem_222: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[25]
        getitem_223: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[26]
        getitem_224: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[27]
        getitem_225: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[28]
        getitem_226: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[29]
        getitem_227: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[30]
        getitem_228: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[31]
        getitem_229: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[32]
        getitem_230: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[33]
        getitem_231: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[34]
        getitem_232: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[35]
        getitem_233: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[36]
        getitem_234: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[37]
        getitem_235: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[38]
        getitem_236: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[39]
        getitem_237: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[40]
        getitem_238: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[41]
        getitem_239: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[42]
        getitem_240: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[43]
        getitem_241: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[44]
        getitem_242: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[45]
        getitem_243: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[46]
        getitem_244: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[47]
        getitem_245: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[48]
        getitem_246: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[49]
        getitem_247: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[50]
        getitem_248: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[51]
        getitem_249: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[52]
        getitem_250: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[53]
        getitem_251: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[54]
        getitem_252: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[55]
        getitem_253: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[56]
        getitem_254: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[57]
        getitem_255: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[58]
        getitem_256: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[59]
        getitem_257: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[60]
        getitem_258: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[61]
        getitem_259: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[62]
        getitem_260: "f16[1, s0, 192][192*s0, 192, 1]cuda:0" = split_2[63];  split_2 = None
        
         # File: <eval_with_key>.27:842 in forward, code: reshape_64 = getitem_255.reshape(-1, 192);  getitem_255 = None
        view_72: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_197, [-1, 192]);  getitem_197 = None
        
         # File: <eval_with_key>.27:843 in forward, code: reshape_65 = getitem_256.reshape(-1, 192);  getitem_256 = None
        view_73: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_198, [-1, 192]);  getitem_198 = None
        
         # File: <eval_with_key>.27:844 in forward, code: reshape_66 = getitem_257.reshape(-1, 192);  getitem_257 = None
        view_74: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_199, [-1, 192]);  getitem_199 = None
        
         # File: <eval_with_key>.27:845 in forward, code: reshape_67 = getitem_258.reshape(-1, 192);  getitem_258 = None
        view_75: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_200, [-1, 192]);  getitem_200 = None
        
         # File: <eval_with_key>.27:846 in forward, code: reshape_68 = getitem_259.reshape(-1, 192);  getitem_259 = None
        view_76: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_201, [-1, 192]);  getitem_201 = None
        
         # File: <eval_with_key>.27:847 in forward, code: reshape_69 = getitem_260.reshape(-1, 192);  getitem_260 = None
        view_77: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_202, [-1, 192]);  getitem_202 = None
        
         # File: <eval_with_key>.27:848 in forward, code: reshape_70 = getitem_261.reshape(-1, 192);  getitem_261 = None
        view_78: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_203, [-1, 192]);  getitem_203 = None
        
         # File: <eval_with_key>.27:849 in forward, code: reshape_71 = getitem_262.reshape(-1, 192);  getitem_262 = None
        view_79: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_204, [-1, 192]);  getitem_204 = None
        
         # File: <eval_with_key>.27:850 in forward, code: reshape_72 = getitem_263.reshape(-1, 192);  getitem_263 = None
        view_80: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_205, [-1, 192]);  getitem_205 = None
        
         # File: <eval_with_key>.27:851 in forward, code: reshape_73 = getitem_264.reshape(-1, 192);  getitem_264 = None
        view_81: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_206, [-1, 192]);  getitem_206 = None
        
         # File: <eval_with_key>.27:852 in forward, code: reshape_74 = getitem_265.reshape(-1, 192);  getitem_265 = None
        view_82: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_207, [-1, 192]);  getitem_207 = None
        
         # File: <eval_with_key>.27:853 in forward, code: reshape_75 = getitem_266.reshape(-1, 192);  getitem_266 = None
        view_83: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_208, [-1, 192]);  getitem_208 = None
        
         # File: <eval_with_key>.27:854 in forward, code: reshape_76 = getitem_267.reshape(-1, 192);  getitem_267 = None
        view_84: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_209, [-1, 192]);  getitem_209 = None
        
         # File: <eval_with_key>.27:855 in forward, code: reshape_77 = getitem_268.reshape(-1, 192);  getitem_268 = None
        view_85: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_210, [-1, 192]);  getitem_210 = None
        
         # File: <eval_with_key>.27:856 in forward, code: reshape_78 = getitem_269.reshape(-1, 192);  getitem_269 = None
        view_86: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_211, [-1, 192]);  getitem_211 = None
        
         # File: <eval_with_key>.27:857 in forward, code: reshape_79 = getitem_270.reshape(-1, 192);  getitem_270 = None
        view_87: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_212, [-1, 192]);  getitem_212 = None
        
         # File: <eval_with_key>.27:858 in forward, code: reshape_80 = getitem_271.reshape(-1, 192);  getitem_271 = None
        view_88: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_213, [-1, 192]);  getitem_213 = None
        
         # File: <eval_with_key>.27:859 in forward, code: reshape_81 = getitem_272.reshape(-1, 192);  getitem_272 = None
        view_89: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_214, [-1, 192]);  getitem_214 = None
        
         # File: <eval_with_key>.27:860 in forward, code: reshape_82 = getitem_273.reshape(-1, 192);  getitem_273 = None
        view_90: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_215, [-1, 192]);  getitem_215 = None
        
         # File: <eval_with_key>.27:861 in forward, code: reshape_83 = getitem_274.reshape(-1, 192);  getitem_274 = None
        view_91: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_216, [-1, 192]);  getitem_216 = None
        
         # File: <eval_with_key>.27:862 in forward, code: reshape_84 = getitem_275.reshape(-1, 192);  getitem_275 = None
        view_92: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_217, [-1, 192]);  getitem_217 = None
        
         # File: <eval_with_key>.27:863 in forward, code: reshape_85 = getitem_276.reshape(-1, 192);  getitem_276 = None
        view_93: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_218, [-1, 192]);  getitem_218 = None
        
         # File: <eval_with_key>.27:864 in forward, code: reshape_86 = getitem_277.reshape(-1, 192);  getitem_277 = None
        view_94: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_219, [-1, 192]);  getitem_219 = None
        
         # File: <eval_with_key>.27:865 in forward, code: reshape_87 = getitem_278.reshape(-1, 192);  getitem_278 = None
        view_95: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_220, [-1, 192]);  getitem_220 = None
        
         # File: <eval_with_key>.27:866 in forward, code: reshape_88 = getitem_279.reshape(-1, 192);  getitem_279 = None
        view_96: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_221, [-1, 192]);  getitem_221 = None
        
         # File: <eval_with_key>.27:867 in forward, code: reshape_89 = getitem_280.reshape(-1, 192);  getitem_280 = None
        view_97: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_222, [-1, 192]);  getitem_222 = None
        
         # File: <eval_with_key>.27:868 in forward, code: reshape_90 = getitem_281.reshape(-1, 192);  getitem_281 = None
        view_98: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_223, [-1, 192]);  getitem_223 = None
        
         # File: <eval_with_key>.27:869 in forward, code: reshape_91 = getitem_282.reshape(-1, 192);  getitem_282 = None
        view_99: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_224, [-1, 192]);  getitem_224 = None
        
         # File: <eval_with_key>.27:870 in forward, code: reshape_92 = getitem_283.reshape(-1, 192);  getitem_283 = None
        view_100: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_225, [-1, 192]);  getitem_225 = None
        
         # File: <eval_with_key>.27:871 in forward, code: reshape_93 = getitem_284.reshape(-1, 192);  getitem_284 = None
        view_101: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_226, [-1, 192]);  getitem_226 = None
        
         # File: <eval_with_key>.27:872 in forward, code: reshape_94 = getitem_285.reshape(-1, 192);  getitem_285 = None
        view_102: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_227, [-1, 192]);  getitem_227 = None
        
         # File: <eval_with_key>.27:873 in forward, code: reshape_95 = getitem_286.reshape(-1, 192);  getitem_286 = None
        view_103: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_228, [-1, 192]);  getitem_228 = None
        
         # File: <eval_with_key>.27:874 in forward, code: reshape_96 = getitem_287.reshape(-1, 192);  getitem_287 = None
        view_104: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_229, [-1, 192]);  getitem_229 = None
        
         # File: <eval_with_key>.27:875 in forward, code: reshape_97 = getitem_288.reshape(-1, 192);  getitem_288 = None
        view_105: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_230, [-1, 192]);  getitem_230 = None
        
         # File: <eval_with_key>.27:876 in forward, code: reshape_98 = getitem_289.reshape(-1, 192);  getitem_289 = None
        view_106: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_231, [-1, 192]);  getitem_231 = None
        
         # File: <eval_with_key>.27:877 in forward, code: reshape_99 = getitem_290.reshape(-1, 192);  getitem_290 = None
        view_107: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_232, [-1, 192]);  getitem_232 = None
        
         # File: <eval_with_key>.27:878 in forward, code: reshape_100 = getitem_291.reshape(-1, 192);  getitem_291 = None
        view_108: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_233, [-1, 192]);  getitem_233 = None
        
         # File: <eval_with_key>.27:879 in forward, code: reshape_101 = getitem_292.reshape(-1, 192);  getitem_292 = None
        view_109: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_234, [-1, 192]);  getitem_234 = None
        
         # File: <eval_with_key>.27:880 in forward, code: reshape_102 = getitem_293.reshape(-1, 192);  getitem_293 = None
        view_110: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_235, [-1, 192]);  getitem_235 = None
        
         # File: <eval_with_key>.27:881 in forward, code: reshape_103 = getitem_294.reshape(-1, 192);  getitem_294 = None
        view_111: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_236, [-1, 192]);  getitem_236 = None
        
         # File: <eval_with_key>.27:882 in forward, code: reshape_104 = getitem_295.reshape(-1, 192);  getitem_295 = None
        view_112: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_237, [-1, 192]);  getitem_237 = None
        
         # File: <eval_with_key>.27:883 in forward, code: reshape_105 = getitem_296.reshape(-1, 192);  getitem_296 = None
        view_113: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_238, [-1, 192]);  getitem_238 = None
        
         # File: <eval_with_key>.27:884 in forward, code: reshape_106 = getitem_297.reshape(-1, 192);  getitem_297 = None
        view_114: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_239, [-1, 192]);  getitem_239 = None
        
         # File: <eval_with_key>.27:885 in forward, code: reshape_107 = getitem_298.reshape(-1, 192);  getitem_298 = None
        view_115: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_240, [-1, 192]);  getitem_240 = None
        
         # File: <eval_with_key>.27:886 in forward, code: reshape_108 = getitem_299.reshape(-1, 192);  getitem_299 = None
        view_116: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_241, [-1, 192]);  getitem_241 = None
        
         # File: <eval_with_key>.27:887 in forward, code: reshape_109 = getitem_300.reshape(-1, 192);  getitem_300 = None
        view_117: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_242, [-1, 192]);  getitem_242 = None
        
         # File: <eval_with_key>.27:888 in forward, code: reshape_110 = getitem_301.reshape(-1, 192);  getitem_301 = None
        view_118: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_243, [-1, 192]);  getitem_243 = None
        
         # File: <eval_with_key>.27:889 in forward, code: reshape_111 = getitem_302.reshape(-1, 192);  getitem_302 = None
        view_119: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_244, [-1, 192]);  getitem_244 = None
        
         # File: <eval_with_key>.27:890 in forward, code: reshape_112 = getitem_303.reshape(-1, 192);  getitem_303 = None
        view_120: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_245, [-1, 192]);  getitem_245 = None
        
         # File: <eval_with_key>.27:891 in forward, code: reshape_113 = getitem_304.reshape(-1, 192);  getitem_304 = None
        view_121: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_246, [-1, 192]);  getitem_246 = None
        
         # File: <eval_with_key>.27:892 in forward, code: reshape_114 = getitem_305.reshape(-1, 192);  getitem_305 = None
        view_122: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_247, [-1, 192]);  getitem_247 = None
        
         # File: <eval_with_key>.27:893 in forward, code: reshape_115 = getitem_306.reshape(-1, 192);  getitem_306 = None
        view_123: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_248, [-1, 192]);  getitem_248 = None
        
         # File: <eval_with_key>.27:894 in forward, code: reshape_116 = getitem_307.reshape(-1, 192);  getitem_307 = None
        view_124: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_249, [-1, 192]);  getitem_249 = None
        
         # File: <eval_with_key>.27:895 in forward, code: reshape_117 = getitem_308.reshape(-1, 192);  getitem_308 = None
        view_125: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_250, [-1, 192]);  getitem_250 = None
        
         # File: <eval_with_key>.27:896 in forward, code: reshape_118 = getitem_309.reshape(-1, 192);  getitem_309 = None
        view_126: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_251, [-1, 192]);  getitem_251 = None
        
         # File: <eval_with_key>.27:897 in forward, code: reshape_119 = getitem_310.reshape(-1, 192);  getitem_310 = None
        view_127: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_252, [-1, 192]);  getitem_252 = None
        
         # File: <eval_with_key>.27:898 in forward, code: reshape_120 = getitem_311.reshape(-1, 192);  getitem_311 = None
        view_128: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_253, [-1, 192]);  getitem_253 = None
        
         # File: <eval_with_key>.27:899 in forward, code: reshape_121 = getitem_312.reshape(-1, 192);  getitem_312 = None
        view_129: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_254, [-1, 192]);  getitem_254 = None
        
         # File: <eval_with_key>.27:900 in forward, code: reshape_122 = getitem_313.reshape(-1, 192);  getitem_313 = None
        view_130: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_255, [-1, 192]);  getitem_255 = None
        
         # File: <eval_with_key>.27:901 in forward, code: reshape_123 = getitem_314.reshape(-1, 192);  getitem_314 = None
        view_131: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_256, [-1, 192]);  getitem_256 = None
        
         # File: <eval_with_key>.27:902 in forward, code: reshape_124 = getitem_315.reshape(-1, 192);  getitem_315 = None
        view_132: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_257, [-1, 192]);  getitem_257 = None
        
         # File: <eval_with_key>.27:903 in forward, code: reshape_125 = getitem_316.reshape(-1, 192);  getitem_316 = None
        view_133: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_258, [-1, 192]);  getitem_258 = None
        
         # File: <eval_with_key>.27:904 in forward, code: reshape_126 = getitem_317.reshape(-1, 192);  getitem_317 = None
        view_134: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_259, [-1, 192]);  getitem_259 = None
        
         # File: <eval_with_key>.27:905 in forward, code: reshape_127 = getitem_318.reshape(-1, 192);  getitem_318 = None
        view_135: "f16[s0, 192][192, 1]cuda:0" = torch.ops.aten.view.default(getitem_260, [-1, 192]);  getitem_260 = None
        
         # File: <eval_with_key>.27:906 in forward, code: cat_41 = torch.cat(tensors = [getitem_42, getitem_95, getitem_41, getitem_94, getitem_40, getitem_93, getitem_39, getitem_92, getitem_38, getitem_91, getitem_37, getitem_90, getitem_36, getitem_89, getitem_35, getitem_88, getitem_34, getitem_87, getitem_33, getitem_86, getitem_32, getitem_85, getitem_31, getitem_84, getitem_30, getitem_83, getitem_29, getitem_82, getitem_28, getitem_81, getitem_27, getitem_80, getitem_26, getitem_79, getitem_25, getitem_78, getitem_24, getitem_77, getitem_23, getitem_76, getitem_22, getitem_75, getitem_21, getitem_74, getitem_20, getitem_73, getitem_19, getitem_72, getitem_18, getitem_71, getitem_17, getitem_70, getitem_16, getitem_69, getitem_15, getitem_68, getitem_14, getitem_67, getitem_13, getitem_66, getitem_12, getitem_65, getitem_11, getitem_64, getitem_10, getitem_63, getitem_9, getitem_62, linear_45, linear_46, linear_47, linear_48, linear_49, linear_50, linear_41, layer_norm_4, layer_norm_5, layer_norm, layer_norm_6, layer_norm_7, layer_norm_8, layer_norm_1, layer_norm_9, layer_norm_10, layer_norm_11, layer_norm_2, layer_norm_3, layer_norm_12, layer_norm_13, layer_norm_14, layer_norm_15, layer_norm_16, layer_norm_17, layer_norm_18, layer_norm_19, layer_norm_20, layer_norm_21, layer_norm_22, layer_norm_23, layer_norm_24, layer_norm_25, layer_norm_26, layer_norm_27, layer_norm_28, layer_norm_29, layer_norm_30, layer_norm_31, layer_norm_32, layer_norm_33, layer_norm_34, layer_norm_35, reshape_64, reshape_65, reshape_66, reshape_67, reshape_68, reshape_69, reshape_70, reshape_71, reshape_72, reshape_73, reshape_74, reshape_75, reshape_76, reshape_77, reshape_78, reshape_79, reshape_80, reshape_81, reshape_82, reshape_83, reshape_84, reshape_85, reshape_86, reshape_87, reshape_88, reshape_89, reshape_90, reshape_91, reshape_92, reshape_93, reshape_94, reshape_95, reshape_96, reshape_97, reshape_98, reshape_99, reshape_100, reshape_101, reshape_102, reshape_103, reshape_104, reshape_105, reshape_106, reshape_107, reshape_108, reshape_109, reshape_110, reshape_111, reshape_112, reshape_113, reshape_114, reshape_115, reshape_116, reshape_117, reshape_118, reshape_119, reshape_120, reshape_121, reshape_122, reshape_123, reshape_124, reshape_125, reshape_126, reshape_127], dim = 1);  getitem_42 = getitem_95 = getitem_41 = getitem_94 = getitem_40 = getitem_93 = getitem_39 = getitem_92 = getitem_38 = getitem_91 = getitem_37 = getitem_90 = getitem_36 = getitem_89 = getitem_35 = getitem_88 = getitem_34 = getitem_87 = getitem_33 = getitem_86 = getitem_32 = getitem_85 = getitem_31 = getitem_84 = getitem_30 = getitem_83 = getitem_29 = getitem_82 = getitem_28 = getitem_81 = getitem_27 = getitem_80 = getitem_26 = getitem_79 = getitem_25 = getitem_78 = getitem_24 = getitem_77 = getitem_23 = getitem_76 = getitem_22 = getitem_75 = getitem_21 = getitem_74 = getitem_20 = getitem_73 = getitem_19 = getitem_72 = getitem_18 = getitem_71 = getitem_17 = getitem_70 = getitem_16 = getitem_69 = getitem_15 = getitem_68 = getitem_14 = getitem_67 = getitem_13 = getitem_66 = getitem_12 = getitem_65 = getitem_11 = getitem_64 = getitem_10 = getitem_63 = getitem_9 = getitem_62 = linear_41 = layer_norm_4 = layer_norm_5 = layer_norm = layer_norm_6 = layer_norm_7 = layer_norm_8 = layer_norm_1 = layer_norm_9 = layer_norm_10 = layer_norm_11 = layer_norm_2 = layer_norm_3 = layer_norm_12 = layer_norm_13 = layer_norm_14 = layer_norm_15 = layer_norm_16 = layer_norm_17 = layer_norm_18 = layer_norm_19 = layer_norm_20 = layer_norm_21 = layer_norm_22 = layer_norm_23 = layer_norm_24 = layer_norm_25 = layer_norm_26 = layer_norm_27 = layer_norm_28 = layer_norm_29 = layer_norm_30 = layer_norm_31 = layer_norm_32 = layer_norm_33 = layer_norm_34 = layer_norm_35 = reshape_64 = reshape_65 = reshape_66 = reshape_67 = reshape_68 = reshape_69 = reshape_70 = reshape_71 = reshape_72 = reshape_73 = reshape_74 = reshape_75 = reshape_76 = reshape_77 = reshape_78 = reshape_79 = reshape_80 = reshape_81 = reshape_82 = reshape_83 = reshape_84 = reshape_85 = reshape_86 = reshape_87 = reshape_88 = reshape_89 = reshape_90 = reshape_91 = reshape_92 = reshape_93 = reshape_94 = reshape_95 = reshape_96 = reshape_97 = reshape_98 = reshape_99 = reshape_100 = reshape_101 = reshape_102 = reshape_103 = reshape_104 = reshape_105 = reshape_106 = reshape_107 = reshape_108 = reshape_109 = reshape_110 = reshape_111 = reshape_112 = reshape_113 = reshape_114 = reshape_115 = reshape_116 = reshape_117 = reshape_118 = reshape_119 = reshape_120 = reshape_121 = reshape_122 = reshape_123 = reshape_124 = reshape_125 = reshape_126 = reshape_127 = None
        cat_5: "f16[s0, 87936][87936, 1]cuda:0" = torch.ops.aten.cat.default([getitem_19, getitem_61, getitem_20, getitem_62, getitem_21, getitem_63, getitem_22, getitem_64, getitem_23, getitem_65, getitem_24, getitem_66, getitem_25, getitem_67, getitem_26, getitem_68, getitem_27, getitem_69, getitem_28, getitem_70, getitem_29, getitem_71, getitem_30, getitem_72, getitem_31, getitem_73, getitem_32, getitem_74, getitem_33, getitem_75, getitem_34, getitem_76, getitem_35, getitem_77, getitem_36, getitem_78, getitem_37, getitem_79, getitem_38, getitem_80, getitem_39, getitem_81, getitem_40, getitem_82, getitem_41, getitem_83, getitem_42, getitem_84, getitem_43, getitem_85, getitem_44, getitem_86, getitem_45, getitem_87, getitem_46, getitem_88, getitem_47, getitem_89, getitem_48, getitem_90, getitem_49, getitem_91, getitem_50, getitem_92, getitem_51, getitem_93, getitem_52, getitem_94, linear_49, linear_50, linear_51, linear_52, linear_53, linear_54, linear_41, layer_norm_4, layer_norm_5, layer_norm, layer_norm_6, layer_norm_7, layer_norm_8, layer_norm_1, layer_norm_9, layer_norm_10, layer_norm_11, layer_norm_2, layer_norm_3, layer_norm_12, layer_norm_13, layer_norm_14, layer_norm_15, layer_norm_16, layer_norm_17, layer_norm_18, layer_norm_19, layer_norm_20, layer_norm_21, layer_norm_22, layer_norm_23, layer_norm_24, layer_norm_25, layer_norm_26, layer_norm_27, layer_norm_28, layer_norm_29, layer_norm_30, layer_norm_31, layer_norm_32, layer_norm_33, layer_norm_34, layer_norm_35, view_72, view_73, view_74, view_75, view_76, view_77, view_78, view_79, view_80, view_81, view_82, view_83, view_84, view_85, view_86, view_87, view_88, view_89, view_90, view_91, view_92, view_93, view_94, view_95, view_96, view_97, view_98, view_99, view_100, view_101, view_102, view_103, view_104, view_105, view_106, view_107, view_108, view_109, view_110, view_111, view_112, view_113, view_114, view_115, view_116, view_117, view_118, view_119, view_120, view_121, view_122, view_123, view_124, view_125, view_126, view_127, view_128, view_129, view_130, view_131, view_132, view_133, view_134, view_135], 1);  getitem_19 = getitem_61 = getitem_20 = getitem_62 = getitem_21 = getitem_63 = getitem_22 = getitem_64 = getitem_23 = getitem_65 = getitem_24 = getitem_66 = getitem_25 = getitem_67 = getitem_26 = getitem_68 = getitem_27 = getitem_69 = getitem_28 = getitem_70 = getitem_29 = getitem_71 = getitem_30 = getitem_72 = getitem_31 = getitem_73 = getitem_32 = getitem_74 = getitem_33 = getitem_75 = getitem_34 = getitem_76 = getitem_35 = getitem_77 = getitem_36 = getitem_78 = getitem_37 = getitem_79 = getitem_38 = getitem_80 = getitem_39 = getitem_81 = getitem_40 = getitem_82 = getitem_41 = getitem_83 = getitem_42 = getitem_84 = getitem_43 = getitem_85 = getitem_44 = getitem_86 = getitem_45 = getitem_87 = getitem_46 = getitem_88 = getitem_47 = getitem_89 = getitem_48 = getitem_90 = getitem_49 = getitem_91 = getitem_50 = getitem_92 = getitem_51 = getitem_93 = getitem_52 = getitem_94 = linear_41 = layer_norm_4 = layer_norm_5 = layer_norm = layer_norm_6 = layer_norm_7 = layer_norm_8 = layer_norm_1 = layer_norm_9 = layer_norm_10 = layer_norm_11 = layer_norm_2 = layer_norm_3 = layer_norm_12 = layer_norm_13 = layer_norm_14 = layer_norm_15 = layer_norm_16 = layer_norm_17 = layer_norm_18 = layer_norm_19 = layer_norm_20 = layer_norm_21 = layer_norm_22 = layer_norm_23 = layer_norm_24 = layer_norm_25 = layer_norm_26 = layer_norm_27 = layer_norm_28 = layer_norm_29 = layer_norm_30 = layer_norm_31 = layer_norm_32 = layer_norm_33 = layer_norm_34 = layer_norm_35 = view_72 = view_73 = view_74 = view_75 = view_76 = view_77 = view_78 = view_79 = view_80 = view_81 = view_82 = view_83 = view_84 = view_85 = view_86 = view_87 = view_88 = view_89 = view_90 = view_91 = view_92 = view_93 = view_94 = view_95 = view_96 = view_97 = view_98 = view_99 = view_100 = view_101 = view_102 = view_103 = view_104 = view_105 = view_106 = view_107 = view_108 = view_109 = view_110 = view_111 = view_112 = view_113 = view_114 = view_115 = view_116 = view_117 = view_118 = view_119 = view_120 = view_121 = view_122 = view_123 = view_124 = view_125 = view_126 = view_127 = view_128 = view_129 = view_130 = view_131 = view_132 = view_133 = view_134 = view_135 = None
        
         # File: <eval_with_key>.27:907 in forward, code: reshape_128 = torch.reshape(cat_41, [-1, 458, 192]);  cat_41 = None
        view_136: "f16[s0, 458, 192][87936, 192, 1]cuda:0" = torch.ops.aten.view.default(cat_5, [-1, 458, 192]);  cat_5 = None
        
         # File: <eval_with_key>.27:909 in forward, code: matmul = torch.matmul(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w, reshape_128);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w = None
        matmul: "f16[s0, 174, 192][33408, 192, 1]cuda:0" = torch.ops.aten.matmul.default(submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w, view_136);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w = None
        
         # File: <eval_with_key>.27:911 in forward, code: add_18 = matmul + main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b;  matmul = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b = None
        add_1891: "f16[s0, 174, 192][33408, 192, 1]cuda:0" = torch.ops.aten.add.Tensor(matmul, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b);  matmul = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b = None
        
         # File: <eval_with_key>.27:912 in forward, code: permute_8 = reshape_128.permute(0, 2, 1)
        permute_8: "f16[s0, 192, 458][87936, 1, 192]cuda:0" = torch.ops.aten.permute.default(view_136, [0, 2, 1])
        
         # File: <eval_with_key>.27:917 in forward, code: layer_norm_40 = torch.nn.functional.layer_norm(add_18, getitem_319, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b, eps = 1e-05);  add_18 = getitem_319 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b = None
        layer_norm_46: "f16[s0, 174, 192][33408, 192, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_1891, [192], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b);  add_1891 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b = None
        
         # File: <eval_with_key>.27:918 in forward, code: split = torch.functional.split(layer_norm_40, [48, 24, 102], dim = 1);  layer_norm_40 = None
        split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(layer_norm_46, [48, 24, 102], 1);  layer_norm_46 = None
        getitem_261: "f16[s0, 48, 192][33408, 192, 1]cuda:0" = split_with_sizes_4[0]
        getitem_262: "f16[s0, 24, 192][33408, 192, 1]cuda:0" = split_with_sizes_4[1]
        getitem_263: "f16[s0, 102, 192][33408, 192, 1]cuda:0" = split_with_sizes_4[2];  split_with_sizes_4 = None
        
         # File: <eval_with_key>.27:922 in forward, code: reshape_129 = getitem_320.reshape(-1, 9216);  getitem_320 = None
        view_137: "f16[s0, 9216][33408, 1]cuda:0" = torch.ops.aten.view.default(getitem_261, [sym_size_int_12, 9216]);  getitem_261 = None
        
         # File: <eval_with_key>.27:923 in forward, code: reshape_130 = torch.reshape(getitem_321, (-1, 4608));  getitem_321 = None
        view_138: "f16[s0, 4608][33408, 1]cuda:0" = torch.ops.aten.view.default(getitem_262, [sym_size_int_12, 4608]);  getitem_262 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_59: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_138, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_60: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_138, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_bias);  view_138 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_47: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_59, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_48: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_60, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_5: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_47);  layer_norm_47 = None
        
         # File: <eval_with_key>.27:929 in forward, code: mul_4 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_1 = None
        mul_1509: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_59, sigmoid_5);  linear_59 = sigmoid_5 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_6: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_48);  layer_norm_48 = None
        
         # File: <eval_with_key>.27:931 in forward, code: mul_5 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_1 = None
        mul_1514: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_60, sigmoid_6);  linear_60 = sigmoid_6 = None
        
         # File: <eval_with_key>.27:934 in forward, code: layer_norm_41 = torch.nn.functional.layer_norm(mul_4, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_4 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        layer_norm_49: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1509, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b);  mul_1509 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:937 in forward, code: layer_norm_42 = torch.nn.functional.layer_norm(mul_5, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_5 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        layer_norm_50: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1514, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b);  mul_1514 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_61: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_49, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias);  layer_norm_49 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_62: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_50, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias);  layer_norm_50 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_51: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_61, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_52: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_62, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_7: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_51);  layer_norm_51 = None
        
         # File: <eval_with_key>.27:943 in forward, code: mul_6 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_1 = None
        mul_1531: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_61, sigmoid_7);  linear_61 = sigmoid_7 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_8: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_52);  layer_norm_52 = None
        
         # File: <eval_with_key>.27:945 in forward, code: mul_7 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_1 = None
        mul_1536: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_62, sigmoid_8);  linear_62 = sigmoid_8 = None
        
         # File: <eval_with_key>.27:948 in forward, code: layer_norm_43 = torch.nn.functional.layer_norm(mul_6, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_6 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        layer_norm_53: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1531, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b);  mul_1531 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        
         # File: <eval_with_key>.27:951 in forward, code: layer_norm_44 = torch.nn.functional.layer_norm(mul_7, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_7 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        layer_norm_54: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1536, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b);  mul_1536 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_63: "f16[s0, 21984][21984, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_53, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_53 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_64: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_54, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_54 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:956 in forward, code: layer_norm_45 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0, (21984,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_55: "f16[s0, 21984][21984, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_63, [21984], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b);  linear_63 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:959 in forward, code: layer_norm_46 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0, (9216,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_56: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_64, [9216], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b);  linear_64 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:960 in forward, code: reshape_131 = torch.reshape(layer_norm_45, (-1, 458, 48));  layer_norm_45 = None
        view_139: "f16[s0, 458, 48][21984, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_55, [-1, 458, 48]);  layer_norm_55 = None
        
         # File: <eval_with_key>.27:961 in forward, code: reshape_132 = torch.reshape(layer_norm_46, (-1, 192, 48));  layer_norm_46 = None
        view_140: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_56, [-1, 192, 48]);  layer_norm_56 = None
        
         # File: <eval_with_key>.27:962 in forward, code: bmm = torch.bmm(permute_8, reshape_131);  permute_8 = reshape_131 = None
        bmm: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.bmm.default(permute_8, view_139);  permute_8 = view_139 = None
        
         # File: <eval_with_key>.27:963 in forward, code: add_19 = reshape_132 + bmm;  reshape_132 = bmm = None
        add_2006: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.add.Tensor(view_140, bmm);  view_140 = bmm = None
        
         # File: <eval_with_key>.27:966 in forward, code: layer_norm_47 = torch.nn.functional.layer_norm(add_19, (48,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b, eps = 1e-05);  add_19 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b = None
        layer_norm_57: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2006, [48], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b);  add_2006 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b = None
        
         # File: <eval_with_key>.27:967 in forward, code: bmm_1 = torch.bmm(reshape_128, layer_norm_47);  reshape_128 = layer_norm_47 = None
        bmm_1: "f16[s0, 458, 48][21984, 48, 1]cuda:0" = torch.ops.aten.bmm.default(view_136, layer_norm_57);  view_136 = layer_norm_57 = None
        
         # File: <eval_with_key>.27:968 in forward, code: flatten = torch.flatten(bmm_1, start_dim = -2);  bmm_1 = None
        view_141: "f16[s0, 21984][21984, 1]cuda:0" = torch.ops.aten.view.default(bmm_1, [sym_size_int_12, 21984]);  bmm_1 = None
        
         # File: <eval_with_key>.27:973 in forward, code: layer_norm_48 = torch.nn.functional.layer_norm(flatten, getitem_323, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b, eps = 1e-05);  flatten = getitem_323 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b = None
        layer_norm_58: "f16[s0, 21984][21984, 1]cuda:0" = torch.ops.aten.layer_norm.default(view_141, [21984], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b);  view_141 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_65: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_58, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias);  layer_norm_58 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_59: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_65, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_9: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_59);  layer_norm_59 = None
        
         # File: <eval_with_key>.27:977 in forward, code: mul_8 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_1 = None
        mul_1580: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_65, sigmoid_9);  linear_65 = sigmoid_9 = None
        
         # File: <eval_with_key>.27:980 in forward, code: layer_norm_49 = torch.nn.functional.layer_norm(mul_8, (2048,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_8 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        layer_norm_60: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1580, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b);  mul_1580 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:981 in forward, code: cat_42 = torch.cat([mul_3, mul_2, layer_norm_49], dim = 1);  layer_norm_49 = None
        cat_6: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.cat.default([mul_570, mul_567, layer_norm_60], 1);  layer_norm_60 = None
        
         # File: <eval_with_key>.27:986 in forward, code: layer_norm_50 = torch.nn.functional.layer_norm(cat_42, getitem_324, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b, eps = 1e-05);  cat_42 = getitem_324 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b = None
        layer_norm_61: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(cat_6, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b);  cat_6 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_66: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_61, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:990 in forward, code: layer_norm_51 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0, (384,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        layer_norm_62: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_66, [384], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b);  linear_66 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_67: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_62, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias);  layer_norm_62 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:994 in forward, code: layer_norm_52 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0, (6354,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        layer_norm_63: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_67, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b);  linear_67 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:995 in forward, code: addcmul = torch.addcmul(input = layer_norm_50, tensor1 = layer_norm_50, tensor2 = layer_norm_52, value = 1.0);  layer_norm_50 = layer_norm_52 = None
        addcmul: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.addcmul.default(layer_norm_61, layer_norm_61, layer_norm_63, value = 1.0);  layer_norm_61 = layer_norm_63 = None
        
         # File: <eval_with_key>.27:1000 in forward, code: layer_norm_53 = torch.nn.functional.layer_norm(addcmul, getitem_325, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b, eps = 1e-05);  addcmul = getitem_325 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b = None
        layer_norm_64: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(addcmul, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b);  addcmul = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_68: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_64, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias);  layer_norm_64 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_65: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_68, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_10: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_65);  layer_norm_65 = None
        
         # File: <eval_with_key>.27:1004 in forward, code: mul_9 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_1 = None
        mul_1607: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_68, sigmoid_10);  linear_68 = sigmoid_10 = None
        
         # File: <eval_with_key>.27:1007 in forward, code: layer_norm_54 = torch.nn.functional.layer_norm(mul_9, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_9 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b = None
        layer_norm_66: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1607, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b);  mul_1607 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_69: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_66, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_67: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_69, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_11: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_67);  layer_norm_67 = None
        
         # File: <eval_with_key>.27:1011 in forward, code: mul_10 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_1 = None
        mul_1618: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_69, sigmoid_11);  linear_69 = sigmoid_11 = None
        
         # File: <eval_with_key>.27:1014 in forward, code: layer_norm_55 = torch.nn.functional.layer_norm(mul_10, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b, eps = 1e-05);  mul_10 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b = None
        layer_norm_68: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1618, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b);  mul_1618 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_70: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_68, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias);  layer_norm_68 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1018 in forward, code: layer_norm_56 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b = None
        layer_norm_69: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_70, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b);  linear_70 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1019 in forward, code: add_20 = layer_norm_54 + layer_norm_56;  layer_norm_56 = None
        add_2100: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_66, layer_norm_69);  layer_norm_69 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_70: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2100, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_12: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_70);  layer_norm_70 = None
        
         # File: <eval_with_key>.27:1022 in forward, code: mul_11 = add_20 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_1;  add_20 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_1 = None
        mul_1633: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2100, sigmoid_12);  add_2100 = sigmoid_12 = None
        
         # File: <eval_with_key>.27:1023 in forward, code: add_21 = layer_norm_54 + mul_11;  layer_norm_54 = None
        add_2113: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_66, mul_1633);  layer_norm_66 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_71: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(mul_1633, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias);  mul_1633 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_71: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_71, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_13: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_71);  layer_norm_71 = None
        
         # File: <eval_with_key>.27:1027 in forward, code: mul_12 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_1 = None
        mul_1644: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_71, sigmoid_13);  linear_71 = sigmoid_13 = None
        
         # File: <eval_with_key>.27:1030 in forward, code: layer_norm_57 = torch.nn.functional.layer_norm(mul_12, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b, eps = 1e-05);  mul_12 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b = None
        layer_norm_72: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1644, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b);  mul_1644 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_72: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_72, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias);  layer_norm_72 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1034 in forward, code: layer_norm_58 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b = None
        layer_norm_73: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_72, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b);  linear_72 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1035 in forward, code: add_22 = add_21 + layer_norm_58;  add_21 = layer_norm_58 = None
        add_2138: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2113, layer_norm_73);  add_2113 = layer_norm_73 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_74: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2138, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_14: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_74);  layer_norm_74 = None
        
         # File: <eval_with_key>.27:1038 in forward, code: mul_13 = add_22 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_1;  add_22 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_1 = None
        mul_1659: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2138, sigmoid_14);  add_2138 = sigmoid_14 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_73: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.linear.default(mul_1659, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias);  mul_1659 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1040 in forward, code: cat_43 = torch.cat([linear_45, linear_46, linear_47, linear_48, linear_49, linear_50, reshape_129, main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0], dim = 1);  reshape_129 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0 = None
        cat_7: "f16[s0, 19584][19584, 1]cuda:0" = torch.ops.aten.cat.default([linear_49, linear_50, linear_51, linear_52, linear_53, linear_54, view_137, linear_73], 1);  view_137 = linear_73 = None
        
         # File: <eval_with_key>.27:1041 in forward, code: _reshape_to_3d = _torch_package_1_legokit_backbones_dhen_prototype__reshape_to_3d(tensor = cat_43, emb_num = 102, emb_dim = 192);  cat_43 = None
        view_142: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.view.default(cat_7, [sym_size_int_12, 102, 192]);  cat_7 = None
        
         # File: <eval_with_key>.27:1042 in forward, code: add_23 = getitem_322 + _reshape_to_3d;  getitem_322 = _reshape_to_3d = None
        add_2161: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_263, view_142);  getitem_263 = view_142 = None
        
         # File: <eval_with_key>.27:1047 in forward, code: layer_norm_59 = torch.nn.functional.layer_norm(add_23, getitem_326, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b, eps = 1e-05);  add_23 = getitem_326 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b = None
        layer_norm_75: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2161, [192], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b);  add_2161 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b = None
        
         # File: <eval_with_key>.27:1049 in forward, code: matmul_1 = torch.matmul(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w, layer_norm_59);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w = None
        matmul_1: "f16[s0, 72, 192][13824, 192, 1]cuda:0" = torch.ops.aten.matmul.default(submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w, layer_norm_75);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w = None
        
         # File: <eval_with_key>.27:1051 in forward, code: add_24 = matmul_1 + main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b;  matmul_1 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b = None
        add_2174: "f16[s0, 72, 192][13824, 192, 1]cuda:0" = torch.ops.aten.add.Tensor(matmul_1, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b);  matmul_1 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b = None
        
         # File: <eval_with_key>.27:1052 in forward, code: permute_9 = layer_norm_59.permute(0, 2, 1)
        permute_9: "f16[s0, 192, 102][19584, 1, 192]cuda:0" = torch.ops.aten.permute.default(layer_norm_75, [0, 2, 1])
        
         # File: <eval_with_key>.27:1057 in forward, code: layer_norm_60 = torch.nn.functional.layer_norm(add_24, getitem_327, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b, eps = 1e-05);  add_24 = getitem_327 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b = None
        layer_norm_76: "f16[s0, 72, 192][13824, 192, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2174, [192], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b);  add_2174 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b = None
        
         # File: <eval_with_key>.27:1058 in forward, code: split_1 = torch.functional.split(layer_norm_60, [48, 24], dim = 1);  layer_norm_60 = None
        split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(layer_norm_76, [48, 24], 1);  layer_norm_76 = None
        getitem_264: "f16[s0, 48, 192][13824, 192, 1]cuda:0" = split_with_sizes_5[0]
        getitem_265: "f16[s0, 24, 192][13824, 192, 1]cuda:0" = split_with_sizes_5[1];  split_with_sizes_5 = None
        
         # File: <eval_with_key>.27:1061 in forward, code: reshape_133 = getitem_328.reshape(-1, 9216);  getitem_328 = None
        view_143: "f16[s0, 9216][13824, 1]cuda:0" = torch.ops.aten.view.default(getitem_264, [sym_size_int_12, 9216]);  getitem_264 = None
        
         # File: <eval_with_key>.27:1062 in forward, code: reshape_134 = torch.reshape(getitem_329, (-1, 4608));  getitem_329 = None
        view_144: "f16[s0, 4608][13824, 1]cuda:0" = torch.ops.aten.view.default(getitem_265, [sym_size_int_12, 4608]);  getitem_265 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_74: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_144, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_75: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_144, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_bias);  view_144 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_77: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_74, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_78: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_75, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_15: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_77);  layer_norm_77 = None
        
         # File: <eval_with_key>.27:1068 in forward, code: mul_14 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_1 = None
        mul_1708: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_74, sigmoid_15);  linear_74 = sigmoid_15 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_16: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_78);  layer_norm_78 = None
        
         # File: <eval_with_key>.27:1070 in forward, code: mul_15 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_1 = None
        mul_1713: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_75, sigmoid_16);  linear_75 = sigmoid_16 = None
        
         # File: <eval_with_key>.27:1073 in forward, code: layer_norm_61 = torch.nn.functional.layer_norm(mul_14, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_14 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        layer_norm_79: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1708, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b);  mul_1708 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:1076 in forward, code: layer_norm_62 = torch.nn.functional.layer_norm(mul_15, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_15 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        layer_norm_80: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1713, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b);  mul_1713 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_76: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_79, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias);  layer_norm_79 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_77: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_80, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias);  layer_norm_80 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_81: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_76, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_82: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_77, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_17: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_81);  layer_norm_81 = None
        
         # File: <eval_with_key>.27:1082 in forward, code: mul_16 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_1 = None
        mul_1730: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_76, sigmoid_17);  linear_76 = sigmoid_17 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_18: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_82);  layer_norm_82 = None
        
         # File: <eval_with_key>.27:1084 in forward, code: mul_17 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_1 = None
        mul_1735: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_77, sigmoid_18);  linear_77 = sigmoid_18 = None
        
         # File: <eval_with_key>.27:1087 in forward, code: layer_norm_63 = torch.nn.functional.layer_norm(mul_16, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_16 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        layer_norm_83: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1730, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b);  mul_1730 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        
         # File: <eval_with_key>.27:1090 in forward, code: layer_norm_64 = torch.nn.functional.layer_norm(mul_17, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_17 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        layer_norm_84: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1735, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b);  mul_1735 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_78: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_83, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_83 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_79: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_84, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_84 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1095 in forward, code: layer_norm_65 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0, (4896,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_85: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_78, [4896], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b);  linear_78 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1098 in forward, code: layer_norm_66 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0, (9216,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_86: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_79, [9216], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b);  linear_79 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1099 in forward, code: reshape_135 = torch.reshape(layer_norm_65, (-1, 102, 48));  layer_norm_65 = None
        view_145: "f16[s0, 102, 48][4896, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_85, [-1, 102, 48]);  layer_norm_85 = None
        
         # File: <eval_with_key>.27:1100 in forward, code: reshape_136 = torch.reshape(layer_norm_66, (-1, 192, 48));  layer_norm_66 = None
        view_146: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_86, [-1, 192, 48]);  layer_norm_86 = None
        
         # File: <eval_with_key>.27:1101 in forward, code: bmm_2 = torch.bmm(permute_9, reshape_135);  permute_9 = reshape_135 = None
        bmm_2: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.bmm.default(permute_9, view_145);  permute_9 = view_145 = None
        
         # File: <eval_with_key>.27:1102 in forward, code: add_25 = reshape_136 + bmm_2;  reshape_136 = bmm_2 = None
        add_2285: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.add.Tensor(view_146, bmm_2);  view_146 = bmm_2 = None
        
         # File: <eval_with_key>.27:1105 in forward, code: layer_norm_67 = torch.nn.functional.layer_norm(add_25, (48,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b, eps = 1e-05);  add_25 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b = None
        layer_norm_87: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2285, [48], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b);  add_2285 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b = None
        
         # File: <eval_with_key>.27:1106 in forward, code: bmm_3 = torch.bmm(layer_norm_59, layer_norm_67);  layer_norm_67 = None
        bmm_3: "f16[s0, 102, 48][4896, 48, 1]cuda:0" = torch.ops.aten.bmm.default(layer_norm_75, layer_norm_87);  layer_norm_87 = None
        
         # File: <eval_with_key>.27:1107 in forward, code: flatten_1 = torch.flatten(bmm_3, start_dim = -2);  bmm_3 = None
        view_147: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.view.default(bmm_3, [sym_size_int_12, 4896]);  bmm_3 = None
        
         # File: <eval_with_key>.27:1112 in forward, code: layer_norm_68 = torch.nn.functional.layer_norm(flatten_1, getitem_330, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b, eps = 1e-05);  flatten_1 = getitem_330 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b = None
        layer_norm_88: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.layer_norm.default(view_147, [4896], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b);  view_147 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_80: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_88, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias);  layer_norm_88 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_89: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_80, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_19: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_89);  layer_norm_89 = None
        
         # File: <eval_with_key>.27:1116 in forward, code: mul_18 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_1 = None
        mul_1779: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_80, sigmoid_19);  linear_80 = sigmoid_19 = None
        
         # File: <eval_with_key>.27:1119 in forward, code: layer_norm_69 = torch.nn.functional.layer_norm(mul_18, (2048,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_18 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        layer_norm_90: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1779, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b);  mul_1779 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:1120 in forward, code: cat_44 = torch.cat([mul_3, mul_2, layer_norm_69], dim = 1);  layer_norm_69 = None
        cat_8: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.cat.default([mul_570, mul_567, layer_norm_90], 1);  layer_norm_90 = None
        
         # File: <eval_with_key>.27:1125 in forward, code: layer_norm_70 = torch.nn.functional.layer_norm(cat_44, getitem_331, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b, eps = 1e-05);  cat_44 = getitem_331 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b = None
        layer_norm_91: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(cat_8, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b);  cat_8 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_81: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_91, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1129 in forward, code: layer_norm_71 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0, (384,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        layer_norm_92: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_81, [384], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b);  linear_81 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_82: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_92, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias);  layer_norm_92 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1133 in forward, code: layer_norm_72 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0, (6354,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        layer_norm_93: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_82, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b);  linear_82 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1134 in forward, code: addcmul_1 = torch.addcmul(input = layer_norm_70, tensor1 = layer_norm_70, tensor2 = layer_norm_72, value = 1.0);  layer_norm_70 = layer_norm_72 = None
        addcmul_1: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.addcmul.default(layer_norm_91, layer_norm_91, layer_norm_93, value = 1.0);  layer_norm_91 = layer_norm_93 = None
        
         # File: <eval_with_key>.27:1139 in forward, code: layer_norm_73 = torch.nn.functional.layer_norm(addcmul_1, getitem_332, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b, eps = 1e-05);  addcmul_1 = getitem_332 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b = None
        layer_norm_94: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(addcmul_1, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b);  addcmul_1 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_83: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_94, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias);  layer_norm_94 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_95: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_83, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_20: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_95);  layer_norm_95 = None
        
         # File: <eval_with_key>.27:1143 in forward, code: mul_19 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_1 = None
        mul_1806: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_83, sigmoid_20);  linear_83 = sigmoid_20 = None
        
         # File: <eval_with_key>.27:1146 in forward, code: layer_norm_74 = torch.nn.functional.layer_norm(mul_19, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_19 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b = None
        layer_norm_96: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1806, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b);  mul_1806 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_84: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_96, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_97: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_84, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_21: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_97);  layer_norm_97 = None
        
         # File: <eval_with_key>.27:1150 in forward, code: mul_20 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_1 = None
        mul_1817: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_84, sigmoid_21);  linear_84 = sigmoid_21 = None
        
         # File: <eval_with_key>.27:1153 in forward, code: layer_norm_75 = torch.nn.functional.layer_norm(mul_20, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b, eps = 1e-05);  mul_20 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b = None
        layer_norm_98: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1817, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b);  mul_1817 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_85: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_98, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias);  layer_norm_98 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1157 in forward, code: layer_norm_76 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b = None
        layer_norm_99: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_85, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b);  linear_85 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1158 in forward, code: add_26 = layer_norm_74 + layer_norm_76;  layer_norm_76 = None
        add_2379: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_96, layer_norm_99);  layer_norm_99 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_100: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2379, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_22: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_100);  layer_norm_100 = None
        
         # File: <eval_with_key>.27:1161 in forward, code: mul_21 = add_26 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_1;  add_26 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_1 = None
        mul_1832: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2379, sigmoid_22);  add_2379 = sigmoid_22 = None
        
         # File: <eval_with_key>.27:1162 in forward, code: add_27 = layer_norm_74 + mul_21;  layer_norm_74 = None
        add_2392: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_96, mul_1832);  layer_norm_96 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_86: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(mul_1832, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias);  mul_1832 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_101: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_86, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_23: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_101);  layer_norm_101 = None
        
         # File: <eval_with_key>.27:1166 in forward, code: mul_22 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_1 = None
        mul_1843: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_86, sigmoid_23);  linear_86 = sigmoid_23 = None
        
         # File: <eval_with_key>.27:1169 in forward, code: layer_norm_77 = torch.nn.functional.layer_norm(mul_22, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b, eps = 1e-05);  mul_22 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b = None
        layer_norm_102: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1843, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b);  mul_1843 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_87: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_102, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias);  layer_norm_102 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1173 in forward, code: layer_norm_78 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b = None
        layer_norm_103: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_87, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b);  linear_87 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1174 in forward, code: add_28 = add_27 + layer_norm_78;  add_27 = layer_norm_78 = None
        add_2417: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2392, layer_norm_103);  add_2392 = layer_norm_103 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_104: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2417, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_24: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_104);  layer_norm_104 = None
        
         # File: <eval_with_key>.27:1177 in forward, code: mul_23 = add_28 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_1;  add_28 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_1 = None
        mul_1858: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2417, sigmoid_24);  add_2417 = sigmoid_24 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_88: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.linear.default(mul_1858, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias);  mul_1858 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1179 in forward, code: cat_45 = torch.cat([linear_45, linear_46, linear_47, linear_48, linear_49, linear_50, reshape_133, main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0], dim = 1);  reshape_133 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0 = None
        cat_9: "f16[s0, 19584][19584, 1]cuda:0" = torch.ops.aten.cat.default([linear_49, linear_50, linear_51, linear_52, linear_53, linear_54, view_143, linear_88], 1);  view_143 = linear_88 = None
        
         # File: <eval_with_key>.27:1180 in forward, code: _reshape_to_3d_1 = _torch_package_1_legokit_backbones_dhen_prototype__reshape_to_3d(tensor = cat_45, emb_num = 102, emb_dim = 192);  cat_45 = None
        view_148: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.view.default(cat_9, [sym_size_int_12, 102, 192]);  cat_9 = None
        
         # File: <eval_with_key>.27:1181 in forward, code: add_29 = _reshape_to_3d_1 + layer_norm_59;  _reshape_to_3d_1 = layer_norm_59 = None
        add_2440: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_148, layer_norm_75);  view_148 = layer_norm_75 = None
        
         # File: <eval_with_key>.27:1186 in forward, code: layer_norm_79 = torch.nn.functional.layer_norm(add_29, getitem_333, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b, eps = 1e-05);  add_29 = getitem_333 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b = None
        layer_norm_105: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2440, [192], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b);  add_2440 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b = None
        
         # File: <eval_with_key>.27:1188 in forward, code: matmul_2 = torch.matmul(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w, layer_norm_79);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w = None
        matmul_2: "f16[s0, 72, 192][13824, 192, 1]cuda:0" = torch.ops.aten.matmul.default(submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w, layer_norm_105);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w = None
        
         # File: <eval_with_key>.27:1190 in forward, code: add_30 = matmul_2 + main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b;  matmul_2 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b = None
        add_2453: "f16[s0, 72, 192][13824, 192, 1]cuda:0" = torch.ops.aten.add.Tensor(matmul_2, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b);  matmul_2 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b = None
        
         # File: <eval_with_key>.27:1191 in forward, code: permute_10 = layer_norm_79.permute(0, 2, 1)
        permute_10: "f16[s0, 192, 102][19584, 1, 192]cuda:0" = torch.ops.aten.permute.default(layer_norm_105, [0, 2, 1])
        
         # File: <eval_with_key>.27:1196 in forward, code: layer_norm_80 = torch.nn.functional.layer_norm(add_30, getitem_334, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b, eps = 1e-05);  add_30 = getitem_334 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b = None
        layer_norm_106: "f16[s0, 72, 192][13824, 192, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2453, [192], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b);  add_2453 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b = None
        
         # File: <eval_with_key>.27:1197 in forward, code: split_2 = torch.functional.split(layer_norm_80, [48, 24], dim = 1);  layer_norm_80 = None
        split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(layer_norm_106, [48, 24], 1);  layer_norm_106 = None
        getitem_266: "f16[s0, 48, 192][13824, 192, 1]cuda:0" = split_with_sizes_6[0]
        getitem_267: "f16[s0, 24, 192][13824, 192, 1]cuda:0" = split_with_sizes_6[1];  split_with_sizes_6 = None
        
         # File: <eval_with_key>.27:1200 in forward, code: reshape_137 = getitem_335.reshape(-1, 9216);  getitem_335 = None
        view_149: "f16[s0, 9216][13824, 1]cuda:0" = torch.ops.aten.view.default(getitem_266, [sym_size_int_12, 9216]);  getitem_266 = None
        
         # File: <eval_with_key>.27:1201 in forward, code: reshape_138 = torch.reshape(getitem_336, (-1, 4608));  getitem_336 = None
        view_150: "f16[s0, 4608][13824, 1]cuda:0" = torch.ops.aten.view.default(getitem_267, [sym_size_int_12, 4608]);  getitem_267 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_89: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_150, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_90: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_150, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_bias);  view_150 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_107: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_89, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_108: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_90, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_25: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_107);  layer_norm_107 = None
        
         # File: <eval_with_key>.27:1207 in forward, code: mul_24 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_1 = None
        mul_1907: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_89, sigmoid_25);  linear_89 = sigmoid_25 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_26: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_108);  layer_norm_108 = None
        
         # File: <eval_with_key>.27:1209 in forward, code: mul_25 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_1 = None
        mul_1912: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_90, sigmoid_26);  linear_90 = sigmoid_26 = None
        
         # File: <eval_with_key>.27:1212 in forward, code: layer_norm_81 = torch.nn.functional.layer_norm(mul_24, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_24 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        layer_norm_109: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1907, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b);  mul_1907 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:1215 in forward, code: layer_norm_82 = torch.nn.functional.layer_norm(mul_25, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_25 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        layer_norm_110: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1912, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b);  mul_1912 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_91: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_109, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias);  layer_norm_109 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_92: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_110, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias);  layer_norm_110 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_111: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_91, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_112: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_92, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_27: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_111);  layer_norm_111 = None
        
         # File: <eval_with_key>.27:1221 in forward, code: mul_26 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_1 = None
        mul_1929: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_91, sigmoid_27);  linear_91 = sigmoid_27 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_28: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_112);  layer_norm_112 = None
        
         # File: <eval_with_key>.27:1223 in forward, code: mul_27 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_1 = None
        mul_1934: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_92, sigmoid_28);  linear_92 = sigmoid_28 = None
        
         # File: <eval_with_key>.27:1226 in forward, code: layer_norm_83 = torch.nn.functional.layer_norm(mul_26, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_26 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        layer_norm_113: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1929, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b);  mul_1929 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        
         # File: <eval_with_key>.27:1229 in forward, code: layer_norm_84 = torch.nn.functional.layer_norm(mul_27, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_27 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        layer_norm_114: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1934, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b);  mul_1934 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_93: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_113, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_113 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_94: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_114, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_114 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1234 in forward, code: layer_norm_85 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0, (4896,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_115: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_93, [4896], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b);  linear_93 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1237 in forward, code: layer_norm_86 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0, (9216,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_116: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_94, [9216], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b);  linear_94 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1238 in forward, code: reshape_139 = torch.reshape(layer_norm_85, (-1, 102, 48));  layer_norm_85 = None
        view_151: "f16[s0, 102, 48][4896, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_115, [-1, 102, 48]);  layer_norm_115 = None
        
         # File: <eval_with_key>.27:1239 in forward, code: reshape_140 = torch.reshape(layer_norm_86, (-1, 192, 48));  layer_norm_86 = None
        view_152: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_116, [-1, 192, 48]);  layer_norm_116 = None
        
         # File: <eval_with_key>.27:1240 in forward, code: bmm_4 = torch.bmm(permute_10, reshape_139);  permute_10 = reshape_139 = None
        bmm_4: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.bmm.default(permute_10, view_151);  permute_10 = view_151 = None
        
         # File: <eval_with_key>.27:1241 in forward, code: add_31 = reshape_140 + bmm_4;  reshape_140 = bmm_4 = None
        add_2564: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.add.Tensor(view_152, bmm_4);  view_152 = bmm_4 = None
        
         # File: <eval_with_key>.27:1244 in forward, code: layer_norm_87 = torch.nn.functional.layer_norm(add_31, (48,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b, eps = 1e-05);  add_31 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b = None
        layer_norm_117: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2564, [48], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b);  add_2564 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b = None
        
         # File: <eval_with_key>.27:1245 in forward, code: bmm_5 = torch.bmm(layer_norm_79, layer_norm_87);  layer_norm_87 = None
        bmm_5: "f16[s0, 102, 48][4896, 48, 1]cuda:0" = torch.ops.aten.bmm.default(layer_norm_105, layer_norm_117);  layer_norm_117 = None
        
         # File: <eval_with_key>.27:1246 in forward, code: flatten_2 = torch.flatten(bmm_5, start_dim = -2);  bmm_5 = None
        view_153: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.view.default(bmm_5, [sym_size_int_12, 4896]);  bmm_5 = None
        
         # File: <eval_with_key>.27:1251 in forward, code: layer_norm_88 = torch.nn.functional.layer_norm(flatten_2, getitem_337, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b, eps = 1e-05);  flatten_2 = getitem_337 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b = None
        layer_norm_118: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.layer_norm.default(view_153, [4896], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b);  view_153 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_95: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_118, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias);  layer_norm_118 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_119: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_95, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_29: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_119);  layer_norm_119 = None
        
         # File: <eval_with_key>.27:1255 in forward, code: mul_28 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_1 = None
        mul_1978: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_95, sigmoid_29);  linear_95 = sigmoid_29 = None
        
         # File: <eval_with_key>.27:1258 in forward, code: layer_norm_89 = torch.nn.functional.layer_norm(mul_28, (2048,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_28 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        layer_norm_120: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_1978, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b);  mul_1978 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:1259 in forward, code: cat_46 = torch.cat([mul_3, mul_2, layer_norm_89], dim = 1);  layer_norm_89 = None
        cat_10: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.cat.default([mul_570, mul_567, layer_norm_120], 1);  layer_norm_120 = None
        
         # File: <eval_with_key>.27:1264 in forward, code: layer_norm_90 = torch.nn.functional.layer_norm(cat_46, getitem_338, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b, eps = 1e-05);  cat_46 = getitem_338 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b = None
        layer_norm_121: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(cat_10, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b);  cat_10 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_96: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_121, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1268 in forward, code: layer_norm_91 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0, (384,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        layer_norm_122: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_96, [384], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b);  linear_96 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_97: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_122, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias);  layer_norm_122 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1272 in forward, code: layer_norm_92 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0, (6354,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        layer_norm_123: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_97, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b);  linear_97 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1273 in forward, code: addcmul_2 = torch.addcmul(input = layer_norm_90, tensor1 = layer_norm_90, tensor2 = layer_norm_92, value = 1.0);  layer_norm_90 = layer_norm_92 = None
        addcmul_2: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.addcmul.default(layer_norm_121, layer_norm_121, layer_norm_123, value = 1.0);  layer_norm_121 = layer_norm_123 = None
        
         # File: <eval_with_key>.27:1278 in forward, code: layer_norm_93 = torch.nn.functional.layer_norm(addcmul_2, getitem_339, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b, eps = 1e-05);  addcmul_2 = getitem_339 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b = None
        layer_norm_124: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(addcmul_2, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b);  addcmul_2 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_98: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_124, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias);  layer_norm_124 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_125: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_98, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_30: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_125);  layer_norm_125 = None
        
         # File: <eval_with_key>.27:1282 in forward, code: mul_29 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_1 = None
        mul_2005: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_98, sigmoid_30);  linear_98 = sigmoid_30 = None
        
         # File: <eval_with_key>.27:1285 in forward, code: layer_norm_94 = torch.nn.functional.layer_norm(mul_29, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_29 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b = None
        layer_norm_126: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2005, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b);  mul_2005 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_99: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_126, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_127: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_99, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_31: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_127);  layer_norm_127 = None
        
         # File: <eval_with_key>.27:1289 in forward, code: mul_30 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_1 = None
        mul_2016: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_99, sigmoid_31);  linear_99 = sigmoid_31 = None
        
         # File: <eval_with_key>.27:1292 in forward, code: layer_norm_95 = torch.nn.functional.layer_norm(mul_30, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b, eps = 1e-05);  mul_30 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b = None
        layer_norm_128: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2016, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b);  mul_2016 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_100: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_128, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias);  layer_norm_128 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1296 in forward, code: layer_norm_96 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b = None
        layer_norm_129: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_100, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b);  linear_100 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1297 in forward, code: add_32 = layer_norm_94 + layer_norm_96;  layer_norm_96 = None
        add_2658: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_126, layer_norm_129);  layer_norm_129 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_130: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2658, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_32: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_130);  layer_norm_130 = None
        
         # File: <eval_with_key>.27:1300 in forward, code: mul_31 = add_32 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_1;  add_32 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_1 = None
        mul_2031: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2658, sigmoid_32);  add_2658 = sigmoid_32 = None
        
         # File: <eval_with_key>.27:1301 in forward, code: add_33 = layer_norm_94 + mul_31;  layer_norm_94 = None
        add_2671: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_126, mul_2031);  layer_norm_126 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_101: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(mul_2031, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias);  mul_2031 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_131: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_101, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_33: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_131);  layer_norm_131 = None
        
         # File: <eval_with_key>.27:1305 in forward, code: mul_32 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_1 = None
        mul_2042: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_101, sigmoid_33);  linear_101 = sigmoid_33 = None
        
         # File: <eval_with_key>.27:1308 in forward, code: layer_norm_97 = torch.nn.functional.layer_norm(mul_32, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b, eps = 1e-05);  mul_32 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b = None
        layer_norm_132: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2042, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b);  mul_2042 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_102: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_132, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias);  layer_norm_132 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1312 in forward, code: layer_norm_98 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b = None
        layer_norm_133: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_102, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b);  linear_102 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1313 in forward, code: add_34 = add_33 + layer_norm_98;  add_33 = layer_norm_98 = None
        add_2696: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2671, layer_norm_133);  add_2671 = layer_norm_133 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_134: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2696, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_34: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_134);  layer_norm_134 = None
        
         # File: <eval_with_key>.27:1316 in forward, code: mul_33 = add_34 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_1;  add_34 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_1 = None
        mul_2057: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2696, sigmoid_34);  add_2696 = sigmoid_34 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_103: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.linear.default(mul_2057, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias);  mul_2057 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1318 in forward, code: cat_47 = torch.cat([linear_45, linear_46, linear_47, linear_48, linear_49, linear_50, reshape_137, main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0], dim = 1);  linear_45 = linear_46 = linear_47 = linear_48 = linear_49 = linear_50 = reshape_137 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0 = None
        cat_11: "f16[s0, 19584][19584, 1]cuda:0" = torch.ops.aten.cat.default([linear_49, linear_50, linear_51, linear_52, linear_53, linear_54, view_149, linear_103], 1);  linear_49 = linear_50 = linear_51 = linear_52 = linear_53 = linear_54 = view_149 = linear_103 = None
        
         # File: <eval_with_key>.27:1319 in forward, code: _reshape_to_3d_2 = _torch_package_1_legokit_backbones_dhen_prototype__reshape_to_3d(tensor = cat_47, emb_num = 102, emb_dim = 192);  cat_47 = None
        view_154: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.view.default(cat_11, [sym_size_int_12, 102, 192]);  cat_11 = None
        
         # File: <eval_with_key>.27:1320 in forward, code: add_35 = _reshape_to_3d_2 + layer_norm_79;  _reshape_to_3d_2 = layer_norm_79 = None
        add_2719: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.add.Tensor(view_154, layer_norm_105);  view_154 = layer_norm_105 = None
        
         # File: <eval_with_key>.27:1325 in forward, code: layer_norm_99 = torch.nn.functional.layer_norm(add_35, getitem_340, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b, eps = 1e-05);  add_35 = getitem_340 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b = None
        layer_norm_135: "f16[s0, 102, 192][19584, 192, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2719, [192], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b);  add_2719 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b = None
        
         # File: <eval_with_key>.27:1327 in forward, code: matmul_3 = torch.matmul(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w, layer_norm_99);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w = None
        matmul_3: "f16[s0, 24, 192][4608, 192, 1]cuda:0" = torch.ops.aten.matmul.default(submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w, layer_norm_135);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w = None
        
         # File: <eval_with_key>.27:1329 in forward, code: add_36 = matmul_3 + main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b;  matmul_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b = None
        add_2732: "f16[s0, 24, 192][4608, 192, 1]cuda:0" = torch.ops.aten.add.Tensor(matmul_3, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b);  matmul_3 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b = None
        
         # File: <eval_with_key>.27:1330 in forward, code: permute_11 = layer_norm_99.permute(0, 2, 1)
        permute_11: "f16[s0, 192, 102][19584, 1, 192]cuda:0" = torch.ops.aten.permute.default(layer_norm_135, [0, 2, 1])
        
         # File: <eval_with_key>.27:1335 in forward, code: layer_norm_100 = torch.nn.functional.layer_norm(add_36, getitem_341, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b, eps = 1e-05);  add_36 = getitem_341 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b = None
        layer_norm_136: "f16[s0, 24, 192][4608, 192, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2732, [192], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b);  add_2732 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b = None
        
         # File: <eval_with_key>.27:1336 in forward, code: reshape_141 = torch.reshape(layer_norm_100, (-1, 4608));  layer_norm_100 = None
        view_155: "f16[s0, 4608][4608, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_136, [-1, 4608]);  layer_norm_136 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_104: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_155, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_105: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(view_155, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_bias);  view_155 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_137: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_104, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_138: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_105, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_35: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_137);  layer_norm_137 = None
        
         # File: <eval_with_key>.27:1342 in forward, code: mul_34 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_1 = None
        mul_2095: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_104, sigmoid_35);  linear_104 = sigmoid_35 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_36: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_138);  layer_norm_138 = None
        
         # File: <eval_with_key>.27:1344 in forward, code: mul_35 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_1 = None
        mul_2100: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_105, sigmoid_36);  linear_105 = sigmoid_36 = None
        
         # File: <eval_with_key>.27:1347 in forward, code: layer_norm_101 = torch.nn.functional.layer_norm(mul_34, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_34 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        layer_norm_139: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2095, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b);  mul_2095 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:1350 in forward, code: layer_norm_102 = torch.nn.functional.layer_norm(mul_35, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b, eps = 1e-05);  mul_35 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        layer_norm_140: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2100, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b);  mul_2100 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_106: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_139, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias);  layer_norm_139 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_107: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_140, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias);  layer_norm_140 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_141: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_106, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_142: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_107, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_37: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_141);  layer_norm_141 = None
        
         # File: <eval_with_key>.27:1356 in forward, code: mul_36 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_1 = None
        mul_2117: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_106, sigmoid_37);  linear_106 = sigmoid_37 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_38: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_142);  layer_norm_142 = None
        
         # File: <eval_with_key>.27:1358 in forward, code: mul_37 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_1 = None
        mul_2122: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_107, sigmoid_38);  linear_107 = sigmoid_38 = None
        
         # File: <eval_with_key>.27:1361 in forward, code: layer_norm_103 = torch.nn.functional.layer_norm(mul_36, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_36 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        layer_norm_143: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2117, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b);  mul_2117 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b = None
        
         # File: <eval_with_key>.27:1364 in forward, code: layer_norm_104 = torch.nn.functional.layer_norm(mul_37, (768,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b, eps = 1e-05);  mul_37 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        layer_norm_144: "f16[s0, 768][768, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2122, [768], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b);  mul_2122 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_108: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_143, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_143 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_109: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_144, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias);  layer_norm_144 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1369 in forward, code: layer_norm_105 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0, (4896,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_145: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_108, [4896], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b);  linear_108 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1372 in forward, code: layer_norm_106 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0, (9216,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        layer_norm_146: "f16[s0, 9216][9216, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_109, [9216], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b);  linear_109 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1373 in forward, code: reshape_142 = torch.reshape(layer_norm_105, (-1, 102, 48));  layer_norm_105 = None
        view_156: "f16[s0, 102, 48][4896, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_145, [-1, 102, 48]);  layer_norm_145 = None
        
         # File: <eval_with_key>.27:1374 in forward, code: reshape_143 = torch.reshape(layer_norm_106, (-1, 192, 48));  layer_norm_106 = None
        view_157: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.view.default(layer_norm_146, [-1, 192, 48]);  layer_norm_146 = None
        
         # File: <eval_with_key>.27:1375 in forward, code: bmm_6 = torch.bmm(permute_11, reshape_142);  permute_11 = reshape_142 = None
        bmm_6: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.bmm.default(permute_11, view_156);  permute_11 = view_156 = None
        
         # File: <eval_with_key>.27:1376 in forward, code: add_37 = reshape_143 + bmm_6;  reshape_143 = bmm_6 = None
        add_2832: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.add.Tensor(view_157, bmm_6);  view_157 = bmm_6 = None
        
         # File: <eval_with_key>.27:1379 in forward, code: layer_norm_107 = torch.nn.functional.layer_norm(add_37, (48,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b, eps = 1e-05);  add_37 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b = None
        layer_norm_147: "f16[s0, 192, 48][9216, 48, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2832, [48], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b);  add_2832 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b = None
        
         # File: <eval_with_key>.27:1380 in forward, code: bmm_7 = torch.bmm(layer_norm_99, layer_norm_107);  layer_norm_99 = layer_norm_107 = None
        bmm_7: "f16[s0, 102, 48][4896, 48, 1]cuda:0" = torch.ops.aten.bmm.default(layer_norm_135, layer_norm_147);  layer_norm_135 = layer_norm_147 = None
        
         # File: <eval_with_key>.27:1381 in forward, code: flatten_3 = torch.flatten(bmm_7, start_dim = -2);  bmm_7 = None
        view_158: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.view.default(bmm_7, [sym_size_int_12, 4896]);  bmm_7 = sym_size_int_12 = None
        
         # File: <eval_with_key>.27:1386 in forward, code: layer_norm_108 = torch.nn.functional.layer_norm(flatten_3, getitem_342, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b, eps = 1e-05);  flatten_3 = getitem_342 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b = None
        layer_norm_148: "f16[s0, 4896][4896, 1]cuda:0" = torch.ops.aten.layer_norm.default(view_158, [4896], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b);  view_158 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_110: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_148, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias);  layer_norm_148 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_149: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_110, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_39: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_149);  layer_norm_149 = None
        
         # File: <eval_with_key>.27:1390 in forward, code: mul_38 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_1 = None
        mul_2166: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_110, sigmoid_39);  linear_110 = sigmoid_39 = None
        
         # File: <eval_with_key>.27:1393 in forward, code: layer_norm_109 = torch.nn.functional.layer_norm(mul_38, (2048,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_38 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        layer_norm_150: "f16[s0, 2048][2048, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2166, [2048], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b);  mul_2166 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b = None
        
         # File: <eval_with_key>.27:1394 in forward, code: cat_48 = torch.cat([mul_3, mul_2, layer_norm_109], dim = 1);  mul_3 = mul_2 = layer_norm_109 = None
        cat_12: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.cat.default([mul_570, mul_567, layer_norm_150], 1);  mul_570 = mul_567 = layer_norm_150 = None
        
         # File: <eval_with_key>.27:1399 in forward, code: layer_norm_110 = torch.nn.functional.layer_norm(cat_48, getitem_343, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b, eps = 1e-05);  cat_48 = getitem_343 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b = None
        layer_norm_151: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(cat_12, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b);  cat_12 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_111: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_151, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1403 in forward, code: layer_norm_111 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0, (384,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        layer_norm_152: "f16[s0, 384][384, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_111, [384], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b);  linear_111 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_112: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_152, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias);  layer_norm_152 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1407 in forward, code: layer_norm_112 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0, (6354,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        layer_norm_153: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_112, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b);  linear_112 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1408 in forward, code: addcmul_3 = torch.addcmul(input = layer_norm_110, tensor1 = layer_norm_110, tensor2 = layer_norm_112, value = 1.0);  layer_norm_110 = layer_norm_112 = None
        addcmul_3: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.addcmul.default(layer_norm_151, layer_norm_151, layer_norm_153, value = 1.0);  layer_norm_151 = layer_norm_153 = None
        
         # File: <eval_with_key>.27:1413 in forward, code: layer_norm_113 = torch.nn.functional.layer_norm(addcmul_3, getitem_344, weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b, eps = 1e-05);  addcmul_3 = getitem_344 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b = None
        layer_norm_154: "f16[s0, 6354][6354, 1]cuda:0" = torch.ops.aten.layer_norm.default(addcmul_3, [6354], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b);  addcmul_3 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_113: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_154, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias);  layer_norm_154 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_155: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_113, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_40: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_155);  layer_norm_155 = None
        
         # File: <eval_with_key>.27:1417 in forward, code: mul_39 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_1 = None
        mul_2193: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_113, sigmoid_40);  linear_113 = sigmoid_40 = None
        
         # File: <eval_with_key>.27:1420 in forward, code: layer_norm_114 = torch.nn.functional.layer_norm(mul_39, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b, eps = 1e-05);  mul_39 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b = None
        layer_norm_156: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2193, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b);  mul_2193 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_114: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_156, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_157: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_114, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_41: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_157);  layer_norm_157 = None
        
         # File: <eval_with_key>.27:1424 in forward, code: mul_40 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_1 = None
        mul_2204: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_114, sigmoid_41);  linear_114 = sigmoid_41 = None
        
         # File: <eval_with_key>.27:1427 in forward, code: layer_norm_115 = torch.nn.functional.layer_norm(mul_40, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b, eps = 1e-05);  mul_40 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b = None
        layer_norm_158: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2204, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b);  mul_2204 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_115: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_158, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias);  layer_norm_158 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1431 in forward, code: layer_norm_116 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b = None
        layer_norm_159: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_115, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b);  linear_115 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1432 in forward, code: add_38 = layer_norm_114 + layer_norm_116;  layer_norm_116 = None
        add_2926: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_156, layer_norm_159);  layer_norm_159 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_160: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2926, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_42: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_160);  layer_norm_160 = None
        
         # File: <eval_with_key>.27:1435 in forward, code: mul_41 = add_38 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_1;  add_38 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_1 = None
        mul_2219: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2926, sigmoid_42);  add_2926 = sigmoid_42 = None
        
         # File: <eval_with_key>.27:1436 in forward, code: add_39 = layer_norm_114 + mul_41;  layer_norm_114 = None
        add_2939: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(layer_norm_156, mul_2219);  layer_norm_156 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_116: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.linear.default(mul_2219, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias);  mul_2219 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_161: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_116, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_43: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_161);  layer_norm_161 = None
        
         # File: <eval_with_key>.27:1440 in forward, code: mul_42 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_1;  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_1 = None
        mul_2230: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_116, sigmoid_43);  linear_116 = sigmoid_43 = None
        
         # File: <eval_with_key>.27:1443 in forward, code: layer_norm_117 = torch.nn.functional.layer_norm(mul_42, (1536,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b, eps = 1e-05);  mul_42 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b = None
        layer_norm_162: "f16[s0, 1536][1536, 1]cuda:0" = torch.ops.aten.layer_norm.default(mul_2230, [1536], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b);  mul_2230 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
        linear_117: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(layer_norm_162, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias);  layer_norm_162 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias = None
        
         # File: <eval_with_key>.27:1447 in forward, code: layer_norm_118 = torch.nn.functional.layer_norm(main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0, (3072,), weight = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w, bias = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b, eps = 1e-05);  main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b = None
        layer_norm_163: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_117, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b);  linear_117 = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b = None
        
         # File: <eval_with_key>.27:1448 in forward, code: add_40 = add_39 + layer_norm_118;  add_39 = layer_norm_118 = None
        add_2964: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(add_2939, layer_norm_163);  add_2939 = layer_norm_163 = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
        layer_norm_164: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.layer_norm.default(add_2964, [3072], submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight, submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias);  submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight = submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias = None
        
         # File: /data/users/shangdiy/fbsource/buck-out/v2/gen/fbcode/d29ee94b913014f1/caffe2/torch/fb/model_transform/experimental/benchmark/__mts_gpu_benchmark__/mts_gpu_benchmark#link-tree/torch/nn/modules/activation.py:327 in forward, code: return torch.sigmoid(input)
        sigmoid_44: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_164);  layer_norm_164 = None
        
         # File: <eval_with_key>.27:1451 in forward, code: mul_43 = add_40 * main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_1;  add_40 = main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_1 = None
        mul_2245: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_2964, sigmoid_44);  add_2964 = sigmoid_44 = None
        
         # File: <eval_with_key>.27:1454 in forward, code: linear_51 = torch._C._nn.linear(mul_43, main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_w, main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b);  main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_w = main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b = None
        linear_118: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.linear.default(mul_2245, submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b = None
        
         # File: <eval_with_key>.27:1455 in forward, code: relu_1 = torch.nn.functional.relu(linear_51, inplace = False);  linear_51 = None
        relu_1: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.relu.default(linear_118);  linear_118 = None
        
         # File: <eval_with_key>.27:1458 in forward, code: linear_52 = torch._C._nn.linear(relu_1, main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_w, main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b);  relu_1 = main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_w = main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b = None
        linear_119: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(relu_1, submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_w, submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b);  relu_1 = submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_w = submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b = None
        
         # File: <eval_with_key>.27:1459 in forward, code: relu_2 = torch.nn.functional.relu(linear_52, inplace = False);  linear_52 = None
        relu_2: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.relu.default(linear_119);  linear_119 = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        add_2989: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2245, relu_2);  relu_2 = None
        
         # File: <eval_with_key>.27:1463 in forward, code: linear_53 = torch._C._nn.linear(main_module_impl_impl_shared_arch_cyclegan_add, main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_w, main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b);  main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_w = main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b = None
        linear_120: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.linear.default(add_2989, submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_w, submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b);  submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_w = submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        full_like: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.full_like.default(add_2989, 0.0, dtype = torch.float16, pin_memory = False)
        
         # File: <eval_with_key>.27:1465 in forward, code: relu_3 = torch.nn.functional.relu(linear_53, inplace = False);  linear_53 = None
        relu_3: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.relu.default(linear_120);  linear_120 = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        mul_2264: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(full_like, add_2989);  add_2989 = None
        
         # File: <eval_with_key>.27:1469 in forward, code: linear_54 = torch._C._nn.linear(relu_3, main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_w, main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b);  relu_3 = main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_w = main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b = None
        linear_121: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(relu_3, submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_w, submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b);  relu_3 = submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_w = submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        add_3008: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2245, mul_2264);  mul_2245 = mul_2264 = None
        
         # File: <eval_with_key>.27:1471 in forward, code: relu_4 = torch.nn.functional.relu(linear_54, inplace = False);  linear_54 = None
        relu_4: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.relu.default(linear_121);  linear_121 = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        mul_2273: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(relu_4, full_like);  relu_4 = full_like = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        add_3018: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.add.Tensor(add_3008, mul_2273);  add_3008 = mul_2273 = None
        
         # File: <eval_with_key>.27:1476 in forward, code: linear_55 = torch._C._nn.linear(main_module_impl_impl_shared_arch_cyclegan_add_2, main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_w, main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b);  main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_w = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b = None
        linear_122: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.linear.default(add_3018, submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_w, submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b);  submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_w = submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b = None
        
         # File: <eval_with_key>.27:1477 in forward, code: sigmoid_5 = torch.sigmoid(linear_55);  linear_55 = None
        sigmoid_45: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.sigmoid.default(linear_122);  linear_122 = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        mul_2282: "f16[s0, 3072][3072, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_3018, sigmoid_45);  add_3018 = sigmoid_45 = None
        
         # File: <eval_with_key>.27:1481 in forward, code: linear_56 = torch._C._nn.linear(main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_mul_module, main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w, main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b);  main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_mul_module = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = None
        linear_123: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.linear.default(mul_2282, submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w, submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b);  mul_2282 = submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w = submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = None
        
         # File: <eval_with_key>.27:1489 in forward, code: layer_norm_119 = torch.nn.functional.layer_norm(linear_56, getitem_345, weight = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale, bias = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias, eps = 1e-05);  getitem_345 = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = None
        layer_norm_165: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.layer_norm.default(linear_123, [512], submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale, submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias);  submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = None
        
         # File: <eval_with_key>.27:1490 in forward, code: sigmoid_6 = torch.sigmoid(layer_norm_119);  layer_norm_119 = None
        sigmoid_46: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.sigmoid.default(layer_norm_165);  layer_norm_165 = None
        
         # File: <eval_with_key>.27:1498 in forward, code: mul_44 = linear_56 * unsqueeze_n_times_4;  linear_56 = unsqueeze_n_times_4 = None
        mul_2291: "f16[s0, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(linear_123, sigmoid_46);  linear_123 = sigmoid_46 = None
        
         # File: <eval_with_key>.27:1501 in forward, code: linear_57 = torch._C._nn.linear(mul_44, main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_w, main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b);  mul_44 = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_w = main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b = None
        linear_124: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.linear.default(mul_2291, submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_w, submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b);  mul_2291 = submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_w = submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        add_3046: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(linear_124, submod_1_main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_positive_weight_calibration_bias);  linear_124 = submod_1_main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_positive_weight_calibration_bias = None
        
         # File: <eval_with_key>.26:7 in forward, code: sigmoid_7 = torch.sigmoid(main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_add);  main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_add = None
        sigmoid_47: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_3046);  add_3046 = None
        
         # File: <eval_with_key>.26:8 in forward, code: detach = sigmoid_7.detach();  sigmoid_7 = None
        detach: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.detach.default(sigmoid_47);  sigmoid_47 = None
        
         # File: <eval_with_key>.26:9 in forward, code: logit = torch.logit(detach, 1e-06)
        logit: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.logit.default(detach, 1e-06)
        
         # File: <eval_with_key>.26:10 in forward, code: add_41 = logit + add_13;  logit = add_13 = None
        add_3059: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(logit, add_808);  logit = add_808 = None
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        full_like_1: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.full_like.default(detach, 0.0, dtype = torch.float32, pin_memory = False)
        
         # File: <torch_package_1>.dper3/core/low_level_module.py:197 in forward, code: return self.forward_pytorch(*args, **kwargs)
        full_like_2: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.full_like.default(detach, 1.0, dtype = torch.float32, pin_memory = False)
        
         # File: <eval_with_key>.26:13 in forward, code: sigmoid_8 = torch.sigmoid(add_41);  add_41 = None
        sigmoid_48: "f16[s0, 1][1, 1]cuda:0" = torch.ops.aten.sigmoid.default(add_3059);  add_3059 = None
        
         # File: <eval_with_key>.26:24 in forward, code: gt = torch.gt(detach, unsqueeze_n_times_5);  unsqueeze_n_times_5 = None
        gt: "b8[s0, 1][1, 1]cuda:0" = torch.ops.aten.gt.Tensor(detach, full_like_1);  full_like_1 = None
        
         # File: <eval_with_key>.26:25 in forward, code: to = gt.to(torch.float32);  gt = None
        _to_copy_9: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten._to_copy.default(gt, dtype = torch.float32);  gt = None
        
         # File: <eval_with_key>.26:26 in forward, code: mul_45 = torch.mul(main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_const_fill_like_1, to);  main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_const_fill_like_1 = to = None
        mul_2304: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(full_like_2, _to_copy_9);  full_like_2 = _to_copy_9 = None
        
         # File: <eval_with_key>.26:28 in forward, code: sub_18 = _tensor_constant1.sub(mul_45);  _tensor_constant1 = None
        sub_950: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.sub.Tensor(submod_1__tensor_constant1, mul_2304);  submod_1__tensor_constant1 = None
        
         # File: <eval_with_key>.26:29 in forward, code: mul_46 = torch.mul(sigmoid_8, mul_45);  sigmoid_8 = mul_45 = None
        mul_2307: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(sigmoid_48, mul_2304);  sigmoid_48 = mul_2304 = None
        
         # File: <eval_with_key>.26:30 in forward, code: mul_47 = torch.mul(detach, sub_18);  detach = sub_18 = None
        mul_2309: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(detach, sub_950);  detach = sub_950 = None
        
         # File: <eval_with_key>.26:31 in forward, code: add_42 = mul_47 + mul_46;  mul_47 = mul_46 = None
        add_3090: "f32[s0, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_2309, mul_2307);  mul_2309 = mul_2307 = None
        return (add_3090, _tensor_constant2)
        

 # graph id: 140226430922096