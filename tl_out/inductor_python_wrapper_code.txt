from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import (
    grid,
    split_scan_grid,
    grid_combo_kernels,
    start_graph,
    end_graph,
    cooperative_reduction_grid,
)
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
import torch._inductor.kernel.mm_common
import torch._inductor.kernel.bmm

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/pn/cpnxmf575lnss4bimoau5kwte3sblxl5ljkcv3nevmsf52hoscss.py
# Topologically Sorted Source Nodes: [cat_default_12, permute_pooled_embs_auto_grad], Original ATen: [aten.cat, fbgemm.permute_pooled_embs_auto_grad]
# Source node to ATen node mapping:
#   cat_default_12 => cat
#   permute_pooled_embs_auto_grad => permute_pooled_embs_auto_grad
# Graph fragment:
#   %cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%arg608_1, %arg609_1, %arg610_1, %arg611_1], 1), kwargs = {})
#   %permute_pooled_embs_auto_grad : [num_users=1] = call_function[target=torch.ops.fbgemm.permute_pooled_embs_auto_grad.default](args = (%cat, %submod_0_cat_fusion_gpu__offset_dim_list, %submod_0_cat_fusion_gpu__permute, %submod_0_cat_fusion_gpu__inv_offset_dim_list, %submod_0_cat_fusion_gpu__inv_permute), kwargs = {})
triton_poi_fused_cat_permute_pooled_embs_auto_grad_11 = async_compile.triton('triton_poi_fused_cat_permute_pooled_embs_auto_grad_11', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[134217728],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_permute_pooled_embs_auto_grad_11', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_permute_pooled_embs_auto_grad_11(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 36284)
    x1 = xindex // 36284
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1], 384, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (384*x1 + (x0)), tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1], 768, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (384*x1 + ((-384) + x0)), tmp9 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1], 18700, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tmp11 & tmp13
    tmp15 = tl.load(in_ptr2 + (17932*x1 + ((-768) + x0)), tmp14 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tmp0 >= tmp12
    tmp17 = tl.full([1], 36284, tl.int64)
    tmp18 = tmp0 < tmp17
    tmp19 = tl.load(in_ptr3 + (17584*x1 + ((-18700) + x0)), tmp16 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.where(tmp14, tmp15, tmp19)
    tmp21 = tl.where(tmp9, tmp10, tmp20)
    tmp22 = tl.where(tmp4, tmp5, tmp21)
    tl.store(out_ptr0 + (x2), tmp22, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/qx/cqxu23gttaccrbtqakvxiiakxkhequrqre6sdyheruke4yrw54i4.py
# Topologically Sorted Source Nodes: [linear], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear => mm_default_101
# Graph fragment:
#   %mm_default_101 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_97, %permute), kwargs = {})
triton_tem_fused_addmm_12 = async_compile.triton('triton_tem_fused_addmm_12', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_12', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_12(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 3026
    B = arg_B

    M = ks0
    N = 192
    K = 240
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3890
    stride_ak = 1
    stride_bk = 1
    stride_bn = 240

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta0 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2q/c2qqt5qnd3rcgslt37viscfpvzpdujxasggbvyysx3akomk3yqwk.py
# Topologically Sorted Source Nodes: [linear, layer_norm], Original ATen: [aten.addmm, aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm => add_587, add_588, convert_element_type_120, convert_element_type_121, mul_379, mul_380, rsqrt, sub_195, var_mean
#   linear => add_tensor_101
# Graph fragment:
#   %add_tensor_101 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_101, %submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b), kwargs = {})
#   %convert_element_type_120 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_101, torch.float32), kwargs = {})
#   %var_mean : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_120, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_195 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_120, %getitem_134), kwargs = {})
#   %add_587 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_133, 1e-05), kwargs = {})
#   %rsqrt : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_587,), kwargs = {})
#   %mul_379 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_195, %rsqrt), kwargs = {})
#   %mul_380 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_379, %submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale), kwargs = {})
#   %add_588 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_380, %submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias), kwargs = {})
#   %convert_element_type_121 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_588, torch.float16), kwargs = {})
triton_per_fused_addmm_native_layer_norm_13 = async_compile.triton('triton_per_fused_addmm_native_layer_norm_13', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_addmm_native_layer_norm_13', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_addmm_native_layer_norm_13(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 192
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + 192*x0), rmask & xmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
    tmp6 = tl.where(rmask & xmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
    tmp9 = tl.where(rmask & xmask, tmp7, 0)
    tmp10 = tl.sum(tmp9, 1)[:, None]
    tmp11 = tl.full([XBLOCK, 1], 192, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
    tmp18 = tl.where(rmask & xmask, tmp16, 0)
    tmp19 = tl.sum(tmp18, 1)[:, None]
    tmp20 = tmp3 - tmp13
    tmp21 = 192.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tl.store(out_ptr2 + (r1 + 87936*x0), tmp33, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/by/cbyorpzkfb5j57xuhruc5v37xsyd22o6b23erdwnaeablmj5dghu.py
# Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_1 => mm_default_100
# Graph fragment:
#   %mm_default_100 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_98, %permute_1), kwargs = {})
triton_tem_fused_addmm_14 = async_compile.triton('triton_tem_fused_addmm_14', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_14', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_14(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 3266
    B = arg_B

    M = ks0
    N = 192
    K = 240
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3890
    stride_ak = 1
    stride_bk = 1
    stride_bn = 240

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/jg/cjgel7fyhmkogdfn64zo7qrerdcvzd5t7fcm2kqo57wxgrpm4wjp.py
# Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_2 => mm_default_99
# Graph fragment:
#   %mm_default_99 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_99, %permute_2), kwargs = {})
triton_tem_fused_addmm_15 = async_compile.triton('triton_tem_fused_addmm_15', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_15', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_15(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 3506
    B = arg_B

    M = ks0
    N = 192
    K = 192
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3890
    stride_ak = 1
    stride_bk = 1
    stride_bn = 192

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta1 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mb/cmbmqczokqhwugwsi646hqpkjk23h4aiagxkme4zf4cjc4gaeczu.py
# Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_3 => mm_default_98
# Graph fragment:
#   %mm_default_98 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_100, %permute_3), kwargs = {})
triton_tem_fused_addmm_16 = async_compile.triton('triton_tem_fused_addmm_16', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_16', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_16(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 3698
    B = arg_B

    M = ks0
    N = 192
    K = 192
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3890
    stride_ak = 1
    stride_bk = 1
    stride_bn = 192

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/63/c63pw22b3x7e3rwt3i5bqbcgukjaf3qucou357tmwy4i7qy472yi.py
# Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_4 => mm_default_97
# Graph fragment:
#   %mm_default_97 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_101, %permute_4), kwargs = {})
triton_tem_fused_addmm_17 = async_compile.triton('triton_tem_fused_addmm_17', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_17', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_17(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta2 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kv/ckvcbosx7vvarqh3fobad4eaycopb3qtpcy474dhiid6h2wber3b.py
# Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_5 => mm_default_96
# Graph fragment:
#   %mm_default_96 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_102, %permute_5), kwargs = {})
triton_tem_fused_addmm_18 = async_compile.triton('triton_tem_fused_addmm_18', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_18', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_18(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 96
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta3 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/o6/co6eooolwiokzu5zstxehd5upcjn6zbg3t7z52z56ngwv5lqjcit.py
# Topologically Sorted Source Nodes: [linear_6], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_6 => mm_default_95
# Graph fragment:
#   %mm_default_95 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_103, %permute_6), kwargs = {})
triton_tem_fused_addmm_19 = async_compile.triton('triton_tem_fused_addmm_19', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_19', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_19(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 168
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/nh/cnhjimbmpcdfuua2dq7unakmsgxgce2ixvvvxgjofyuqmaeeazxi.py
# Topologically Sorted Source Nodes: [linear_7], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_7 => mm_default_94
# Graph fragment:
#   %mm_default_94 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_104, %permute_7), kwargs = {})
triton_tem_fused_addmm_20 = async_compile.triton('triton_tem_fused_addmm_20', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_20', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_20(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 32
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 264
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta4 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kp/ckpaycqnocw6rxug7vxqg2gyaie7o7rslidmd3nfvfx26mvspiil.py
# Topologically Sorted Source Nodes: [linear_8], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_8 => mm_default_93
# Graph fragment:
#   %mm_default_93 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_105, %permute_8), kwargs = {})
triton_tem_fused_addmm_21 = async_compile.triton('triton_tem_fused_addmm_21', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_21', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_21(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 360
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/am/camojji4jw3vhhtt7h5snuzdon6lnfio5mn45y4iiymx3fssccs7.py
# Topologically Sorted Source Nodes: [linear_9], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_9 => mm_default_92
# Graph fragment:
#   %mm_default_92 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_106, %permute_9), kwargs = {})
triton_tem_fused_addmm_22 = async_compile.triton('triton_tem_fused_addmm_22', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_22', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_22(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 456
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/c5/cc5yi5xtilre3tcweibsjununi373ua4ldz7hiqlhsyo6gkqxpii.py
# Topologically Sorted Source Nodes: [linear_10], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_10 => mm_default_91
# Graph fragment:
#   %mm_default_91 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_107, %permute_10), kwargs = {})
triton_tem_fused_addmm_23 = async_compile.triton('triton_tem_fused_addmm_23', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_23', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_23(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 552
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/fz/cfz26yn35ak37e45gzymulkaropbwdkdzy4c6nhafrirq4hhxql6.py
# Topologically Sorted Source Nodes: [linear_11], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_11 => mm_default_90
# Graph fragment:
#   %mm_default_90 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_108, %permute_11), kwargs = {})
triton_tem_fused_addmm_24 = async_compile.triton('triton_tem_fused_addmm_24', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_24', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_24(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 624
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ny/cny3gmp5zhcgycf7wnnee5rn7ghgzxa7a5keorh2bytgcxlyb7si.py
# Topologically Sorted Source Nodes: [linear_12], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_12 => mm_default_89
# Graph fragment:
#   %mm_default_89 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_109, %permute_12), kwargs = {})
triton_tem_fused_addmm_25 = async_compile.triton('triton_tem_fused_addmm_25', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_25', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_25(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 720
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta5 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/m2/cm2gmaeualrz4ev6ifchzpe6eebdxdoqc4lgvilk6hygnav3d73l.py
# Topologically Sorted Source Nodes: [linear_13], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_13 => mm_default_88
# Graph fragment:
#   %mm_default_88 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_110, %permute_13), kwargs = {})
triton_tem_fused_addmm_26 = async_compile.triton('triton_tem_fused_addmm_26', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_26', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_26(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 816
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/3v/c3v7ihuautdiknqbao376hezqhdixmarpkhcjdmic3v4lfth5pzj.py
# Topologically Sorted Source Nodes: [linear_14], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_14 => mm_default_87
# Graph fragment:
#   %mm_default_87 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_111, %permute_14), kwargs = {})
triton_tem_fused_addmm_27 = async_compile.triton('triton_tem_fused_addmm_27', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_27', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_27(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 912
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/26/c26wem2v5plpoprzqba3zyexcawnhwewryr3ww7ng2tm6yezfz6b.py
# Topologically Sorted Source Nodes: [linear_15], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_15 => mm_default_86
# Graph fragment:
#   %mm_default_86 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_112, %permute_15), kwargs = {})
triton_tem_fused_addmm_28 = async_compile.triton('triton_tem_fused_addmm_28', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_28', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_28(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 984
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/5m/c5m2qvggew2ttipi5umlm5u6nudf7ie652x7szvlq3yqw5eitw5l.py
# Topologically Sorted Source Nodes: [linear_16], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_16 => mm_default_85
# Graph fragment:
#   %mm_default_85 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_113, %permute_16), kwargs = {})
triton_tem_fused_addmm_29 = async_compile.triton('triton_tem_fused_addmm_29', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_29', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_29(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 32
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1080
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta6 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/nb/cnb3v5zu44rbmr6sfl4yni2yut3sewq3ga767c5pqn3esoc5u5p6.py
# Topologically Sorted Source Nodes: [linear_17], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_17 => mm_default_84
# Graph fragment:
#   %mm_default_84 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_114, %permute_17), kwargs = {})
triton_tem_fused_addmm_30 = async_compile.triton('triton_tem_fused_addmm_30', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_30', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_30(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 1144
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/rk/crktosl5xupdl4ab5nkqbk7savle43watsomkz3g7ovhluarxnxb.py
# Topologically Sorted Source Nodes: [linear_18], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_18 => mm_default_83
# Graph fragment:
#   %mm_default_83 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_115, %permute_18), kwargs = {})
triton_tem_fused_addmm_31 = async_compile.triton('triton_tem_fused_addmm_31', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_31', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_31(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 32
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1240
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/j2/cj22ulgexsc2ywhitmltmdvwbgkfo35iq6mblbosp27dpxpnkfwt.py
# Topologically Sorted Source Nodes: [linear_19], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_19 => mm_default_82
# Graph fragment:
#   %mm_default_82 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_116, %permute_19), kwargs = {})
triton_tem_fused_addmm_32 = async_compile.triton('triton_tem_fused_addmm_32', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_32', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_32(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1304
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/x4/cx4udj7qi54ossjd3eskqafxbfm3e3fwomar5rb2fcddnkawrt23.py
# Topologically Sorted Source Nodes: [linear_20], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_20 => mm_default_81
# Graph fragment:
#   %mm_default_81 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_117, %permute_20), kwargs = {})
triton_tem_fused_addmm_33 = async_compile.triton('triton_tem_fused_addmm_33', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_33', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_33(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1376
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2l/c2lne6pucr7r4pfeudnk4bkz7sl5scqresjfiq5kuy4a3bh25iuj.py
# Topologically Sorted Source Nodes: [linear_21], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_21 => mm_default_80
# Graph fragment:
#   %mm_default_80 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_118, %permute_21), kwargs = {})
triton_tem_fused_addmm_34 = async_compile.triton('triton_tem_fused_addmm_34', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_34', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_34(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1448
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/eh/cehpfnpumuaali4rmfciya4d2n4m4npe67mcvrxf3dinkgpythpc.py
# Topologically Sorted Source Nodes: [linear_22], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_22 => mm_default_79
# Graph fragment:
#   %mm_default_79 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_119, %permute_22), kwargs = {})
triton_tem_fused_addmm_35 = async_compile.triton('triton_tem_fused_addmm_35', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_35', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_35(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1544
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/5s/c5szdn2f46wvwagj6dyp2mipjg2mnf73re2qy27psablwkng4vkh.py
# Topologically Sorted Source Nodes: [linear_23], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_23 => mm_default_78
# Graph fragment:
#   %mm_default_78 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_120, %permute_23), kwargs = {})
triton_tem_fused_addmm_36 = async_compile.triton('triton_tem_fused_addmm_36', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_36', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_36(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 1608
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta7 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/k3/ck3b32qkuco5scwqcszd7ewgmcvoerrteecu5qfwb3fhmeytjsjc.py
# Topologically Sorted Source Nodes: [linear_24], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_24 => mm_default_77
# Graph fragment:
#   %mm_default_77 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_121, %permute_24), kwargs = {})
triton_tem_fused_addmm_37 = async_compile.triton('triton_tem_fused_addmm_37', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_37', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_37(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 1680
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta8 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/tm/ctmcmvhxsnckhtwavgwmpznozugsanbpwmgkrjqhhjkxjyl4xb3v.py
# Topologically Sorted Source Nodes: [linear_25], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_25 => mm_default_76
# Graph fragment:
#   %mm_default_76 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_122, %permute_25), kwargs = {})
triton_tem_fused_addmm_38 = async_compile.triton('triton_tem_fused_addmm_38', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_38', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_38(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1752
    B = arg_B

    M = ks0
    N = 192
    K = 72
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 72

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/fi/cfiettg7jo5xdkqbogfbtr7tvmb26txsopqxa2g63hoffjomramu.py
# Topologically Sorted Source Nodes: [linear_26], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_26 => mm_default_75
# Graph fragment:
#   %mm_default_75 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_123, %permute_26), kwargs = {})
triton_tem_fused_addmm_39 = async_compile.triton('triton_tem_fused_addmm_39', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_39', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_39(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 32
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1824
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/yt/cytqnwxrv6ebp57giysunvvjcknm6greswhzsypwipqcqrkp3yu7.py
# Topologically Sorted Source Nodes: [linear_27], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_27 => mm_default_74
# Graph fragment:
#   %mm_default_74 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_124, %permute_27), kwargs = {})
triton_tem_fused_addmm_40 = async_compile.triton('triton_tem_fused_addmm_40', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_40', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_40(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 32
    BLOCK_N : tl.constexpr = 32
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 1888
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/cu/ccuwgmby45nzi4xgxexyakv4oszw6c64lzf5lg33h56tje3vca7c.py
# Topologically Sorted Source Nodes: [linear_28], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_28 => mm_default_73
# Graph fragment:
#   %mm_default_73 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_125, %permute_28), kwargs = {})
triton_tem_fused_addmm_41 = async_compile.triton('triton_tem_fused_addmm_41', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_41', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_41(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 32
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 1952
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/qd/cqd2duaocidtawo2ygha5qo5td6owy6yymrevqeuyecrxvccegx5.py
# Topologically Sorted Source Nodes: [linear_29], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_29 => mm_default_72
# Graph fragment:
#   %mm_default_72 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_126, %permute_29), kwargs = {})
triton_tem_fused_addmm_42 = async_compile.triton('triton_tem_fused_addmm_42', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_42', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_42(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 128
    A = arg_A + 2016
    B = arg_B

    M = ks0
    N = 192
    K = 144
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 144

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta9 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 128}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mi/cmin32owuwmw3eppxjavvux4ch2gfhtirh2toaauwitqonmucees.py
# Topologically Sorted Source Nodes: [linear_30], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_30 => mm_default_71
# Graph fragment:
#   %mm_default_71 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_127, %permute_30), kwargs = {})
triton_tem_fused_addmm_43 = async_compile.triton('triton_tem_fused_addmm_43', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_43', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_43(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 2160
    B = arg_B

    M = ks0
    N = 192
    K = 144
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 144

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/3p/c3pztumeoexcvu2zuu4k3tbhb6pgax5qommkbgozgaebd37adbig.py
# Topologically Sorted Source Nodes: [linear_31], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_31 => mm_default_70
# Graph fragment:
#   %mm_default_70 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_128, %permute_31), kwargs = {})
triton_tem_fused_addmm_44 = async_compile.triton('triton_tem_fused_addmm_44', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_44', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_44(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 2304
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kc/ckc7pexrz75ui6y247t3q2gbswwcoflmcmfzinnvxsi4q4ronm23.py
# Topologically Sorted Source Nodes: [linear_32], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_32 => mm_default_69
# Graph fragment:
#   %mm_default_69 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_129, %permute_32), kwargs = {})
triton_tem_fused_addmm_45 = async_compile.triton('triton_tem_fused_addmm_45', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_45', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_45(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 2368
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mq/cmqrbzu7wof5uunj45rk4jxa3o5u7ikmlkxqwpirsm4eblybsvmx.py
# Topologically Sorted Source Nodes: [linear_33], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_33 => mm_default_68
# Graph fragment:
#   %mm_default_68 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_130, %permute_33), kwargs = {})
triton_tem_fused_addmm_46 = async_compile.triton('triton_tem_fused_addmm_46', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_46', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_46(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 2432
    B = arg_B

    M = ks0
    N = 192
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/zq/czqwttauvx4yn24owvzhqvlot7swldos7tx4nqkjyl6i2xg5lpnz.py
# Topologically Sorted Source Nodes: [linear_34], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_34 => mm_default_67
# Graph fragment:
#   %mm_default_67 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_131, %permute_34), kwargs = {})
triton_tem_fused_addmm_47 = async_compile.triton('triton_tem_fused_addmm_47', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_47', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_47(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 2496
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kw/ckwee5n5zgwlyxk5245reouodipqhletmt2zjv7gh5imkmlylcvp.py
# Topologically Sorted Source Nodes: [linear_35], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_35 => mm_default_66
# Graph fragment:
#   %mm_default_66 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%getitem_132, %permute_35), kwargs = {})
triton_tem_fused_addmm_48 = async_compile.triton('triton_tem_fused_addmm_48', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_48', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_48(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A + 2592
    B = arg_B

    M = ks0
    N = 192
    K = 96
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 2688
    stride_ak = 1
    stride_bk = 1
    stride_bn = 96

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/yq/cyqhg2yzawowzar5ryqm7zp72v2sgdaliu5wugypkiwxrj53gvhg.py
# Topologically Sorted Source Nodes: [cat_default_11, linear_default], Original ATen: [aten.cat, aten.addmm]
# Source node to ATen node mapping:
#   cat_default_11 => cat_1
#   linear_default => constant_pad_nd_default_22
# Graph fragment:
#   %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_96, %pow_1], 1), kwargs = {})
#   %constant_pad_nd_default_22 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%cat_1, [0, 4, 0, 0]), kwargs = {})
triton_poi_fused_addmm_cat_49 = async_compile.triton('triton_poi_fused_addmm_cat_49', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[16777216],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_cat_49', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_addmm_cat_49(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 6056)
    x1 = xindex // 6056
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 6052, tl.int64)
    tmp2 = tmp0 < tmp1
    tmp3 = x0
    tmp4 = tl.full([1], 0, tl.int64)
    tmp5 = tmp3 >= tmp4
    tmp6 = tl.full([1], 3026, tl.int64)
    tmp7 = tmp3 < tmp6
    tmp8 = tmp7 & tmp2
    tmp9 = tl.load(in_ptr0 + (3890*x1 + (x0)), tmp8 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp10 = tmp3 >= tmp6
    tmp11 = tl.full([1], 6052, tl.int64)
    tmp12 = tmp3 < tmp11
    tmp13 = tmp10 & tmp2
    tmp14 = tl.load(in_ptr0 + (3890*x1 + ((-3026) + x0)), tmp13 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp15 = tmp14 * tmp14
    tmp16 = tl.full(tmp15.shape, 0.0, tmp15.dtype)
    tmp17 = tl.where(tmp13, tmp15, tmp16)
    tmp18 = tl.where(tmp7, tmp9, tmp17)
    tmp19 = tl.full(tmp18.shape, 0.0, tmp18.dtype)
    tmp20 = tl.where(tmp2, tmp18, tmp19)
    tl.store(out_ptr0 + (x2), tmp20, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/pa/cpaznz7bbmrkejh7illmefq3irahdnhqaultwa77gupd7w4nro5c.py
# Topologically Sorted Source Nodes: [cat_default_11, linear_default], Original ATen: [aten.cat, aten.addmm]
# Source node to ATen node mapping:
#   cat_default_11 => cat_1
#   linear_default => constant_pad_nd_default_22, mm_default_65
# Graph fragment:
#   %cat_1 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_96, %pow_1], 1), kwargs = {})
#   %constant_pad_nd_default_22 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%cat_1, [0, 4, 0, 0]), kwargs = {})
#   %mm_default_65 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%constant_pad_nd_default_22, %constant_pad_nd_default_23), kwargs = {})
triton_tem_fused_addmm_cat_50 = async_compile.triton('triton_tem_fused_addmm_cat_50', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_cat_50', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_cat_50(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 256
    K = 6056
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 6056
    stride_ak = 1
    stride_bk = 256
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 256*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/yr/cyrpji4zmdlgqbg7ibwzq3thhndf7n5wwy2rnrzpfsbaxuhya4a2.py
# Topologically Sorted Source Nodes: [linear_default, layer_norm_36, sigmoid, mul_477], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_36 => add_998, add_999, convert_element_type_192, convert_element_type_193, mul_599, mul_600, rsqrt_36, sub_343, var_mean_36
#   linear_default => add_tensor_65
#   mul_477 => mul_635
#   sigmoid => sigmoid
# Graph fragment:
#   %add_tensor_65 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_65, %cat_3), kwargs = {})
#   %convert_element_type_192 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_65, torch.float32), kwargs = {})
#   %var_mean_36 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_192, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_343 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_192, %getitem_206), kwargs = {})
#   %add_998 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_205, 1e-05), kwargs = {})
#   %rsqrt_36 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_998,), kwargs = {})
#   %mul_599 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_343, %rsqrt_36), kwargs = {})
#   %mul_600 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_599, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale), kwargs = {})
#   %add_999 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_600, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_193 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_999, torch.float16), kwargs = {})
#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_193,), kwargs = {})
#   %mul_635 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_65, %sigmoid), kwargs = {})
triton_per_fused_addmm_mul_native_layer_norm_sigmoid_51 = async_compile.triton('triton_per_fused_addmm_mul_native_layer_norm_sigmoid_51', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_addmm_mul_native_layer_norm_sigmoid_51', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_addmm_mul_native_layer_norm_sigmoid_51(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 256
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = tl.full([RBLOCK], True, tl.int1)
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (r1 + 256*x0), None).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp24 = tl.load(in_ptr1 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [RBLOCK])
    tmp6 = tl.broadcast_to(tmp4, [RBLOCK])
    tmp8 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
    tmp9 = tl.full([1], 256, tl.int32)
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp8 / tmp10
    tmp12 = tmp4 - tmp11
    tmp13 = tmp12 * tmp12
    tmp14 = tl.broadcast_to(tmp13, [RBLOCK])
    tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp14, 0))
    tmp17 = tmp3 - tmp11
    tmp18 = 256.0
    tmp19 = tmp16 / tmp18
    tmp20 = 1e-05
    tmp21 = tmp19 + tmp20
    tmp22 = libdevice.rsqrt(tmp21)
    tmp23 = tmp17 * tmp22
    tmp25 = tmp24.to(tl.float32)
    tmp26 = tmp23 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 + tmp28
    tmp30 = tmp29.to(tl.float32)
    tmp31 = tl.sigmoid(tmp30)
    tmp32 = tmp2 * tmp31
    tl.store(in_out_ptr0 + (r1 + 256*x0), tmp32, None)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/4d/c4dkrav5jhxbdz2bhyv6g6pxavfgahfrte2cfw4gkcjmyw3e53zq.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_52 = async_compile.triton('triton_poi_fused_cat_52', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[131072],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_52', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_52(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 36)
    x1 = xindex // 36
    tmp0 = tl.load(in_ptr0 + (x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ma/cmabfy2sxhk2c3wvhqalbi7zdmq7vfftgszk3w2tkimeuml53fre.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_53 = async_compile.triton('triton_poi_fused_cat_53', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[65536],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0,), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_53', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_53(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 28)
    x1 = xindex // 28
    tmp0 = tl.load(in_ptr0 + (x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/6e/c6eeifmourj75c4qw7wtxvjgiss5lcojiiofwp2cd2ccp2s4rrm5.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_54 = async_compile.triton('triton_poi_fused_cat_54', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_54', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_54(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 228)
    x1 = xindex // 228
    tmp0 = tl.load(in_ptr0 + (36 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/rl/crlwdzpojoutmsodrq4es2h3duy3tu5ukqkqwyt46l7amq74lh74.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_55 = async_compile.triton('triton_poi_fused_cat_55', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[32768],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_55', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_55(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 16)
    x1 = xindex // 16
    tmp0 = tl.load(in_ptr0 + (28 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/wx/cwxgjaphwuqqzfpzeqhbgng7plxrhquolk5eiowtnjsfsmheceyf.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_56 = async_compile.triton('triton_poi_fused_cat_56', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[65536],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_56', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_56(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp0 = tl.load(in_ptr0 + (264 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/yb/cyb4t5ehnscjvcohbi6ezi3r6yclxjo5curbvwgim7pp3niten3e.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_57 = async_compile.triton('triton_poi_fused_cat_57', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[65536],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_57', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_57(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp0 = tl.load(in_ptr0 + (44 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/4x/c4xjziwv7o3ub2bduwquj3pxqlegfz3ioelaehggd5onxyfb7cfu.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_58 = async_compile.triton('triton_poi_fused_cat_58', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[65536],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_58', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_58(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp0 = tl.load(in_ptr0 + (296 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/f6/cf6x3gy3stj3sazfw63vgbbtmny5cx3sczjsniafro3w5buhawxs.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_59 = async_compile.triton('triton_poi_fused_cat_59', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[65536],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_59', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_59(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp0 = tl.load(in_ptr0 + (76 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/5k/c5kk46xd4xfjpxgdwte7ob4yhdtdqsrvi6u4kte4rwobq35yqluv.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_60 = async_compile.triton('triton_poi_fused_cat_60', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[262144],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_60', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_60(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 80)
    x1 = xindex // 80
    tmp0 = tl.load(in_ptr0 + (328 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/42/c42d2mxfn6adm44scs44obrf5xbym4yaunkebkn3wr4fnls4kqua.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_61 = async_compile.triton('triton_poi_fused_cat_61', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[32768],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_61', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_61(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 16)
    x1 = xindex // 16
    tmp0 = tl.load(in_ptr0 + (108 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ue/cuehhbepm6cgvkz5gj7bh3wvrhbxo7eu76qjpctpg5qkk4rueg4p.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_62 = async_compile.triton('triton_poi_fused_cat_62', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[65536],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_62', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_62(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp0 = tl.load(in_ptr0 + (408 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/tt/cttrxxnwrpuzmfawkbyfbpzuwc3uiepiylubt2suxaebexqoyylr.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_63 = async_compile.triton('triton_poi_fused_cat_63', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[65536],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_63', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_63(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 32)
    x1 = xindex // 32
    tmp0 = tl.load(in_ptr0 + (124 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/46/c464l2b6g65qs6grufrl643cxh6dwdb6lshdkbxa5s242nw6ivkx.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_64 = async_compile.triton('triton_poi_fused_cat_64', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[32768],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_64', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_64(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 16)
    x1 = xindex // 16
    tmp0 = tl.load(in_ptr0 + (440 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/b2/cb22uazhhkzh34ozoebqr4ols6wsttqvv2e6kxtzbqmewmyj5egc.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_65 = async_compile.triton('triton_poi_fused_cat_65', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[32768],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_65', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_65(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 16)
    x1 = xindex // 16
    tmp0 = tl.load(in_ptr0 + (156 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/zp/czpow6wicd2miuevgqj73uim4euepvgeega65ls3ehj4hvn2z2f3.py
# Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_9 => cat_4
# Graph fragment:
#   %cat_4 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem, %getitem_53, %getitem_1, %getitem_54, %getitem_2, %getitem_55, %getitem_3, %getitem_56, %getitem_4, %getitem_57, %getitem_5, %getitem_58, %getitem_6, %getitem_59, %getitem_7], 1), kwargs = {})
triton_poi_fused_cat_66 = async_compile.triton('triton_poi_fused_cat_66', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[262144],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0,), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_66', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_66(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 72)
    x1 = xindex // 72
    tmp0 = tl.load(in_ptr0 + (456 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 700*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/vj/cvjt3dza2dk4gykjvycktsq3sldag6kr7i7vzlterjncsoxgu44o.py
# Topologically Sorted Source Nodes: [linear_default_1], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_default_1 => addmm_37
# Graph fragment:
#   %addmm_37 : [num_users=3] = call_function[target=torch.ops.aten.addmm.default](args = (%cat_6, %cat_4, %permute_37), kwargs = {})
triton_tem_fused_addmm_67 = async_compile.triton('triton_tem_fused_addmm_67', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_67', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_67(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = ks0
    N = 512
    K = 700
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 700
    stride_ak = 1
    stride_bk = 1
    stride_bn = 700

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 512*idx_m
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/sa/csaoducsrbspoknfy2jdpwuv2zdycxn25nhasgjsvtjqevpp35kt.py
# Topologically Sorted Source Nodes: [contiguous_2, layer_norm_37, sigmoid_1, mul_508], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   contiguous_2 => clone
#   layer_norm_37 => add_1064, add_1065, convert_element_type_202, convert_element_type_203, mul_638, mul_639, rsqrt_37, sub_364, var_mean_37
#   mul_508 => mul_682
#   sigmoid_1 => sigmoid_1
# Graph fragment:
#   %clone : [num_users=2] = call_function[target=torch.ops.aten.clone.default](args = (%slice_2,), kwargs = {memory_format: torch.contiguous_format})
#   %convert_element_type_202 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%clone, torch.float32), kwargs = {})
#   %var_mean_37 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_202, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_364 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_202, %getitem_208), kwargs = {})
#   %add_1064 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_207, 1e-05), kwargs = {})
#   %rsqrt_37 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1064,), kwargs = {})
#   %mul_638 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_364, %rsqrt_37), kwargs = {})
#   %mul_639 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_638, %submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale), kwargs = {})
#   %add_1065 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_639, %submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_203 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1065, torch.float16), kwargs = {})
#   %sigmoid_1 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_203,), kwargs = {})
#   %mul_682 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%clone, %sigmoid_1), kwargs = {})
triton_per_fused_clone_mul_native_layer_norm_sigmoid_68 = async_compile.triton('triton_per_fused_clone_mul_native_layer_norm_sigmoid_68', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_clone_mul_native_layer_norm_sigmoid_68', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 3, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_clone_mul_native_layer_norm_sigmoid_68(in_ptr0, in_ptr1, in_ptr2, out_ptr2, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 256
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = tl.full([RBLOCK], True, tl.int1)
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + 512*x0), None).to(tl.float32)
    tmp22 = tl.load(in_ptr1 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp25 = tl.load(in_ptr2 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp2 = tl.broadcast_to(tmp1, [RBLOCK])
    tmp4 = tl.broadcast_to(tmp2, [RBLOCK])
    tmp6 = triton_helpers.promote_to_tensor(tl.sum(tmp4, 0))
    tmp7 = tl.full([1], 256, tl.int32)
    tmp8 = tmp7.to(tl.float32)
    tmp9 = tmp6 / tmp8
    tmp10 = tmp2 - tmp9
    tmp11 = tmp10 * tmp10
    tmp12 = tl.broadcast_to(tmp11, [RBLOCK])
    tmp14 = triton_helpers.promote_to_tensor(tl.sum(tmp12, 0))
    tmp15 = tmp1 - tmp9
    tmp16 = 256.0
    tmp17 = tmp14 / tmp16
    tmp18 = 1e-05
    tmp19 = tmp17 + tmp18
    tmp20 = libdevice.rsqrt(tmp19)
    tmp21 = tmp15 * tmp20
    tmp23 = tmp22.to(tl.float32)
    tmp24 = tmp21 * tmp23
    tmp26 = tmp25.to(tl.float32)
    tmp27 = tmp24 + tmp26
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tl.sigmoid(tmp28)
    tmp30 = tmp0 * tmp29
    tl.store(out_ptr2 + (r1 + 256*x0), tmp30, None)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/5q/c5qkvhqyhyyvtpagy5nmgdzkoc4znzddc4aikth6cqvgmpfahvju.py
# Topologically Sorted Source Nodes: [add_779, layer_norm_38], Original ATen: [aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_779 => add_1095
#   layer_norm_38 => add_1112, add_1113, convert_element_type_210, convert_element_type_211, mul_662, mul_663, rsqrt_38, sub_378, var_mean_38
# Graph fragment:
#   %add_1095 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%arg613_1, %submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb), kwargs = {})
#   %convert_element_type_210 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1095, torch.float32), kwargs = {})
#   %var_mean_38 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_210, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_378 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_210, %getitem_210), kwargs = {})
#   %add_1112 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_209, 1e-05), kwargs = {})
#   %rsqrt_38 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1112,), kwargs = {})
#   %mul_662 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_378, %rsqrt_38), kwargs = {})
#   %mul_663 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_662, %submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight), kwargs = {})
#   %add_1113 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_663, %submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias), kwargs = {})
#   %convert_element_type_211 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1113, torch.float16), kwargs = {})
triton_per_fused_add_native_layer_norm_69 = async_compile.triton('triton_per_fused_add_native_layer_norm_69', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[524288, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_layer_norm_69', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_add_native_layer_norm_69(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 64
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = tl.full([XBLOCK, RBLOCK], True, tl.int1)
    r2 = rindex
    x3 = xindex
    x0 = (xindex % 200)
    tmp0 = tl.load(in_ptr0 + (r2 + 64*x3), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (r2 + 64*x0), xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r2), None, eviction_policy='evict_last').to(tl.float32)
    tmp30 = tl.load(in_ptr3 + (r2), None, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
    tmp6 = tl.where(xmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
    tmp9 = tl.where(xmask, tmp7, 0)
    tmp10 = tl.sum(tmp9, 1)[:, None]
    tmp11 = tl.full([XBLOCK, 1], 64, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
    tmp18 = tl.where(xmask, tmp16, 0)
    tmp19 = tl.sum(tmp18, 1)[:, None]
    tmp20 = tmp3 - tmp13
    tmp21 = 64.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tl.store(out_ptr2 + (r2 + 64*x3), tmp33, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/xj/cxjovl5xg5bkpx2hmtfxv2d5m2nkzvuelqapvz5ozgu3hctt34nx.py
# Topologically Sorted Source Nodes: [repeat, layer_norm_40], Original ATen: [aten.repeat, aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_40 => add_1188, add_1189, convert_element_type_224, convert_element_type_225, mul_725, mul_726, rsqrt_40, sub_400, var_mean_40
#   repeat => repeat
# Graph fragment:
#   %repeat : [num_users=3] = call_function[target=torch.ops.aten.repeat.default](args = (%submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb, [%sym_size_int, 1, 1]), kwargs = {})
#   %convert_element_type_224 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%repeat, torch.float32), kwargs = {})
#   %var_mean_40 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_224, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_400 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_224, %getitem_214), kwargs = {})
#   %add_1188 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_213, 1e-05), kwargs = {})
#   %rsqrt_40 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1188,), kwargs = {})
#   %mul_725 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_400, %rsqrt_40), kwargs = {})
#   %mul_726 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_725, %submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight), kwargs = {})
#   %add_1189 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_726, %submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias), kwargs = {})
#   %convert_element_type_225 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1189, torch.float16), kwargs = {})
triton_per_fused_native_layer_norm_repeat_70 = async_compile.triton('triton_per_fused_native_layer_norm_repeat_70', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[65536, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_layer_norm_repeat_70', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_native_layer_norm_repeat_70(in_ptr0, in_ptr1, in_ptr2, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 64
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = tl.full([XBLOCK, RBLOCK], True, tl.int1)
    r2 = rindex
    x0 = (xindex % 32)
    x3 = xindex
    tmp0 = tl.load(in_ptr0 + (r2 + 64*x0), xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp25 = tl.load(in_ptr1 + (r2), None, eviction_policy='evict_last').to(tl.float32)
    tmp28 = tl.load(in_ptr2 + (r2), None, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])
    tmp4 = tl.where(xmask, tmp2, 0)
    tmp5 = tl.broadcast_to(tmp2, [XBLOCK, RBLOCK])
    tmp7 = tl.where(xmask, tmp5, 0)
    tmp8 = tl.sum(tmp7, 1)[:, None]
    tmp9 = tl.full([XBLOCK, 1], 64, tl.int32)
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp8 / tmp10
    tmp12 = tmp2 - tmp11
    tmp13 = tmp12 * tmp12
    tmp14 = tl.broadcast_to(tmp13, [XBLOCK, RBLOCK])
    tmp16 = tl.where(xmask, tmp14, 0)
    tmp17 = tl.sum(tmp16, 1)[:, None]
    tmp18 = tmp1 - tmp11
    tmp19 = 64.0
    tmp20 = tmp17 / tmp19
    tmp21 = 1e-05
    tmp22 = tmp20 + tmp21
    tmp23 = libdevice.rsqrt(tmp22)
    tmp24 = tmp18 * tmp23
    tmp26 = tmp25.to(tl.float32)
    tmp27 = tmp24 * tmp26
    tmp29 = tmp28.to(tl.float32)
    tmp30 = tmp27 + tmp29
    tmp31 = tmp30.to(tl.float32)
    tl.store(out_ptr2 + (r2 + 64*x3), tmp31, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/wu/cwuzckmm2yn66gnkj3ksztttznrmsfoxkha3pg3vvmwcddwswi4y.py
# Topologically Sorted Source Nodes: [linear_default, layer_norm_36, sigmoid, mul_477, linear_42], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_36 => add_998, add_999, convert_element_type_192, convert_element_type_193, mul_599, mul_600, rsqrt_36, sub_343, var_mean_36
#   linear_42 => mm_default_64
#   linear_default => add_tensor_65
#   mul_477 => mul_635
#   sigmoid => sigmoid
# Graph fragment:
#   %add_tensor_65 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_65, %cat_3), kwargs = {})
#   %convert_element_type_192 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_65, torch.float32), kwargs = {})
#   %var_mean_36 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_192, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_343 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_192, %getitem_206), kwargs = {})
#   %add_998 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_205, 1e-05), kwargs = {})
#   %rsqrt_36 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_998,), kwargs = {})
#   %mul_599 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_343, %rsqrt_36), kwargs = {})
#   %mul_600 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_599, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale), kwargs = {})
#   %add_999 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_600, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_193 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_999, torch.float16), kwargs = {})
#   %sigmoid : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_193,), kwargs = {})
#   %mul_635 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_65, %sigmoid), kwargs = {})
#   %mm_default_64 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%mul_635, %permute_40), kwargs = {})
triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_71 = async_compile.triton('triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_71', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_71', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_71(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = ks0
    N = 3026
    K = 256
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 256
    stride_ak = 1
    stride_bk = 1
    stride_bn = 256

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 3026*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta10 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/me/cmeevgwvuxj33tfdwxli45ua2yivclsdesnb7e7fvftxk566gvnu.py
# Topologically Sorted Source Nodes: [linear_42, sigmoid_2, mul_497], Original ATen: [aten.addmm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   linear_42 => add_tensor_64
#   mul_497 => mul_659
#   sigmoid_2 => sigmoid_2
# Graph fragment:
#   %add_tensor_64 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_64, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b), kwargs = {})
#   %sigmoid_2 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_tensor_64,), kwargs = {})
#   %mul_659 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_96, %sigmoid_2), kwargs = {})
triton_poi_fused_addmm_mul_sigmoid_72 = async_compile.triton('triton_poi_fused_addmm_mul_sigmoid_72', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_mul_sigmoid_72', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_addmm_mul_sigmoid_72(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 3026)
    x1 = xindex // 3026
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 3890*x1), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tmp1 + tmp2
    tmp4 = tl.sigmoid(tmp3)
    tmp5 = tmp0 * tmp4
    tl.store(out_ptr0 + (x0 + 3282*x1), tmp5, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/in/cinbuferw6w5wd5pieyosjklkd7z7irw74jypx6si2sbqddouh2e.py
# Topologically Sorted Source Nodes: [linear_42, sigmoid_2, mul_497, linear_43], Original ATen: [aten.addmm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   linear_42 => add_tensor_64
#   linear_43 => addmm_41
#   mul_497 => mul_659
#   sigmoid_2 => sigmoid_2
# Graph fragment:
#   %add_tensor_64 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_64, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b), kwargs = {})
#   %sigmoid_2 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_tensor_64,), kwargs = {})
#   %mul_659 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%getitem_96, %sigmoid_2), kwargs = {})
#   %addmm_41 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b, %mul_659, %permute_41), kwargs = {})
triton_tem_fused_addmm_mul_sigmoid_73 = async_compile.triton('triton_tem_fused_addmm_mul_sigmoid_73', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=8,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_mul_sigmoid_73', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_mul_sigmoid_73(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 256
    K = 3026
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3282
    stride_ak = 1
    stride_bk = 1
    stride_bn = 3026

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 256*idx_m
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/qf/cqfeu4v4xwuq7druiptphayd4wpoomyn4ozouuj7hwqqqx6sugle.py
# Topologically Sorted Source Nodes: [cat_default_8], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_8 => cat_7
# Graph fragment:
#   %cat_7 : [num_users=2] = call_function[target=torch.ops.aten.cat.default](args = ([%addmm_41, %mul_659], 1), kwargs = {})
triton_poi_fused_cat_74 = async_compile.triton('triton_poi_fused_cat_74', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_74', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_74(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 256)
    x1 = xindex // 256
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 3282*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ww/cww7nceuhcvaq2xo63ebs3etqx2hzj4syb5jdma2g44sdarlln7h.py
# Topologically Sorted Source Nodes: [layer_norm_42, sigmoid_3, mul_567], Original ATen: [aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_42 => add_1236, add_1237, convert_element_type_234, convert_element_type_235, mul_761, mul_762, rsqrt_42, sub_414, var_mean_42
#   mul_567 => mul_809
#   sigmoid_3 => sigmoid_3
# Graph fragment:
#   %convert_element_type_234 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_7, torch.float32), kwargs = {})
#   %var_mean_42 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_234, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_414 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_234, %getitem_218), kwargs = {})
#   %add_1236 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_217, 1e-05), kwargs = {})
#   %rsqrt_42 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1236,), kwargs = {})
#   %mul_761 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_414, %rsqrt_42), kwargs = {})
#   %mul_762 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_761, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale), kwargs = {})
#   %add_1237 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_762, %submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_235 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1237, torch.float16), kwargs = {})
#   %sigmoid_3 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_235,), kwargs = {})
#   %mul_809 : [num_users=5] = call_function[target=torch.ops.aten.mul.Tensor](args = (%cat_7, %sigmoid_3), kwargs = {})
triton_red_fused_mul_native_layer_norm_sigmoid_75 = async_compile.triton('triton_red_fused_mul_native_layer_norm_sigmoid_75', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_mul_native_layer_norm_sigmoid_75', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_mul_native_layer_norm_sigmoid_75(in_ptr0, in_ptr1, in_ptr2, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 3282
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (r1 + 3282*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(rmask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(rmask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(rmask & xmask, tmp3_weight_next, tmp3_weight)
    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(
        tmp3_mean, tmp3_m2, tmp3_weight, 1
    )
    tmp3 = tmp3_tmp[:, None]
    tmp4 = tmp4_tmp[:, None]
    tmp5 = tmp5_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp6 = tl.load(in_ptr0 + (r1 + 3282*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tmp7 - tmp3
        tmp9 = 3282.0
        tmp10 = tmp4 / tmp9
        tmp11 = 1e-05
        tmp12 = tmp10 + tmp11
        tmp13 = libdevice.rsqrt(tmp12)
        tmp14 = tmp8 * tmp13
        tmp16 = tmp15.to(tl.float32)
        tmp17 = tmp14 * tmp16
        tmp19 = tmp18.to(tl.float32)
        tmp20 = tmp17 + tmp19
        tmp21 = tmp20.to(tl.float32)
        tmp22 = tl.sigmoid(tmp21)
        tmp23 = tmp6 * tmp22
        tl.store(out_ptr2 + (r1 + 3282*x0), tmp23, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2z/c2zxk4utigrgge36p5gypo6ok4zeyxqmvdwofhvl7ifdsfwyw4mb.py
# Topologically Sorted Source Nodes: [contiguous_2, layer_norm_37, sigmoid_1, mul_508, linear_46], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul, aten.addmm]
# Source node to ATen node mapping:
#   contiguous_2 => clone
#   layer_norm_37 => add_1064, add_1065, convert_element_type_202, convert_element_type_203, mul_638, mul_639, rsqrt_37, sub_364, var_mean_37
#   linear_46 => mm_default_63
#   mul_508 => mul_682
#   sigmoid_1 => sigmoid_1
# Graph fragment:
#   %clone : [num_users=2] = call_function[target=torch.ops.aten.clone.default](args = (%slice_2,), kwargs = {memory_format: torch.contiguous_format})
#   %convert_element_type_202 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%clone, torch.float32), kwargs = {})
#   %var_mean_37 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_202, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_364 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_202, %getitem_208), kwargs = {})
#   %add_1064 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_207, 1e-05), kwargs = {})
#   %rsqrt_37 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1064,), kwargs = {})
#   %mul_638 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_364, %rsqrt_37), kwargs = {})
#   %mul_639 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_638, %submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale), kwargs = {})
#   %add_1065 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_639, %submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_203 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1065, torch.float16), kwargs = {})
#   %sigmoid_1 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_203,), kwargs = {})
#   %mul_682 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%clone, %sigmoid_1), kwargs = {})
#   %mm_default_63 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%mul_682, %permute_44), kwargs = {})
triton_tem_fused_addmm_clone_mul_native_layer_norm_sigmoid_76 = async_compile.triton('triton_tem_fused_addmm_clone_mul_native_layer_norm_sigmoid_76', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_clone_mul_native_layer_norm_sigmoid_76', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_clone_mul_native_layer_norm_sigmoid_76(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = ks0
    N = 1024
    K = 256
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 256
    stride_ak = 1
    stride_bk = 1
    stride_bn = 256

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1024*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ux/cuxivnbslg5o7jgoeu6jbwuu54mjmpp5dazfsu2b3pryeprp33ns.py
# Topologically Sorted Source Nodes: [linear_46, layer_norm_43, sigmoid_4, mul_570], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_43 => add_1247, add_1248, convert_element_type_236, convert_element_type_237, mul_767, mul_768, rsqrt_43, sub_418, var_mean_43
#   linear_46 => add_tensor_63
#   mul_570 => mul_812
#   sigmoid_4 => sigmoid_4
# Graph fragment:
#   %add_tensor_63 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_63, %submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b), kwargs = {})
#   %convert_element_type_236 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_63, torch.float32), kwargs = {})
#   %var_mean_43 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_236, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_418 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_236, %getitem_220), kwargs = {})
#   %add_1247 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_219, 1e-05), kwargs = {})
#   %rsqrt_43 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1247,), kwargs = {})
#   %mul_767 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_418, %rsqrt_43), kwargs = {})
#   %mul_768 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_767, %submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale), kwargs = {})
#   %add_1248 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_768, %submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_237 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1248, torch.float16), kwargs = {})
#   %sigmoid_4 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_237,), kwargs = {})
#   %mul_812 : [num_users=4] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_63, %sigmoid_4), kwargs = {})
triton_per_fused_addmm_mul_native_layer_norm_sigmoid_77 = async_compile.triton('triton_per_fused_addmm_mul_native_layer_norm_sigmoid_77', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_addmm_mul_native_layer_norm_sigmoid_77', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_addmm_mul_native_layer_norm_sigmoid_77(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 1024
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = tl.full([RBLOCK], True, tl.int1)
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (r1 + 1024*x0), None).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp24 = tl.load(in_ptr1 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [RBLOCK])
    tmp6 = tl.broadcast_to(tmp4, [RBLOCK])
    tmp8 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
    tmp9 = tl.full([1], 1024, tl.int32)
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp8 / tmp10
    tmp12 = tmp4 - tmp11
    tmp13 = tmp12 * tmp12
    tmp14 = tl.broadcast_to(tmp13, [RBLOCK])
    tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp14, 0))
    tmp17 = tmp3 - tmp11
    tmp18 = 1024.0
    tmp19 = tmp16 / tmp18
    tmp20 = 1e-05
    tmp21 = tmp19 + tmp20
    tmp22 = libdevice.rsqrt(tmp21)
    tmp23 = tmp17 * tmp22
    tmp25 = tmp24.to(tl.float32)
    tmp26 = tmp23 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 + tmp28
    tmp30 = tmp29.to(tl.float32)
    tmp31 = tl.sigmoid(tmp30)
    tmp32 = tmp2 * tmp31
    tl.store(in_out_ptr0 + (r1 + 1024*x0), tmp32, None)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/3h/c3hwrdaiz5ek22jg4almgb3h4dun4bmj3sdstgbtbynve2omrg7b.py
# Topologically Sorted Source Nodes: [linear_47], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_47 => addmm_43
# Graph fragment:
#   %addmm_43 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias, %view_4, %permute_45), kwargs = {})
triton_tem_fused_addmm_78 = async_compile.triton('triton_tem_fused_addmm_78', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=2,
    num_warps=4,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_78', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_78(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = 32*ks0
    N = 64
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 64
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 64*idx_m
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')
meta11 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/p5/cp5oy5y3pl6m7a3cmt37rtyh6q5i2mqnm3gfsbouqsdwgoyld4jf.py
# Topologically Sorted Source Nodes: [linear_44], Original ATen: [aten.mm]
# Source node to ATen node mapping:
#   linear_44 => mm
# Graph fragment:
#   %mm : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view, %permute_42), kwargs = {})
triton_tem_fused_mm_79 = async_compile.triton('triton_tem_fused_mm_79', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_mm_79', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_mm_79(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = 200*ks0
    N = 64
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 64
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 64*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/rx/crxhfoscda6unohl4wpob7a5o3w4e7mimfczj5o7qlwlk4wqazoe.py
# Topologically Sorted Source Nodes: [repeat, add_976, layer_norm_44], Original ATen: [aten.repeat, aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_976 => add_1419
#   layer_norm_44 => add_1433, add_1434, convert_element_type_241, convert_element_type_242, mul_871, mul_872, rsqrt_44, sub_467, var_mean_44
#   repeat => repeat
# Graph fragment:
#   %repeat : [num_users=3] = call_function[target=torch.ops.aten.repeat.default](args = (%submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb, [%sym_size_int, 1, 1]), kwargs = {})
#   %add_1419 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_14, %repeat), kwargs = {})
#   %convert_element_type_241 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1419, torch.float32), kwargs = {})
#   %var_mean_44 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_241, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_467 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_241, %getitem_240), kwargs = {})
#   %add_1433 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_239, 1e-05), kwargs = {})
#   %rsqrt_44 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1433,), kwargs = {})
#   %mul_871 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_467, %rsqrt_44), kwargs = {})
#   %mul_872 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_871, %submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight), kwargs = {})
#   %add_1434 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_872, %submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias), kwargs = {})
#   %convert_element_type_242 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1434, torch.float16), kwargs = {})
triton_per_fused_add_native_layer_norm_repeat_80 = async_compile.triton('triton_per_fused_add_native_layer_norm_repeat_80', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[65536, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_layer_norm_repeat_80', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_add_native_layer_norm_repeat_80(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 64
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = tl.full([XBLOCK, RBLOCK], True, tl.int1)
    r2 = rindex
    x3 = xindex
    x0 = (xindex % 32)
    tmp0 = tl.load(in_ptr0 + (r2 + 64*x3), xmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (r2 + 64*x0), xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r2), None, eviction_policy='evict_last').to(tl.float32)
    tmp30 = tl.load(in_ptr3 + (r2), None, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
    tmp6 = tl.where(xmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
    tmp9 = tl.where(xmask, tmp7, 0)
    tmp10 = tl.sum(tmp9, 1)[:, None]
    tmp11 = tl.full([XBLOCK, 1], 64, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
    tmp18 = tl.where(xmask, tmp16, 0)
    tmp19 = tl.sum(tmp18, 1)[:, None]
    tmp20 = tmp3 - tmp13
    tmp21 = 64.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tl.store(out_ptr2 + (r2 + 64*x3), tmp33, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/y6/cy67mtq5sewa5m46jt7oo4pnwo4y6rke4hmwwdf6wtbepbxmlhwz.py
# Topologically Sorted Source Nodes: [linear_56], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_56 => mm_default_62
# Graph fragment:
#   %mm_default_62 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_18, %permute_57), kwargs = {})
triton_tem_fused_addmm_81 = async_compile.triton('triton_tem_fused_addmm_81', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_81', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_81(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = 32*ks0
    N = 128
    K = 64
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 64
    stride_ak = 1
    stride_bk = 1
    stride_bn = 64

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 128*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta12 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ld/cldjezlflg2pvidltwqktbxx7qqm6vxw5bhyzeibkyhyqhcrizby.py
# Topologically Sorted Source Nodes: [gelu_1], Original ATen: [aten.gelu]
# Source node to ATen node mapping:
#   gelu_1 => add_1486, convert_element_type_253, convert_element_type_254, erf_1, mul_912, mul_913, mul_914
# Graph fragment:
#   %convert_element_type_253 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%view_19, torch.float32), kwargs = {})
#   %mul_912 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_253, 0.5), kwargs = {})
#   %mul_913 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_253, 0.7071067811865476), kwargs = {})
#   %erf_1 : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_913,), kwargs = {})
#   %add_1486 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf_1, 1), kwargs = {})
#   %mul_914 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_912, %add_1486), kwargs = {})
#   %convert_element_type_254 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_914, torch.float16), kwargs = {})
triton_poi_fused_gelu_82 = async_compile.triton('triton_poi_fused_gelu_82', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_gelu_82', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_gelu_82(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x2 = xindex
    x0 = (xindex % 128)
    tmp0 = tl.load(in_out_ptr0 + (x2), None).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), None, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = 0.5
    tmp5 = tmp3 * tmp4
    tmp6 = 0.7071067811865476
    tmp7 = tmp3 * tmp6
    tmp8 = libdevice.erf(tmp7)
    tmp9 = 1.0
    tmp10 = tmp8 + tmp9
    tmp11 = tmp5 * tmp10
    tmp12 = tmp11.to(tl.float32)
    tl.store(in_out_ptr0 + (x2), tmp12, None)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/q5/cq5v6nz5r222o33ydczm24vrsajwsoeot6vvocqoodbcsu52it25.py
# Topologically Sorted Source Nodes: [linear_58], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_58 => mm_default_61
# Graph fragment:
#   %mm_default_61 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_22, %permute_59), kwargs = {})
triton_tem_fused_addmm_83 = async_compile.triton('triton_tem_fused_addmm_83', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_83', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_83(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = 32*ks0
    N = 64
    K = 128
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 128
    stride_ak = 1
    stride_bk = 1
    stride_bn = 128

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 64*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ay/caytggxyzzsxin6oqpzzoxt47znbhshn2w326d7delz5ky6p3mn3.py
# Topologically Sorted Source Nodes: [linear_default_2], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_default_2 => constant_pad_nd_default_20
# Graph fragment:
#   %constant_pad_nd_default_20 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%mul_809, [0, 6, 0, 0]), kwargs = {})
triton_poi_fused_addmm_84 = async_compile.triton('triton_poi_fused_addmm_84', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_84', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_addmm_84(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 3288)
    x1 = xindex // 3288
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 3282, tl.int64)
    tmp2 = tmp0 < tmp1
    tmp3 = tl.load(in_ptr0 + (x0 + 3282*x1), tmp2 & xmask, other=0.0).to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp3, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2p/c2p5vpptn5w5z2axumtfjd5ttqdsytfxqkdqzcokzx3ggp5kfn57.py
# Topologically Sorted Source Nodes: [linear_default_2], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_default_2 => addmm_default_8, constant_pad_nd_default_20
# Graph fragment:
#   %constant_pad_nd_default_20 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%mul_809, [0, 6, 0, 0]), kwargs = {})
#   %addmm_default_8 : [num_users=7] = call_function[target=torch.ops.aten.addmm.default](args = (%cat_9, %constant_pad_nd_default_20, %constant_pad_nd_default_21), kwargs = {})
triton_tem_fused_addmm_85 = async_compile.triton('triton_tem_fused_addmm_85', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_85', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_85(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 1152
    K = 3288
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3288
    stride_ak = 1
    stride_bk = 1152
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1152*idx_m
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')
meta13 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/pu/cpuk6eztmnvcsm4lkcvi67yanmlsklehbqdbm4oclryh3odf3tkx.py
# Topologically Sorted Source Nodes: [contiguous_3, relu_default, nan_to_num_default], Original ATen: [aten.clone, aten.relu, aten.nan_to_num]
# Source node to ATen node mapping:
#   contiguous_3 => clone_1
#   nan_to_num_default => convert_element_type_197, convert_element_type_198, eq_326, eq_327, isnan, scalar_tensor, scalar_tensor_1, scalar_tensor_2, where, where_1, where_2
#   relu_default => relu
# Graph fragment:
#   %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%slice_4,), kwargs = {memory_format: torch.contiguous_format})
#   %relu : [num_users=4] = call_function[target=torch.ops.aten.relu.default](args = (%clone_1,), kwargs = {})
#   %convert_element_type_198 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu, torch.float32), kwargs = {})
#   %eq_327 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_198, inf), kwargs = {})
#   %scalar_tensor_2 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_197 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu, torch.float32), kwargs = {})
#   %eq_326 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_197, -inf), kwargs = {})
#   %scalar_tensor_1 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%relu,), kwargs = {})
#   %scalar_tensor : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan, %scalar_tensor, %relu), kwargs = {})
#   %where_1 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_326, %scalar_tensor_1, %where), kwargs = {})
#   %where_2 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_327, %scalar_tensor_2, %where_1), kwargs = {})
triton_poi_fused_clone_nan_to_num_relu_86 = async_compile.triton('triton_poi_fused_clone_nan_to_num_relu_86', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clone_nan_to_num_relu_86', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clone_nan_to_num_relu_86(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 256)
    x1 = xindex // 256
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (256 + x0 + 512*x1), xmask).to(tl.float32)
    tmp1 = tl.full([1], 0, tl.int32)
    tmp2 = triton_helpers.maximum(tmp1, tmp0)
    tmp3 = tmp2.to(tl.float32)
    tmp4 = float("inf")
    tmp5 = tmp3 == tmp4
    tmp6 = float("-inf")
    tmp7 = tmp3 == tmp6
    tmp8 = libdevice.isnan(tmp2).to(tl.int1)
    tmp9 = 0.0
    tmp10 = tl.where(tmp8, tmp9, tmp2)
    tmp11 = -65504.0
    tmp12 = tl.where(tmp7, tmp11, tmp10)
    tmp13 = 65504.0
    tmp14 = tl.where(tmp5, tmp13, tmp12)
    tl.store(out_ptr0 + (x2), tmp14, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ra/crasx3uknjf73ghhudy5rcfersno6q2yrb2vbwj6kbd2rp2gszqc.py
# Topologically Sorted Source Nodes: [contiguous_3, relu_default, nan_to_num_default, linear_40], Original ATen: [aten.clone, aten.relu, aten.nan_to_num, aten.addmm]
# Source node to ATen node mapping:
#   contiguous_3 => clone_1
#   linear_40 => addmm_38
#   nan_to_num_default => convert_element_type_197, convert_element_type_198, eq_326, eq_327, isnan, scalar_tensor, scalar_tensor_1, scalar_tensor_2, where, where_1, where_2
#   relu_default => relu
# Graph fragment:
#   %clone_1 : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%slice_4,), kwargs = {memory_format: torch.contiguous_format})
#   %relu : [num_users=4] = call_function[target=torch.ops.aten.relu.default](args = (%clone_1,), kwargs = {})
#   %convert_element_type_198 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu, torch.float32), kwargs = {})
#   %eq_327 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_198, inf), kwargs = {})
#   %scalar_tensor_2 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_197 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu, torch.float32), kwargs = {})
#   %eq_326 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_197, -inf), kwargs = {})
#   %scalar_tensor_1 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%relu,), kwargs = {})
#   %scalar_tensor : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan, %scalar_tensor, %relu), kwargs = {})
#   %where_1 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_326, %scalar_tensor_1, %where), kwargs = {})
#   %where_2 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_327, %scalar_tensor_2, %where_1), kwargs = {})
#   %addmm_38 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b, %where_2, %permute_38), kwargs = {})
triton_tem_fused_addmm_clone_nan_to_num_relu_87 = async_compile.triton('triton_tem_fused_addmm_clone_nan_to_num_relu_87', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_clone_nan_to_num_relu_87', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_clone_nan_to_num_relu_87(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 960
    K = 256
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 256
    stride_ak = 1
    stride_bk = 1
    stride_bn = 256

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 960*idx_m
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')
meta14 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/pj/cpjjpisnrhisgqsnuxuq5ndgn5m7xuzq7ary4gwrr2ql6jjbn62c.py
# Topologically Sorted Source Nodes: [linear_41], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_41 => addmm_39
# Graph fragment:
#   %addmm_39 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b, %addmm_38, %permute_39), kwargs = {})
triton_tem_fused_addmm_88 = async_compile.triton('triton_tem_fused_addmm_88', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_88', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_88(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 1536
    K = 960
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 960
    stride_ak = 1
    stride_bk = 1
    stride_bn = 960

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1536*idx_m
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/4n/c4nlzsr2ggdkplww2qfxw3pmtqxxh44fj6v42fr7zzvfzwk3o237.py
# Topologically Sorted Source Nodes: [stack_default], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   stack_default => cat_10
# Graph fragment:
#   %cat_10 : [num_users=4] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3, %unsqueeze_4, %unsqueeze_5, %unsqueeze_6, %unsqueeze_7, %unsqueeze_8, %unsqueeze_9, %unsqueeze_10, %unsqueeze_11, %unsqueeze_12, %unsqueeze_13, %unsqueeze_14, %unsqueeze_15, %unsqueeze_16, %unsqueeze_17, %unsqueeze_18, %unsqueeze_19, %unsqueeze_20, %unsqueeze_21, %unsqueeze_22, %unsqueeze_23, %unsqueeze_24, %unsqueeze_25, %unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29, %unsqueeze_30, %unsqueeze_31, %permute_60],), kwargs = {})
triton_poi_fused_cat_89 = async_compile.triton('triton_poi_fused_cat_89', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[131072],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'out_ptr1': '*fp16', 'out_ptr2': '*fp16', 'out_ptr3': '*fp16', 'out_ptr4': '*fp16', 'out_ptr5': '*fp16', 'out_ptr6': '*fp16', 'out_ptr7': '*fp16', 'out_ptr8': '*fp16', 'out_ptr9': '*fp16', 'out_ptr10': '*fp16', 'out_ptr11': '*fp16', 'out_ptr12': '*fp16', 'out_ptr13': '*fp16', 'out_ptr14': '*fp16', 'out_ptr15': '*fp16', 'out_ptr16': '*fp16', 'out_ptr17': '*fp16', 'out_ptr18': '*fp16', 'out_ptr19': '*fp16', 'out_ptr20': '*fp16', 'out_ptr21': '*fp16', 'out_ptr22': '*fp16', 'out_ptr23': '*fp16', 'out_ptr24': '*fp16', 'out_ptr25': '*fp16', 'out_ptr26': '*fp16', 'out_ptr27': '*fp16', 'out_ptr28': '*fp16', 'out_ptr29': '*fp16', 'out_ptr30': '*fp16', 'out_ptr31': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_89', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 97, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_89(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, out_ptr2, out_ptr3, out_ptr4, out_ptr5, out_ptr6, out_ptr7, out_ptr8, out_ptr9, out_ptr10, out_ptr11, out_ptr12, out_ptr13, out_ptr14, out_ptr15, out_ptr16, out_ptr17, out_ptr18, out_ptr19, out_ptr20, out_ptr21, out_ptr22, out_ptr23, out_ptr24, out_ptr25, out_ptr26, out_ptr27, out_ptr28, out_ptr29, out_ptr30, out_ptr31, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 64)
    x1 = xindex // 64
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + 2048*x1), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (x0 + 2048*x1), xmask).to(tl.float32)
    tmp4 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp7 = tl.load(in_ptr0 + (64 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp8 = tl.load(in_ptr1 + (64 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp10 = tl.load(in_ptr2 + (64 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp13 = tl.load(in_ptr0 + (128 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp14 = tl.load(in_ptr1 + (128 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp16 = tl.load(in_ptr2 + (128 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp19 = tl.load(in_ptr0 + (192 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp20 = tl.load(in_ptr1 + (192 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp22 = tl.load(in_ptr2 + (192 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp25 = tl.load(in_ptr0 + (256 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp26 = tl.load(in_ptr1 + (256 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp28 = tl.load(in_ptr2 + (256 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp31 = tl.load(in_ptr0 + (320 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp32 = tl.load(in_ptr1 + (320 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp34 = tl.load(in_ptr2 + (320 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp37 = tl.load(in_ptr0 + (384 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp38 = tl.load(in_ptr1 + (384 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp40 = tl.load(in_ptr2 + (384 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp43 = tl.load(in_ptr0 + (448 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp44 = tl.load(in_ptr1 + (448 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp46 = tl.load(in_ptr2 + (448 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp49 = tl.load(in_ptr0 + (512 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp50 = tl.load(in_ptr1 + (512 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp52 = tl.load(in_ptr2 + (512 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp55 = tl.load(in_ptr0 + (576 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp56 = tl.load(in_ptr1 + (576 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp58 = tl.load(in_ptr2 + (576 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp61 = tl.load(in_ptr0 + (640 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp62 = tl.load(in_ptr1 + (640 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp64 = tl.load(in_ptr2 + (640 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp67 = tl.load(in_ptr0 + (704 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp68 = tl.load(in_ptr1 + (704 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp70 = tl.load(in_ptr2 + (704 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp73 = tl.load(in_ptr0 + (768 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp74 = tl.load(in_ptr1 + (768 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp76 = tl.load(in_ptr2 + (768 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp79 = tl.load(in_ptr0 + (832 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp80 = tl.load(in_ptr1 + (832 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp82 = tl.load(in_ptr2 + (832 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp85 = tl.load(in_ptr0 + (896 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp86 = tl.load(in_ptr1 + (896 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp88 = tl.load(in_ptr2 + (896 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp91 = tl.load(in_ptr0 + (960 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp92 = tl.load(in_ptr1 + (960 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp94 = tl.load(in_ptr2 + (960 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp97 = tl.load(in_ptr0 + (1024 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp98 = tl.load(in_ptr1 + (1024 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp100 = tl.load(in_ptr2 + (1024 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp103 = tl.load(in_ptr0 + (1088 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp104 = tl.load(in_ptr1 + (1088 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp106 = tl.load(in_ptr2 + (1088 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp109 = tl.load(in_ptr0 + (1152 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp110 = tl.load(in_ptr1 + (1152 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp112 = tl.load(in_ptr2 + (1152 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp115 = tl.load(in_ptr0 + (1216 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp116 = tl.load(in_ptr1 + (1216 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp118 = tl.load(in_ptr2 + (1216 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp121 = tl.load(in_ptr0 + (1280 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp122 = tl.load(in_ptr1 + (1280 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp124 = tl.load(in_ptr2 + (1280 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp127 = tl.load(in_ptr0 + (1344 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp128 = tl.load(in_ptr1 + (1344 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp130 = tl.load(in_ptr2 + (1344 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp133 = tl.load(in_ptr0 + (1408 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp134 = tl.load(in_ptr1 + (1408 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp136 = tl.load(in_ptr2 + (1408 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp139 = tl.load(in_ptr0 + (1472 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp140 = tl.load(in_ptr1 + (1472 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp142 = tl.load(in_ptr2 + (1472 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp145 = tl.load(in_ptr0 + (1536 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp146 = tl.load(in_ptr1 + (1536 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp148 = tl.load(in_ptr2 + (1536 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp151 = tl.load(in_ptr0 + (1600 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp152 = tl.load(in_ptr1 + (1600 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp154 = tl.load(in_ptr2 + (1600 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp157 = tl.load(in_ptr0 + (1664 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp158 = tl.load(in_ptr1 + (1664 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp160 = tl.load(in_ptr2 + (1664 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp163 = tl.load(in_ptr0 + (1728 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp164 = tl.load(in_ptr1 + (1728 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp166 = tl.load(in_ptr2 + (1728 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp169 = tl.load(in_ptr0 + (1792 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp170 = tl.load(in_ptr1 + (1792 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp172 = tl.load(in_ptr2 + (1792 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp175 = tl.load(in_ptr0 + (1856 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp176 = tl.load(in_ptr1 + (1856 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp178 = tl.load(in_ptr2 + (1856 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp181 = tl.load(in_ptr0 + (1920 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp182 = tl.load(in_ptr1 + (1920 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp184 = tl.load(in_ptr2 + (1920 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp187 = tl.load(in_ptr0 + (1984 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp188 = tl.load(in_ptr1 + (1984 + x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp190 = tl.load(in_ptr2 + (1984 + x0 + 2048*x1), xmask).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp5 = tmp3 + tmp4
    tmp6 = tmp2 + tmp5
    tmp9 = tmp7 + tmp8
    tmp11 = tmp10 + tmp4
    tmp12 = tmp9 + tmp11
    tmp15 = tmp13 + tmp14
    tmp17 = tmp16 + tmp4
    tmp18 = tmp15 + tmp17
    tmp21 = tmp19 + tmp20
    tmp23 = tmp22 + tmp4
    tmp24 = tmp21 + tmp23
    tmp27 = tmp25 + tmp26
    tmp29 = tmp28 + tmp4
    tmp30 = tmp27 + tmp29
    tmp33 = tmp31 + tmp32
    tmp35 = tmp34 + tmp4
    tmp36 = tmp33 + tmp35
    tmp39 = tmp37 + tmp38
    tmp41 = tmp40 + tmp4
    tmp42 = tmp39 + tmp41
    tmp45 = tmp43 + tmp44
    tmp47 = tmp46 + tmp4
    tmp48 = tmp45 + tmp47
    tmp51 = tmp49 + tmp50
    tmp53 = tmp52 + tmp4
    tmp54 = tmp51 + tmp53
    tmp57 = tmp55 + tmp56
    tmp59 = tmp58 + tmp4
    tmp60 = tmp57 + tmp59
    tmp63 = tmp61 + tmp62
    tmp65 = tmp64 + tmp4
    tmp66 = tmp63 + tmp65
    tmp69 = tmp67 + tmp68
    tmp71 = tmp70 + tmp4
    tmp72 = tmp69 + tmp71
    tmp75 = tmp73 + tmp74
    tmp77 = tmp76 + tmp4
    tmp78 = tmp75 + tmp77
    tmp81 = tmp79 + tmp80
    tmp83 = tmp82 + tmp4
    tmp84 = tmp81 + tmp83
    tmp87 = tmp85 + tmp86
    tmp89 = tmp88 + tmp4
    tmp90 = tmp87 + tmp89
    tmp93 = tmp91 + tmp92
    tmp95 = tmp94 + tmp4
    tmp96 = tmp93 + tmp95
    tmp99 = tmp97 + tmp98
    tmp101 = tmp100 + tmp4
    tmp102 = tmp99 + tmp101
    tmp105 = tmp103 + tmp104
    tmp107 = tmp106 + tmp4
    tmp108 = tmp105 + tmp107
    tmp111 = tmp109 + tmp110
    tmp113 = tmp112 + tmp4
    tmp114 = tmp111 + tmp113
    tmp117 = tmp115 + tmp116
    tmp119 = tmp118 + tmp4
    tmp120 = tmp117 + tmp119
    tmp123 = tmp121 + tmp122
    tmp125 = tmp124 + tmp4
    tmp126 = tmp123 + tmp125
    tmp129 = tmp127 + tmp128
    tmp131 = tmp130 + tmp4
    tmp132 = tmp129 + tmp131
    tmp135 = tmp133 + tmp134
    tmp137 = tmp136 + tmp4
    tmp138 = tmp135 + tmp137
    tmp141 = tmp139 + tmp140
    tmp143 = tmp142 + tmp4
    tmp144 = tmp141 + tmp143
    tmp147 = tmp145 + tmp146
    tmp149 = tmp148 + tmp4
    tmp150 = tmp147 + tmp149
    tmp153 = tmp151 + tmp152
    tmp155 = tmp154 + tmp4
    tmp156 = tmp153 + tmp155
    tmp159 = tmp157 + tmp158
    tmp161 = tmp160 + tmp4
    tmp162 = tmp159 + tmp161
    tmp165 = tmp163 + tmp164
    tmp167 = tmp166 + tmp4
    tmp168 = tmp165 + tmp167
    tmp171 = tmp169 + tmp170
    tmp173 = tmp172 + tmp4
    tmp174 = tmp171 + tmp173
    tmp177 = tmp175 + tmp176
    tmp179 = tmp178 + tmp4
    tmp180 = tmp177 + tmp179
    tmp183 = tmp181 + tmp182
    tmp185 = tmp184 + tmp4
    tmp186 = tmp183 + tmp185
    tmp189 = tmp187 + tmp188
    tmp191 = tmp190 + tmp4
    tmp192 = tmp189 + tmp191
    tl.store(out_ptr0 + (x2), tmp6, xmask)
    tl.store(out_ptr1 + (x2), tmp12, xmask)
    tl.store(out_ptr2 + (x2), tmp18, xmask)
    tl.store(out_ptr3 + (x2), tmp24, xmask)
    tl.store(out_ptr4 + (x2), tmp30, xmask)
    tl.store(out_ptr5 + (x2), tmp36, xmask)
    tl.store(out_ptr6 + (x2), tmp42, xmask)
    tl.store(out_ptr7 + (x2), tmp48, xmask)
    tl.store(out_ptr8 + (x2), tmp54, xmask)
    tl.store(out_ptr9 + (x2), tmp60, xmask)
    tl.store(out_ptr10 + (x2), tmp66, xmask)
    tl.store(out_ptr11 + (x2), tmp72, xmask)
    tl.store(out_ptr12 + (x2), tmp78, xmask)
    tl.store(out_ptr13 + (x2), tmp84, xmask)
    tl.store(out_ptr14 + (x2), tmp90, xmask)
    tl.store(out_ptr15 + (x2), tmp96, xmask)
    tl.store(out_ptr16 + (x2), tmp102, xmask)
    tl.store(out_ptr17 + (x2), tmp108, xmask)
    tl.store(out_ptr18 + (x2), tmp114, xmask)
    tl.store(out_ptr19 + (x2), tmp120, xmask)
    tl.store(out_ptr20 + (x2), tmp126, xmask)
    tl.store(out_ptr21 + (x2), tmp132, xmask)
    tl.store(out_ptr22 + (x2), tmp138, xmask)
    tl.store(out_ptr23 + (x2), tmp144, xmask)
    tl.store(out_ptr24 + (x2), tmp150, xmask)
    tl.store(out_ptr25 + (x2), tmp156, xmask)
    tl.store(out_ptr26 + (x2), tmp162, xmask)
    tl.store(out_ptr27 + (x2), tmp168, xmask)
    tl.store(out_ptr28 + (x2), tmp174, xmask)
    tl.store(out_ptr29 + (x2), tmp180, xmask)
    tl.store(out_ptr30 + (x2), tmp186, xmask)
    tl.store(out_ptr31 + (x2), tmp192, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/6u/c6uezhjp46na2banmgkst5524pnvwu5wp2vpe5s4z3mrz3vhj3xs.py
# Topologically Sorted Source Nodes: [stack_default], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   stack_default => cat_10
# Graph fragment:
#   %cat_10 : [num_users=4] = call_function[target=torch.ops.aten.cat.default](args = ([%unsqueeze, %unsqueeze_1, %unsqueeze_2, %unsqueeze_3, %unsqueeze_4, %unsqueeze_5, %unsqueeze_6, %unsqueeze_7, %unsqueeze_8, %unsqueeze_9, %unsqueeze_10, %unsqueeze_11, %unsqueeze_12, %unsqueeze_13, %unsqueeze_14, %unsqueeze_15, %unsqueeze_16, %unsqueeze_17, %unsqueeze_18, %unsqueeze_19, %unsqueeze_20, %unsqueeze_21, %unsqueeze_22, %unsqueeze_23, %unsqueeze_24, %unsqueeze_25, %unsqueeze_26, %unsqueeze_27, %unsqueeze_28, %unsqueeze_29, %unsqueeze_30, %unsqueeze_31, %permute_60],), kwargs = {})
triton_poi_fused_cat_90 = async_compile.triton('triton_poi_fused_cat_90', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[4194304],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_90', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_90(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x3 = xindex
    x4 = (xindex % 2048)
    x0 = (xindex % 64)
    x1 = ((xindex // 64) % 32)
    x2 = xindex // 2048
    tmp0 = tl.load(in_ptr0 + (x3), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (x4), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr2 + (x3), xmask).to(tl.float32)
    tmp4 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp5 = tmp3 + tmp4
    tmp6 = tmp2 + tmp5
    tl.store(out_ptr0 + (x0 + 64*x2 + 64*ks0*x1), tmp6, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/32/c32gupedlvqawac4chdkmdg7iuxdqdbsox62suw3cbzonwk5h7by.py
# Topologically Sorted Source Nodes: [nan_to_num, clamp], Original ATen: [aten.nan_to_num, aten.clamp]
# Source node to ATen node mapping:
#   clamp => clamp_max, clamp_min, convert_element_type_263, convert_element_type_264
#   nan_to_num => convert_element_type_261, convert_element_type_262, eq_583, eq_584, isnan_1, scalar_tensor_3, scalar_tensor_4, scalar_tensor_5, where_3, where_4, where_5
# Graph fragment:
#   %convert_element_type_262 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_10, torch.float32), kwargs = {})
#   %eq_584 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_262, inf), kwargs = {})
#   %scalar_tensor_5 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_261 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_10, torch.float32), kwargs = {})
#   %eq_583 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_261, -inf), kwargs = {})
#   %scalar_tensor_4 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan_1 : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%cat_10,), kwargs = {})
#   %scalar_tensor_3 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where_3 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan_1, %scalar_tensor_3, %cat_10), kwargs = {})
#   %where_4 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_583, %scalar_tensor_4, %where_3), kwargs = {})
#   %where_5 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_584, %scalar_tensor_5, %where_4), kwargs = {})
#   %convert_element_type_263 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_5, torch.float32), kwargs = {})
#   %clamp_min : [num_users=1] = call_function[target=torch.ops.aten.clamp_min.default](args = (%convert_element_type_263, -1000.1), kwargs = {})
#   %clamp_max : [num_users=1] = call_function[target=torch.ops.aten.clamp_max.default](args = (%clamp_min, 1000.1), kwargs = {})
#   %convert_element_type_264 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%clamp_max, torch.float16), kwargs = {})
triton_poi_fused_clamp_nan_to_num_91 = async_compile.triton('triton_poi_fused_clamp_nan_to_num_91', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clamp_nan_to_num_91', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clamp_nan_to_num_91(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
    tmp1 = tmp0.to(tl.float32)
    tmp2 = float("inf")
    tmp3 = tmp1 == tmp2
    tmp4 = float("-inf")
    tmp5 = tmp1 == tmp4
    tmp6 = libdevice.isnan(tmp0).to(tl.int1)
    tmp7 = 0.0
    tmp8 = tl.where(tmp6, tmp7, tmp0)
    tmp9 = -65504.0
    tmp10 = tl.where(tmp5, tmp9, tmp8)
    tmp11 = 65504.0
    tmp12 = tl.where(tmp3, tmp11, tmp10)
    tmp13 = tmp12.to(tl.float32)
    tmp14 = -1000.1
    tmp15 = triton_helpers.maximum(tmp13, tmp14)
    tmp16 = 1000.1
    tmp17 = triton_helpers.minimum(tmp15, tmp16)
    tmp18 = tmp17.to(tl.float32)
    tl.store(out_ptr0 + (x0), tmp18, None)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ub/cubgo7bstnsjz5zutslw3523k4vigd23gbucikw3uu45t75alxud.py
# Topologically Sorted Source Nodes: [nan_to_num, clamp, baddbmm], Original ATen: [aten.nan_to_num, aten.clamp, aten.baddbmm]
# Source node to ATen node mapping:
#   baddbmm => baddbmm
#   clamp => clamp_max, clamp_min, convert_element_type_263, convert_element_type_264
#   nan_to_num => convert_element_type_261, convert_element_type_262, eq_583, eq_584, isnan_1, scalar_tensor_3, scalar_tensor_4, scalar_tensor_5, where_3, where_4, where_5
# Graph fragment:
#   %convert_element_type_262 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_10, torch.float32), kwargs = {})
#   %eq_584 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_262, inf), kwargs = {})
#   %scalar_tensor_5 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_261 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_10, torch.float32), kwargs = {})
#   %eq_583 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_261, -inf), kwargs = {})
#   %scalar_tensor_4 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan_1 : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%cat_10,), kwargs = {})
#   %scalar_tensor_3 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where_3 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan_1, %scalar_tensor_3, %cat_10), kwargs = {})
#   %where_4 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_583, %scalar_tensor_4, %where_3), kwargs = {})
#   %where_5 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_584, %scalar_tensor_5, %where_4), kwargs = {})
#   %convert_element_type_263 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%where_5, torch.float32), kwargs = {})
#   %clamp_min : [num_users=1] = call_function[target=torch.ops.aten.clamp_min.default](args = (%convert_element_type_263, -1000.1), kwargs = {})
#   %clamp_max : [num_users=1] = call_function[target=torch.ops.aten.clamp_max.default](args = (%clamp_min, 1000.1), kwargs = {})
#   %convert_element_type_264 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%clamp_max, torch.float16), kwargs = {})
#   %baddbmm : [num_users=1] = call_function[target=torch.ops.aten.baddbmm.default](args = (%submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias, %convert_element_type_264, %submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight), kwargs = {})
triton_tem_fused_baddbmm_clamp_nan_to_num_92 = async_compile.triton('triton_tem_fused_baddbmm_clamp_nan_to_num_92', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=8,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_baddbmm_clamp_nan_to_num_92', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_baddbmm_clamp_nan_to_num_92(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = ks0
    N = 192
    K = 64

    stride_aq = 64*ks0
    stride_am = 64
    stride_ak = 1

    stride_bq = 12288
    stride_bk = 192
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m + 192*idx_q*ks0
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n + 192*idx_q, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/hz/chzpxeorxbhpr6xeutx6cb6wtskzvgw57ovcvq3aj2uqtr6v7dpp.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_93 = async_compile.triton('triton_poi_fused_cat_93', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_93', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_93(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 2880)
    x1 = xindex // 2880
    tmp0 = tl.load(in_ptr0 + (572 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/eb/ceblsxggn7tyw4hdd7nzvg27gkno5tct4xnlsa3lvg2cnv2oucor.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_94 = async_compile.triton('triton_poi_fused_cat_94', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_94', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_94(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (176 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/on/con5mggvyxnp4it5nj2rdesaoueqy43zett4rwgeskaoiul5d5mj.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_95 = async_compile.triton('triton_poi_fused_cat_95', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_95', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_95(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 3648)
    x1 = xindex // 3648
    tmp0 = tl.load(in_ptr0 + (3452 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kb/ckbqd2jnfj6jsianmkynvj4rlsjvhnj2jrxmnspfoc2h7uike5g3.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_96 = async_compile.triton('triton_poi_fused_cat_96', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_96', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_96(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (368 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/7i/c7ii7bx6kmsymbkzsohd5z3jomd7cg6yccjz4ppb3hkr2yagpdic.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_97 = async_compile.triton('triton_poi_fused_cat_97', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_97', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_97(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 768)
    x1 = xindex // 768
    tmp0 = tl.load(in_ptr0 + (7100 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/uk/cukhsvzskafchozwrlxd4relmurmdm6gfkpwsb7tfqrihiqzoix4.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_98 = async_compile.triton('triton_poi_fused_cat_98', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_98', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_98(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (560 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/37/c37muxpu3xoctvmci52zix7l3l2ph4azs2vlf4rmbh7qtvifbtdr.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_99 = async_compile.triton('triton_poi_fused_cat_99', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_99', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_99(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (7868 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/hu/chu64qaneeh6xyrc7u4ewvn2kcdf57vvmzu6wpgxyusvnuntmybr.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_100 = async_compile.triton('triton_poi_fused_cat_100', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_100', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_100(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (752 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/7m/c7mabjtjftq3pse6ovvr5ioy75xilcuxtqiehttamsanwnegmvhc.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_101 = async_compile.triton('triton_poi_fused_cat_101', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[4194304],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_101', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_101(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 1152)
    x1 = xindex // 1152
    tmp0 = tl.load(in_ptr0 + (8060 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/t2/ct27j4cwprkxh7qjat774lfllikpsggwf6qtuf2nhsxygb5aexwd.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_102 = async_compile.triton('triton_poi_fused_cat_102', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_102', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_102(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (1136 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/72/c72fytt6ajuwr4f4uvfifxjbomghsun55cvwmtutbs7qpbaeem4e.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_103 = async_compile.triton('triton_poi_fused_cat_103', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_103', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_103(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 960)
    x1 = xindex // 960
    tmp0 = tl.load(in_ptr0 + (9212 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/23/c23ezbgfvwknxkldelkmacpdphcw3b5wzcyv6fatjfah4letfy3w.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_104 = async_compile.triton('triton_poi_fused_cat_104', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_104', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_104(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (1328 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ua/cua4atb4nqp52iwn6jlhvcqg7rhph77mpe6mi6zkvvwc6gmnji7m.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_105 = async_compile.triton('triton_poi_fused_cat_105', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_105', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_105(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 576)
    x1 = xindex // 576
    tmp0 = tl.load(in_ptr0 + (10172 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ft/cftpvjhtrdnlyubbdfqa5zplpgao7h4cx2tbkjg442hunagjuibg.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_106 = async_compile.triton('triton_poi_fused_cat_106', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_106', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_106(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 768)
    x1 = xindex // 768
    tmp0 = tl.load(in_ptr0 + (1520 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2a/c2a5v3sux2u45zg56gs4ej2z2rhdjesxznmbrd2xdaolehs7i4eh.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_107 = async_compile.triton('triton_poi_fused_cat_107', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_107', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_107(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (10748 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/hd/chd233lzoh5bimevl54tw3hunbwkwbkyj6l4ni25xcmol7uii4ln.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_108 = async_compile.triton('triton_poi_fused_cat_108', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_108', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_108(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (2288 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/sy/csyom4bfz3kp3grf3fpuuwlp3dmm3xrquyw5z26negzfxzdmp6ho.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_109 = async_compile.triton('triton_poi_fused_cat_109', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_109', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_109(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 768)
    x1 = xindex // 768
    tmp0 = tl.load(in_ptr0 + (10940 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/gu/cgu3nqcmsq63kysji4tyizzbhsl2tofo3itdyin2r4px6qu663lx.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_110 = async_compile.triton('triton_poi_fused_cat_110', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_110', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_110(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (2480 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/eg/cegshyi5mvft7v574ziedjz6uc6aub5ktvgcypcz3odzgjq7jv2s.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_111 = async_compile.triton('triton_poi_fused_cat_111', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_111', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_111(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (11708 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mh/cmhkgqgciuepcvklleexuliprhqpqwtuedqna5draq2jvipohbui.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_112 = async_compile.triton('triton_poi_fused_cat_112', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_112', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_112(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (2864 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/s3/cs3bh3a63bc4cju4asujblmu4lcb3z7sy63tl5eellh5yf3p335e.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_113 = async_compile.triton('triton_poi_fused_cat_113', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_113', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_113(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (12092 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/av/cavbmbs3jlrt5cy5ovu6rrkhptrufyykfcylutrtlub7vcshxjs3.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_114 = async_compile.triton('triton_poi_fused_cat_114', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_114', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_114(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (3056 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/3a/c3abl7vcla6bgmrjy5ryf7j37aumihrlizwbwk6msf72vzsdqrb3.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_115 = async_compile.triton('triton_poi_fused_cat_115', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[4194304],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_115', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_115(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 1152)
    x1 = xindex // 1152
    tmp0 = tl.load(in_ptr0 + (12476 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ke/cke7jozmlfpvauznyrwsljt3uy6oxc46c7j7knkuwvhjtovryfcg.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_116 = async_compile.triton('triton_poi_fused_cat_116', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_116', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_116(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (3248 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/rp/crp32l5egqp6nnfukl66xa47fsmw6esrcswjy6vjdmxjnlof7yvf.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_117 = async_compile.triton('triton_poi_fused_cat_117', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_117', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_117(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 768)
    x1 = xindex // 768
    tmp0 = tl.load(in_ptr0 + (13628 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/6v/c6v5u4unganuxjmo2msv3rjb3ksnbj2usk3ijj7xylj2hnpnudxj.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_118 = async_compile.triton('triton_poi_fused_cat_118', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_118', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_118(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (3440 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/o4/co43jj6u42oj67aab5l3ygfmlmivjjrutqb2oo3lbz5s465btxwo.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_119 = async_compile.triton('triton_poi_fused_cat_119', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_119', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_119(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (14396 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/to/ctotn5bcy6mchsk4hrrjecvxftb6q7f5qt5mqf5reuhvy3uhkvwu.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_120 = async_compile.triton('triton_poi_fused_cat_120', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_120', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_120(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (3632 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/h4/ch4atdtiem43xnzp2cfqqxsgkvvuipwrswyf6ulnxstcfqhj45nd.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_121 = async_compile.triton('triton_poi_fused_cat_121', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_121', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_121(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (14780 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mm/cmmtgozofs3btkb2m2tf3ca6b45zhixhai7fpt7q7joftimvj5oy.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_122 = async_compile.triton('triton_poi_fused_cat_122', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_122', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_122(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (3824 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/bc/cbcndvnkxxl5kccjkeyoyf5gy5olabdwucir3k7p6d2vxz6ynmth.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_123 = async_compile.triton('triton_poi_fused_cat_123', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_123', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_123(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (14972 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/n4/cn4x4olh3rc5tybj2qxc4qziiik73orpf57mnct6l4gesr6cythw.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_124 = async_compile.triton('triton_poi_fused_cat_124', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_124', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_124(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (4208 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/46/c4625bvyn53lypjpb4sbvmvr6v2yjaxjryozrehxhz2hh77ofihz.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_125 = async_compile.triton('triton_poi_fused_cat_125', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_125', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_125(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (15356 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/my/cmytx4jjdqbrh4amnxzqp6wytvnbgxpevryg6sx7ckvj3ekqw4dj.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_126 = async_compile.triton('triton_poi_fused_cat_126', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_126', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_126(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (4400 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/jl/cjlam3fvn7w2zetfcchyi5lbu27ktnxvlrm3xvobpaoie44xtbmq.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_127 = async_compile.triton('triton_poi_fused_cat_127', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_127', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_127(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (15548 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/c7/cc7unwtkiy4o4ykh24pv6qvkwzdqjfsbzavlib76l5fznr6wwouu.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_128 = async_compile.triton('triton_poi_fused_cat_128', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_128', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_128(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (4592 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mf/cmfm4p35kgaw53x4lgnby6pmboazxxn3vcyhuiloeq4qjwp3kd5c.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_129 = async_compile.triton('triton_poi_fused_cat_129', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_129', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_129(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (15932 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/if/cif45sc4v7k7tb36jjjyp2x6puit2ec6pghwijzlr3ny42ar4lhb.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_130 = async_compile.triton('triton_poi_fused_cat_130', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[33554432],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_130', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_130(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 13056)
    x1 = xindex // 13056
    tmp0 = tl.load(in_ptr0 + (4976 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ws/cwsng6hdbhzdqvh6uh3oyw7cqiqm4qnfmahm5romuqnca2kd2ysx.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_131 = async_compile.triton('triton_poi_fused_cat_131', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_131', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_131(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 768)
    x1 = xindex // 768
    tmp0 = tl.load(in_ptr0 + (16316 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2a/c2agaqyvfj2t4oroh3yhunsgo5z4q2cx4m52yt5u2ezlhbxduuhc.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_132 = async_compile.triton('triton_poi_fused_cat_132', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_132', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_132(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (18032 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2y/c2yrrxitd4dfrxcjzwwkfw3d3w4zovkbz674j4hmaxkhegxjd5u5.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_133 = async_compile.triton('triton_poi_fused_cat_133', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[4194304],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_133', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_133(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 1152)
    x1 = xindex // 1152
    tmp0 = tl.load(in_ptr0 + (17084 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/vk/cvkzkisjcerxbxyibxo46mtbjfs2djit54xpo3ihpj3ckn2hzlvk.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_134 = async_compile.triton('triton_poi_fused_cat_134', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_134', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_134(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (18224 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ne/cnewut4nipghchazu7yelzyazilrhk4ww4tjwd4xp3ybbntp4p4m.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_135 = async_compile.triton('triton_poi_fused_cat_135', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_135', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_135(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 576)
    x1 = xindex // 576
    tmp0 = tl.load(in_ptr0 + (18236 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/wp/cwphfdph3rzfrhovt7kgh64tepwhcig3yolapuuevaiz7wptsrzx.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_136 = async_compile.triton('triton_poi_fused_cat_136', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_136', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_136(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (18416 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/7w/c7wxpl7xkodikylkbc4kpnuxq73zaennrmeshrvpwoydorkbndjh.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_137 = async_compile.triton('triton_poi_fused_cat_137', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_137', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_137(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 2880)
    x1 = xindex // 2880
    tmp0 = tl.load(in_ptr0 + (18812 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ef/cef47elo7pe72eku3q4cw2wcqvti7yegdvbm3ksqun45qml2qvaz.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_138 = async_compile.triton('triton_poi_fused_cat_138', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_138', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_138(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (18608 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/yy/cyydfhbmfosyawfeje2mpcycaqra73r5rtjitq6objjialznuq6a.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_139 = async_compile.triton('triton_poi_fused_cat_139', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_139', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_139(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (21692 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/jb/cjbaa7m72dxkjoosyl42pajapoypbaf57f4zp3qsxoc6h4cfsjf3.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_140 = async_compile.triton('triton_poi_fused_cat_140', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_140', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_140(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (18992 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/r5/cr524irstonogh6i67dxmnhohhq262e3qkumh64m23xnimctsp3g.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_141 = async_compile.triton('triton_poi_fused_cat_141', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_141', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_141(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (22076 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/w4/cw4e6rdgo3u7r6ij5qb5m6hwmv74p5uhnotpedsmchgr6rm7iyym.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_142 = async_compile.triton('triton_poi_fused_cat_142', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_142', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_142(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (19184 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/io/ciozhd3o7vf5owz7hmgii3ol4wsfjgkaqnkkv7btdrctgqyowaoz.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_143 = async_compile.triton('triton_poi_fused_cat_143', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[4194304],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_143', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_143(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 1920)
    x1 = xindex // 1920
    tmp0 = tl.load(in_ptr0 + (22268 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kq/ckqz6ljplrym3b2nprnijbr73pisw764lpiiasuqurrzjdeltl3x.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_144 = async_compile.triton('triton_poi_fused_cat_144', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_144', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_144(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (19376 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/vf/cvffm6623ramdjbokjfyidtcmkkccgb7t7mwevislptt7qfovu6p.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_145 = async_compile.triton('triton_poi_fused_cat_145', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[33554432],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_145', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_145(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 9792)
    x1 = xindex // 9792
    tmp0 = tl.load(in_ptr0 + (24188 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/oj/cojt7smho4satfuh3kjs7uswkn2phnddkni5hl4io74edfww5zum.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_146 = async_compile.triton('triton_poi_fused_cat_146', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_146', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_146(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (19760 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/z7/cz7mxu37e5nvpcucpvgmkbhhegrobnmgbz74uyfmvzpo7t7stfv6.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_147 = async_compile.triton('triton_poi_fused_cat_147', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_147', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_147(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (33980 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/y6/cy6b5dr6byehla6k63jvflnwa4fi4hvcltqdgt743vg2m4ge7rpw.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_148 = async_compile.triton('triton_poi_fused_cat_148', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_148', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_148(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 768)
    x1 = xindex // 768
    tmp0 = tl.load(in_ptr0 + (19952 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/bs/cbsbdsk5mod4xut7lruqke2xudbumstogkkw5qk2dbkne3r4sxas.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_149 = async_compile.triton('triton_poi_fused_cat_149', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_149', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_149(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 576)
    x1 = xindex // 576
    tmp0 = tl.load(in_ptr0 + (34172 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/sg/csggsekd563bgda5ma7isvbnjvlqvzkqhxyeejye23zp7lsxxn4i.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_150 = async_compile.triton('triton_poi_fused_cat_150', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_150', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_150(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (20720 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/e3/ce3p7qsjkozgtmiv3iwyixvtbgbrfhehnt36pvqu65hdwvkmuaef.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_151 = async_compile.triton('triton_poi_fused_cat_151', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_151', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_151(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (34748 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/nh/cnhlvunlxi6jyqg7xcynug2hsl3uxc35fpismxn2ppniyguuqt22.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_152 = async_compile.triton('triton_poi_fused_cat_152', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_152', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_152(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (20912 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/yj/cyjbixr27ru6zq7l2ehcwfx5hxd4z75kspggfcqrarcojxawidku.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_153 = async_compile.triton('triton_poi_fused_cat_153', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_153', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_153(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 384)
    x1 = xindex // 384
    tmp0 = tl.load(in_ptr0 + (34940 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/qz/cqzaadsophddsgyruacw4mnsyrn2ldgctdrinpltpr3loh3fosdk.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_154 = async_compile.triton('triton_poi_fused_cat_154', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_154', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_154(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 2496)
    x1 = xindex // 2496
    tmp0 = tl.load(in_ptr0 + (21296 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/pg/cpgxp2t5tn47qavrrt3ydgpsfgxn6nroepfqx4jkescstq6qfmw3.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_155 = async_compile.triton('triton_poi_fused_cat_155', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_155', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_155(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 576)
    x1 = xindex // 576
    tmp0 = tl.load(in_ptr0 + (35324 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/tb/ctbgx4f2tvx7o7jb63wah3wcgwwhrfjchcqesbvp72jw7ih2g2av.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_156 = async_compile.triton('triton_poi_fused_cat_156', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_156', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_156(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (23792 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/t7/ct7ko4636v3kcbhav6qcjg6q7f7tiqldqkxqbub6uztrnofrnbcu.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_157 = async_compile.triton('triton_poi_fused_cat_157', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_157', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_157(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (35900 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/52/c52fq47ruvecu4rcjqi74y4try25dlfmtbmjk6ffqikntjiyjwaz.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_158 = async_compile.triton('triton_poi_fused_cat_158', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2097152],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_158', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_158(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 768)
    x1 = xindex // 768
    tmp0 = tl.load(in_ptr0 + (23984 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kg/ckgclrgg3t5zg2y2b4627jo6u4ezwgfjpuerdfkvsslhafd5v2mx.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_159 = async_compile.triton('triton_poi_fused_cat_159', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_159', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_159(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (36092 + x0 + 36284*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/jz/cjzvmdtrsanqva6nrgeudedl4jv2oap4anavncvaoovy45svpyym.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_160 = async_compile.triton('triton_poi_fused_cat_160', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[16777216],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_160', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_160(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 5760)
    x1 = xindex // 5760
    tmp0 = tl.load(in_ptr0 + (24752 + x0 + 30516*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ss/css5wnv5m75b56xhm4ymiw7ai6ogb4cf6yntkjew44m6q54wqf56.py
# Topologically Sorted Source Nodes: [contiguous_4], Original ATen: [aten.clone]
# Source node to ATen node mapping:
#   contiguous_4 => clone_2
# Graph fragment:
#   %clone_2 : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%slice_6,), kwargs = {memory_format: torch.contiguous_format})
triton_poi_fused_clone_161 = async_compile.triton('triton_poi_fused_clone_161', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clone_161', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clone_161(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (x0 + 1152*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/fg/cfgkgigeb6msnsezk3adjubkcoav54zblqlwv5zxrmwisjy76ekn.py
# Topologically Sorted Source Nodes: [contiguous_5], Original ATen: [aten.clone]
# Source node to ATen node mapping:
#   contiguous_5 => clone_3
# Graph fragment:
#   %clone_3 : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%slice_8,), kwargs = {memory_format: torch.contiguous_format})
triton_poi_fused_clone_162 = async_compile.triton('triton_poi_fused_clone_162', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clone_162', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clone_162(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (192 + x0 + 1152*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/pk/cpkqvz2osb3yhjw6amlqcbygrz2x4kvwvflsig5oyefuz3z2k3ti.py
# Topologically Sorted Source Nodes: [contiguous_6], Original ATen: [aten.clone]
# Source node to ATen node mapping:
#   contiguous_6 => clone_4
# Graph fragment:
#   %clone_4 : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%slice_10,), kwargs = {memory_format: torch.contiguous_format})
triton_poi_fused_clone_163 = async_compile.triton('triton_poi_fused_clone_163', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clone_163', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clone_163(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (384 + x0 + 1152*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/up/cupqnjf2rtp7x75ffp7rdtwf236mo7j53ncqe6jhedtorjqcqgmp.py
# Topologically Sorted Source Nodes: [contiguous_7], Original ATen: [aten.clone]
# Source node to ATen node mapping:
#   contiguous_7 => clone_5
# Graph fragment:
#   %clone_5 : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%slice_12,), kwargs = {memory_format: torch.contiguous_format})
triton_poi_fused_clone_164 = async_compile.triton('triton_poi_fused_clone_164', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clone_164', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clone_164(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (576 + x0 + 1152*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/tm/ctmxdrv6s7snbv365tv2t3dn7d3ouxmjms6gmetbaurz552vqkmj.py
# Topologically Sorted Source Nodes: [contiguous_8], Original ATen: [aten.clone]
# Source node to ATen node mapping:
#   contiguous_8 => clone_6
# Graph fragment:
#   %clone_6 : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%slice_14,), kwargs = {memory_format: torch.contiguous_format})
triton_poi_fused_clone_165 = async_compile.triton('triton_poi_fused_clone_165', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clone_165', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clone_165(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (768 + x0 + 1152*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/sh/cshzrwiz45soao6bstr7r5zjcbsae42r5i5whp6erpxledoyadxb.py
# Topologically Sorted Source Nodes: [contiguous_9], Original ATen: [aten.clone]
# Source node to ATen node mapping:
#   contiguous_9 => clone_7
# Graph fragment:
#   %clone_7 : [num_users=4] = call_function[target=torch.ops.aten.clone.default](args = (%slice_16,), kwargs = {memory_format: torch.contiguous_format})
triton_poi_fused_clone_166 = async_compile.triton('triton_poi_fused_clone_166', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[524288],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_clone_166', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_clone_166(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (960 + x0 + 1152*x1), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ap/capgpatfqqfgk5wfqydyljaf7u4fhl6ub7dfreuedvrzfwh44322.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_167 = async_compile.triton('triton_poi_fused_cat_167', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[4194304],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_167', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_167(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 1536)
    x1 = xindex // 1536
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ha/chahaf2mdtynjglra6fukcvccyvmpyhjk7aqb4zwpquzd2y6k6qh.py
# Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
# Source node to ATen node mapping:
#   cat_default_7 => cat_11
# Graph fragment:
#   %cat_11 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%getitem_19, %getitem_61, %getitem_20, %getitem_62, %getitem_21, %getitem_63, %getitem_22, %getitem_64, %getitem_23, %getitem_65, %getitem_24, %getitem_66, %getitem_25, %getitem_67, %getitem_26, %getitem_68, %getitem_27, %getitem_69, %getitem_28, %getitem_70, %getitem_29, %getitem_71, %getitem_30, %getitem_72, %getitem_31, %getitem_73, %getitem_32, %getitem_74, %getitem_33, %getitem_75, %getitem_34, %getitem_76, %getitem_35, %getitem_77, %getitem_36, %getitem_78, %getitem_37, %getitem_79, %getitem_38, %getitem_80, %getitem_39, %getitem_81, %getitem_40, %getitem_82, %getitem_41, %getitem_83, %getitem_42, %getitem_84, %getitem_43, %getitem_85, %getitem_44, %getitem_86, %getitem_45, %getitem_87, %getitem_46, %getitem_88, %getitem_47, %getitem_89, %getitem_48, %getitem_90, %getitem_49, %getitem_91, %getitem_50, %getitem_92, %getitem_51, %getitem_93, %getitem_52, %getitem_94, %clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %addmm_39, %convert_element_type_129, %convert_element_type_131, %convert_element_type_121, %convert_element_type_133, %convert_element_type_135, %convert_element_type_137, %convert_element_type_123, %convert_element_type_139, %convert_element_type_141, %convert_element_type_143, %convert_element_type_125, %convert_element_type_127, %convert_element_type_145, %convert_element_type_147, %convert_element_type_149, %convert_element_type_151, %convert_element_type_153, %convert_element_type_155, %convert_element_type_157, %convert_element_type_159, %convert_element_type_161, %convert_element_type_163, %convert_element_type_165, %convert_element_type_167, %convert_element_type_169, %convert_element_type_171, %convert_element_type_173, %convert_element_type_175, %convert_element_type_177, %convert_element_type_179, %convert_element_type_181, %convert_element_type_183, %convert_element_type_185, %convert_element_type_187, %convert_element_type_189, %convert_element_type_191, %view_56], 1), kwargs = {})
triton_poi_fused_cat_168 = async_compile.triton('triton_poi_fused_cat_168', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[33554432],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_168', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_cat_168(in_ptr0, out_ptr0, ks0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = tl.full([XBLOCK], True, tl.int1)
    x0 = (xindex % 12288)
    x1 = xindex // 12288
    tmp0 = tl.load(in_ptr0 + (192*x1 + 192*ks0*(x0 // 192) + ((x0 % 192))), None).to(tl.float32)
    tl.store(out_ptr0 + (x0 + 87936*x1), tmp0, None)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mr/cmrmxfwjsmkiisn25qtx2vj7sssv3yurkbuxtkwpfrohqoz6fbsr.py
# Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.bmm]
# Source node to ATen node mapping:
#   matmul => bmm
# Graph fragment:
#   %bmm : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%view_58, %view_59), kwargs = {})
triton_tem_fused_bmm_169 = async_compile.triton('triton_tem_fused_bmm_169', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_bmm_169', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_bmm_169(arg_A, arg_B, out_ptr0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = 174
    N = 192
    K = 458

    stride_aq = 0
    stride_am = 458
    stride_ak = 1

    stride_bq = 87936
    stride_bk = 192
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m + 33408*idx_q
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/c2/cc2uhbo2bdascywlunllaqjxu655fecdfwhpbvnnoratwp6mzb4n.py
# Topologically Sorted Source Nodes: [add_1891, layer_norm_46], Original ATen: [aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_1891 => add_1894
#   layer_norm_46 => add_1903, add_1904, convert_element_type_267, convert_element_type_268, mul_1224, mul_1225, rsqrt_46, sub_605, var_mean_46
# Graph fragment:
#   %add_1894 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_60, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b), kwargs = {})
#   %convert_element_type_267 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1894, torch.float32), kwargs = {})
#   %var_mean_46 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_267, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_605 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_267, %getitem_276), kwargs = {})
#   %add_1903 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_275, 1e-05), kwargs = {})
#   %rsqrt_46 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1903,), kwargs = {})
#   %mul_1224 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_605, %rsqrt_46), kwargs = {})
#   %mul_1225 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1224, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w), kwargs = {})
#   %add_1904 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1225, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b), kwargs = {})
#   %convert_element_type_268 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1904, torch.float16), kwargs = {})
triton_per_fused_add_native_layer_norm_170 = async_compile.triton('triton_per_fused_add_native_layer_norm_170', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[524288, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_layer_norm_170', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_add_native_layer_norm_170(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 192
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = rindex < rnumel
    r2 = rindex
    x3 = xindex
    x0 = (xindex % 174)
    tmp0 = tl.load(in_out_ptr0 + (r2 + 192*x3), rmask & xmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp27 = tl.load(in_ptr1 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr2 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
    tmp6 = tl.where(rmask & xmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
    tmp9 = tl.where(rmask & xmask, tmp7, 0)
    tmp10 = tl.sum(tmp9, 1)[:, None]
    tmp11 = tl.full([XBLOCK, 1], 192, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
    tmp18 = tl.where(rmask & xmask, tmp16, 0)
    tmp19 = tl.sum(tmp18, 1)[:, None]
    tmp20 = tmp3 - tmp13
    tmp21 = 192.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tl.store(in_out_ptr0 + (r2 + 192*x3), tmp33, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/25/c257r7oqvmxarja5qldbpyyzc4wesrafdlzllxzktgm5sn36j5fi.py
# Topologically Sorted Source Nodes: [linear_default_3], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_default_3 => mm_default_58
# Graph fragment:
#   %mm_default_58 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_62, %permute_63), kwargs = {})
triton_tem_fused_addmm_171 = async_compile.triton('triton_tem_fused_addmm_171', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_171', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_171(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 9216
    B = arg_B

    M = ks0
    N = 1536
    K = 4608
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 33408
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4608

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1536*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/xw/cxwp5ysj3p3kkuau2zh6im52vhxwqn2qbldxeus7hw6dnlytyq5f.py
# Topologically Sorted Source Nodes: [contiguous_10, layer_norm_47, sigmoid_5, mul_1509, layer_norm_49], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   contiguous_10 => clone_9
#   layer_norm_47 => add_1956, add_1957, convert_element_type_272, convert_element_type_273, mul_1258, mul_1259, rsqrt_47, sub_621, var_mean_47
#   layer_norm_49 => add_1990, add_1991, convert_element_type_276, convert_element_type_277, mul_1280, mul_1281, rsqrt_49, sub_633, var_mean_49
#   mul_1509 => mul_1272
#   sigmoid_5 => sigmoid_5
# Graph fragment:
#   %clone_9 : [num_users=2] = call_function[target=torch.ops.aten.clone.default](args = (%slice_18,), kwargs = {memory_format: torch.contiguous_format})
#   %convert_element_type_272 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%clone_9, torch.float32), kwargs = {})
#   %var_mean_47 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_272, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_621 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_272, %getitem_281), kwargs = {})
#   %add_1956 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_280, 1e-05), kwargs = {})
#   %rsqrt_47 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1956,), kwargs = {})
#   %mul_1258 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_621, %rsqrt_47), kwargs = {})
#   %mul_1259 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1258, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight), kwargs = {})
#   %add_1957 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1259, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias), kwargs = {})
#   %convert_element_type_273 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1957, torch.float16), kwargs = {})
#   %sigmoid_5 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_273,), kwargs = {})
#   %mul_1272 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%clone_9, %sigmoid_5), kwargs = {})
#   %convert_element_type_276 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1272, torch.float32), kwargs = {})
#   %var_mean_49 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_276, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_633 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_276, %getitem_285), kwargs = {})
#   %add_1990 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_284, 1e-05), kwargs = {})
#   %rsqrt_49 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1990,), kwargs = {})
#   %mul_1280 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_633, %rsqrt_49), kwargs = {})
#   %mul_1281 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1280, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w), kwargs = {})
#   %add_1991 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1281, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b), kwargs = {})
#   %convert_element_type_277 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1991, torch.float16), kwargs = {})
triton_per_fused_clone_mul_native_layer_norm_sigmoid_172 = async_compile.triton('triton_per_fused_clone_mul_native_layer_norm_sigmoid_172', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'out_ptr5': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_clone_mul_native_layer_norm_sigmoid_172', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 6, 'num_reduction': 8, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_clone_mul_native_layer_norm_sigmoid_172(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr5, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 768
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + 1536*x0), rmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp56 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp59 = tl.load(in_ptr5 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [RBLOCK])
    tmp6 = tl.where(rmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [RBLOCK])
    tmp9 = tl.where(rmask, tmp7, 0)
    tmp10 = triton_helpers.promote_to_tensor(tl.sum(tmp9, 0))
    tmp11 = tl.full([1], 768, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [RBLOCK])
    tmp18 = tl.where(rmask, tmp16, 0)
    tmp19 = triton_helpers.promote_to_tensor(tl.sum(tmp18, 0))
    tmp20 = tmp3 - tmp13
    tmp21 = 768.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tmp34 = tl.sigmoid(tmp33)
    tmp35 = tmp2 * tmp34
    tmp36 = tmp35.to(tl.float32)
    tmp37 = tl.broadcast_to(tmp36, [RBLOCK])
    tmp39 = tl.where(rmask, tmp37, 0)
    tmp40 = tl.broadcast_to(tmp37, [RBLOCK])
    tmp42 = tl.where(rmask, tmp40, 0)
    tmp43 = triton_helpers.promote_to_tensor(tl.sum(tmp42, 0))
    tmp44 = tmp43 / tmp12
    tmp45 = tmp37 - tmp44
    tmp46 = tmp45 * tmp45
    tmp47 = tl.broadcast_to(tmp46, [RBLOCK])
    tmp49 = tl.where(rmask, tmp47, 0)
    tmp50 = triton_helpers.promote_to_tensor(tl.sum(tmp49, 0))
    tmp51 = tmp36 - tmp44
    tmp52 = tmp50 / tmp21
    tmp53 = tmp52 + tmp23
    tmp54 = libdevice.rsqrt(tmp53)
    tmp55 = tmp51 * tmp54
    tmp57 = tmp56.to(tl.float32)
    tmp58 = tmp55 * tmp57
    tmp60 = tmp59.to(tl.float32)
    tmp61 = tmp58 + tmp60
    tmp62 = tmp61.to(tl.float32)
    tl.store(out_ptr5 + (r1 + 768*x0), tmp62, rmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2b/c2bkpybkyfkslls47kscjdhwsma6urazri3sklnj4yge5jlw4dme.py
# Topologically Sorted Source Nodes: [contiguous_11, layer_norm_48, sigmoid_6, mul_1514, layer_norm_50], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   contiguous_11 => clone_10
#   layer_norm_48 => add_1967, add_1968, convert_element_type_274, convert_element_type_275, mul_1264, mul_1265, rsqrt_48, sub_625, var_mean_48
#   layer_norm_50 => add_2001, add_2002, convert_element_type_278, convert_element_type_279, mul_1286, mul_1287, rsqrt_50, sub_637, var_mean_50
#   mul_1514 => mul_1277
#   sigmoid_6 => sigmoid_6
# Graph fragment:
#   %clone_10 : [num_users=2] = call_function[target=torch.ops.aten.clone.default](args = (%slice_20,), kwargs = {memory_format: torch.contiguous_format})
#   %convert_element_type_274 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%clone_10, torch.float32), kwargs = {})
#   %var_mean_48 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_274, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_625 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_274, %getitem_283), kwargs = {})
#   %add_1967 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_282, 1e-05), kwargs = {})
#   %rsqrt_48 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1967,), kwargs = {})
#   %mul_1264 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_625, %rsqrt_48), kwargs = {})
#   %mul_1265 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1264, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight), kwargs = {})
#   %add_1968 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1265, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias), kwargs = {})
#   %convert_element_type_275 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1968, torch.float16), kwargs = {})
#   %sigmoid_6 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_275,), kwargs = {})
#   %mul_1277 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%clone_10, %sigmoid_6), kwargs = {})
#   %convert_element_type_278 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1277, torch.float32), kwargs = {})
#   %var_mean_50 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_278, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_637 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_278, %getitem_287), kwargs = {})
#   %add_2001 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_286, 1e-05), kwargs = {})
#   %rsqrt_50 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2001,), kwargs = {})
#   %mul_1286 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_637, %rsqrt_50), kwargs = {})
#   %mul_1287 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1286, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w), kwargs = {})
#   %add_2002 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1287, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b), kwargs = {})
#   %convert_element_type_279 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2002, torch.float16), kwargs = {})
triton_per_fused_clone_mul_native_layer_norm_sigmoid_173 = async_compile.triton('triton_per_fused_clone_mul_native_layer_norm_sigmoid_173', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'out_ptr5': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_clone_mul_native_layer_norm_sigmoid_173', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 6, 'num_reduction': 8, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_clone_mul_native_layer_norm_sigmoid_173(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr5, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 768
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (768 + r1 + 1536*x0), rmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (768 + r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp56 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp59 = tl.load(in_ptr5 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [RBLOCK])
    tmp6 = tl.where(rmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [RBLOCK])
    tmp9 = tl.where(rmask, tmp7, 0)
    tmp10 = triton_helpers.promote_to_tensor(tl.sum(tmp9, 0))
    tmp11 = tl.full([1], 768, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [RBLOCK])
    tmp18 = tl.where(rmask, tmp16, 0)
    tmp19 = triton_helpers.promote_to_tensor(tl.sum(tmp18, 0))
    tmp20 = tmp3 - tmp13
    tmp21 = 768.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tmp34 = tl.sigmoid(tmp33)
    tmp35 = tmp2 * tmp34
    tmp36 = tmp35.to(tl.float32)
    tmp37 = tl.broadcast_to(tmp36, [RBLOCK])
    tmp39 = tl.where(rmask, tmp37, 0)
    tmp40 = tl.broadcast_to(tmp37, [RBLOCK])
    tmp42 = tl.where(rmask, tmp40, 0)
    tmp43 = triton_helpers.promote_to_tensor(tl.sum(tmp42, 0))
    tmp44 = tmp43 / tmp12
    tmp45 = tmp37 - tmp44
    tmp46 = tmp45 * tmp45
    tmp47 = tl.broadcast_to(tmp46, [RBLOCK])
    tmp49 = tl.where(rmask, tmp47, 0)
    tmp50 = triton_helpers.promote_to_tensor(tl.sum(tmp49, 0))
    tmp51 = tmp36 - tmp44
    tmp52 = tmp50 / tmp21
    tmp53 = tmp52 + tmp23
    tmp54 = libdevice.rsqrt(tmp53)
    tmp55 = tmp51 * tmp54
    tmp57 = tmp56.to(tl.float32)
    tmp58 = tmp55 * tmp57
    tmp60 = tmp59.to(tl.float32)
    tmp61 = tmp58 + tmp60
    tmp62 = tmp61.to(tl.float32)
    tl.store(out_ptr5 + (r1 + 768*x0), tmp62, rmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/y3/cy3lzjuciu54vapummyrsthpbvdh5ddef4n44le45cvtgzivf2xa.py
# Topologically Sorted Source Nodes: [layer_norm_49, linear_61], Original ATen: [aten.native_layer_norm, aten.addmm]
# Source node to ATen node mapping:
#   layer_norm_49 => add_1990, add_1991, convert_element_type_277, mul_1280, mul_1281, rsqrt_49, sub_633, var_mean_49
#   linear_61 => mm_default_57
# Graph fragment:
#   %var_mean_49 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_276, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_633 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_276, %getitem_285), kwargs = {})
#   %add_1990 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_284, 1e-05), kwargs = {})
#   %rsqrt_49 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_1990,), kwargs = {})
#   %mul_1280 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_633, %rsqrt_49), kwargs = {})
#   %mul_1281 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1280, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w), kwargs = {})
#   %add_1991 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1281, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b), kwargs = {})
#   %convert_element_type_277 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_1991, torch.float16), kwargs = {})
#   %mm_default_57 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%convert_element_type_277, %permute_64), kwargs = {})
triton_tem_fused_addmm_native_layer_norm_174 = async_compile.triton('triton_tem_fused_addmm_native_layer_norm_174', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=5,
    num_warps=8,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_native_layer_norm_174', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_native_layer_norm_174(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 768
    K = 768
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 768
    stride_ak = 1
    stride_bk = 1
    stride_bn = 768

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 768*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/bn/cbnfe5m4frtldkhw22lfizifagpeiri7rkm3sfejmpf5bgpikb7i.py
# Topologically Sorted Source Nodes: [linear_61, layer_norm_51, sigmoid_7, mul_1531, layer_norm_53], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_51 => add_2018, add_2019, convert_element_type_286, convert_element_type_287, mul_1296, mul_1297, rsqrt_51, sub_643, var_mean_51
#   layer_norm_53 => add_2052, add_2053, convert_element_type_290, convert_element_type_291, mul_1318, mul_1319, rsqrt_53, sub_655, var_mean_53
#   linear_61 => add_tensor_57
#   mul_1531 => mul_1310
#   sigmoid_7 => sigmoid_7
# Graph fragment:
#   %add_tensor_57 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_57, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias), kwargs = {})
#   %convert_element_type_286 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_57, torch.float32), kwargs = {})
#   %var_mean_51 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_286, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_643 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_286, %getitem_289), kwargs = {})
#   %add_2018 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_288, 1e-05), kwargs = {})
#   %rsqrt_51 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2018,), kwargs = {})
#   %mul_1296 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_643, %rsqrt_51), kwargs = {})
#   %mul_1297 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1296, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight), kwargs = {})
#   %add_2019 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1297, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias), kwargs = {})
#   %convert_element_type_287 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2019, torch.float16), kwargs = {})
#   %sigmoid_7 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_287,), kwargs = {})
#   %mul_1310 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_57, %sigmoid_7), kwargs = {})
#   %convert_element_type_290 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1310, torch.float32), kwargs = {})
#   %var_mean_53 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_290, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_655 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_290, %getitem_293), kwargs = {})
#   %add_2052 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_292, 1e-05), kwargs = {})
#   %rsqrt_53 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2052,), kwargs = {})
#   %mul_1318 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_655, %rsqrt_53), kwargs = {})
#   %mul_1319 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1318, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w), kwargs = {})
#   %add_2053 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1319, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b), kwargs = {})
#   %convert_element_type_291 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2053, torch.float16), kwargs = {})
triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175 = async_compile.triton('triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'out_ptr5': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 6, 'num_reduction': 8, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr5, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 768
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + 768*x0), rmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp56 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp59 = tl.load(in_ptr5 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [RBLOCK])
    tmp6 = tl.where(rmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [RBLOCK])
    tmp9 = tl.where(rmask, tmp7, 0)
    tmp10 = triton_helpers.promote_to_tensor(tl.sum(tmp9, 0))
    tmp11 = tl.full([1], 768, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [RBLOCK])
    tmp18 = tl.where(rmask, tmp16, 0)
    tmp19 = triton_helpers.promote_to_tensor(tl.sum(tmp18, 0))
    tmp20 = tmp3 - tmp13
    tmp21 = 768.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tmp34 = tl.sigmoid(tmp33)
    tmp35 = tmp2 * tmp34
    tmp36 = tmp35.to(tl.float32)
    tmp37 = tl.broadcast_to(tmp36, [RBLOCK])
    tmp39 = tl.where(rmask, tmp37, 0)
    tmp40 = tl.broadcast_to(tmp37, [RBLOCK])
    tmp42 = tl.where(rmask, tmp40, 0)
    tmp43 = triton_helpers.promote_to_tensor(tl.sum(tmp42, 0))
    tmp44 = tmp43 / tmp12
    tmp45 = tmp37 - tmp44
    tmp46 = tmp45 * tmp45
    tmp47 = tl.broadcast_to(tmp46, [RBLOCK])
    tmp49 = tl.where(rmask, tmp47, 0)
    tmp50 = triton_helpers.promote_to_tensor(tl.sum(tmp49, 0))
    tmp51 = tmp36 - tmp44
    tmp52 = tmp50 / tmp21
    tmp53 = tmp52 + tmp23
    tmp54 = libdevice.rsqrt(tmp53)
    tmp55 = tmp51 * tmp54
    tmp57 = tmp56.to(tl.float32)
    tmp58 = tmp55 * tmp57
    tmp60 = tmp59.to(tl.float32)
    tmp61 = tmp58 + tmp60
    tmp62 = tmp61.to(tl.float32)
    tl.store(out_ptr5 + (r1 + 768*x0), tmp62, rmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/7r/c7r2ceaqogoxtugonvrqm2w5mdc4ybw4ou2j2oge3rbpkbjt7hfz.py
# Topologically Sorted Source Nodes: [linear_63, layer_norm_55], Original ATen: [aten.addmm, aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_55 => add_2080, add_2081, convert_element_type_300, convert_element_type_301, mul_1334, mul_1335, rsqrt_55, sub_665, var_mean_55
#   linear_63 => add_tensor_55
# Graph fragment:
#   %add_tensor_55 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_55, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_300 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_55, torch.float32), kwargs = {})
#   %var_mean_55 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_300, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_665 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_300, %getitem_297), kwargs = {})
#   %add_2080 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_296, 1e-05), kwargs = {})
#   %rsqrt_55 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2080,), kwargs = {})
#   %mul_1334 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_665, %rsqrt_55), kwargs = {})
#   %mul_1335 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1334, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w), kwargs = {})
#   %add_2081 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1335, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b), kwargs = {})
#   %convert_element_type_301 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2081, torch.float16), kwargs = {})
triton_red_fused_addmm_native_layer_norm_176 = async_compile.triton('triton_red_fused_addmm_native_layer_norm_176', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 32768],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_addmm_native_layer_norm_176', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_addmm_native_layer_norm_176(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 21984
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_out_ptr0 + (r1 + 21984*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_out_ptr0 + (r1 + 21984*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp19 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp22 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tmp8 + tmp9
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp11 - tmp5
        tmp13 = 21984.0
        tmp14 = tmp6 / tmp13
        tmp15 = 1e-05
        tmp16 = tmp14 + tmp15
        tmp17 = libdevice.rsqrt(tmp16)
        tmp18 = tmp12 * tmp17
        tmp20 = tmp19.to(tl.float32)
        tmp21 = tmp18 * tmp20
        tmp23 = tmp22.to(tl.float32)
        tmp24 = tmp21 + tmp23
        tmp25 = tmp24.to(tl.float32)
        tl.store(in_out_ptr0 + (r1 + 21984*x0), tmp25, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kc/ckclzkyrknz72jfrfplojdieyalyoct6vocpi45p6wfdgpcw3zk2.py
# Topologically Sorted Source Nodes: [linear_64, layer_norm_56], Original ATen: [aten.addmm, aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_56 => convert_element_type_302, var_mean_56
#   linear_64 => add_tensor_54
# Graph fragment:
#   %add_tensor_54 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_54, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_302 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_54, torch.float32), kwargs = {})
#   %var_mean_56 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_302, [1]), kwargs = {correction: 0, keepdim: True})
triton_red_fused_addmm_native_layer_norm_177 = async_compile.triton('triton_red_fused_addmm_native_layer_norm_177', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 16384],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'out_ptr0': '*fp32', 'out_ptr1': '*fp32', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_addmm_native_layer_norm_177', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_addmm_native_layer_norm_177(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 9216
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (r1 + 9216*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    tl.store(out_ptr0 + (x0), tmp5, xmask)
    tl.store(out_ptr1 + (x0), tmp6, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/5m/c5m3i2mf353ro76ecbw6ss2wnuhn73jrs3j7pvv4vpf24pntywr3.py
# Topologically Sorted Source Nodes: [bmm], Original ATen: [aten.bmm]
# Source node to ATen node mapping:
#   bmm => bmm_1
# Graph fragment:
#   %bmm_1 : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%permute_62, %view_63), kwargs = {})
triton_tem_fused_bmm_178 = async_compile.triton('triton_tem_fused_bmm_178', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_bmm_178', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_bmm_178(arg_A, arg_B, out_ptr0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = 192
    N = 48
    K = 458

    stride_aq = 87936
    stride_am = 1
    stride_ak = 192

    stride_bq = 21984
    stride_bk = 48
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 48*idx_m + 9216*idx_q
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta15 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/d7/cd7spg7rogrbuuknmlean6kwqfhmwo77yguxit42agmsqeoy4h2s.py
# Topologically Sorted Source Nodes: [add_2006, layer_norm_57], Original ATen: [aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_2006 => add_2114
#   layer_norm_57 => add_2119, add_2120, convert_element_type_306, convert_element_type_307, mul_1354, mul_1355, rsqrt_57, sub_677, var_mean_57
# Graph fragment:
#   %add_2114 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_64, %bmm_1), kwargs = {})
#   %convert_element_type_306 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2114, torch.float32), kwargs = {})
#   %var_mean_57 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_306, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_677 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_306, %getitem_301), kwargs = {})
#   %add_2119 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_300, 1e-05), kwargs = {})
#   %rsqrt_57 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2119,), kwargs = {})
#   %mul_1354 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_677, %rsqrt_57), kwargs = {})
#   %mul_1355 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1354, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w), kwargs = {})
#   %add_2120 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1355, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b), kwargs = {})
#   %convert_element_type_307 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2120, torch.float16), kwargs = {})
triton_per_fused_add_native_layer_norm_179 = async_compile.triton('triton_per_fused_add_native_layer_norm_179', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[524288, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'out_ptr3': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_layer_norm_179', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_add_native_layer_norm_179(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr3, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 48
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = rindex < rnumel
    r2 = rindex
    x3 = xindex
    x0 = (xindex % 192)
    x1 = xindex // 192
    tmp0 = tl.load(in_ptr0 + (r2 + 48*x3), rmask & xmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr1 + (r2 + 48*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp4 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x1), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr4 + (r2 + 48*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tl.load(in_ptr5 + (r2 + 48*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp20 = tl.load(in_ptr6 + (r2 + 48*x3), rmask & xmask, other=0.0).to(tl.float32)
    tmp45 = tl.load(in_ptr7 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp48 = tl.load(in_ptr8 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp5 = tmp3 - tmp4
    tmp7 = 9216.0
    tmp8 = tmp6 / tmp7
    tmp9 = 1e-05
    tmp10 = tmp8 + tmp9
    tmp11 = libdevice.rsqrt(tmp10)
    tmp12 = tmp5 * tmp11
    tmp14 = tmp13.to(tl.float32)
    tmp15 = tmp12 * tmp14
    tmp17 = tmp16.to(tl.float32)
    tmp18 = tmp15 + tmp17
    tmp19 = tmp18.to(tl.float32)
    tmp21 = tmp19 + tmp20
    tmp22 = tmp21.to(tl.float32)
    tmp23 = tl.broadcast_to(tmp22, [XBLOCK, RBLOCK])
    tmp25 = tl.where(rmask & xmask, tmp23, 0)
    tmp26 = tl.broadcast_to(tmp23, [XBLOCK, RBLOCK])
    tmp28 = tl.where(rmask & xmask, tmp26, 0)
    tmp29 = tl.sum(tmp28, 1)[:, None]
    tmp30 = tl.full([XBLOCK, 1], 48, tl.int32)
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 / tmp31
    tmp33 = tmp23 - tmp32
    tmp34 = tmp33 * tmp33
    tmp35 = tl.broadcast_to(tmp34, [XBLOCK, RBLOCK])
    tmp37 = tl.where(rmask & xmask, tmp35, 0)
    tmp38 = tl.sum(tmp37, 1)[:, None]
    tmp39 = tmp22 - tmp32
    tmp40 = 48.0
    tmp41 = tmp38 / tmp40
    tmp42 = tmp41 + tmp9
    tmp43 = libdevice.rsqrt(tmp42)
    tmp44 = tmp39 * tmp43
    tmp46 = tmp45.to(tl.float32)
    tmp47 = tmp44 * tmp46
    tmp49 = tmp48.to(tl.float32)
    tmp50 = tmp47 + tmp49
    tmp51 = tmp50.to(tl.float32)
    tl.store(out_ptr3 + (r2 + 48*x3), tmp51, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/tx/ctxaendyxnlq7bruknrk7cqlkwohvinrri6k7xtabvsbh42wb5rc.py
# Topologically Sorted Source Nodes: [layer_norm_57, bmm_1], Original ATen: [aten.native_layer_norm, aten.bmm]
# Source node to ATen node mapping:
#   bmm_1 => bmm_2
#   layer_norm_57 => add_2119, add_2120, convert_element_type_307, mul_1354, mul_1355, rsqrt_57, sub_677, var_mean_57
# Graph fragment:
#   %var_mean_57 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_306, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_677 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_306, %getitem_301), kwargs = {})
#   %add_2119 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_300, 1e-05), kwargs = {})
#   %rsqrt_57 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2119,), kwargs = {})
#   %mul_1354 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_677, %rsqrt_57), kwargs = {})
#   %mul_1355 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1354, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w), kwargs = {})
#   %add_2120 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1355, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b), kwargs = {})
#   %convert_element_type_307 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2120, torch.float16), kwargs = {})
#   %bmm_2 : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%view_57, %convert_element_type_307), kwargs = {})
triton_tem_fused_bmm_native_layer_norm_180 = async_compile.triton('triton_tem_fused_bmm_native_layer_norm_180', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_bmm_native_layer_norm_180', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_bmm_native_layer_norm_180(arg_A, arg_B, out_ptr0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = 458
    N = 48
    K = 192

    stride_aq = 87936
    stride_am = 192
    stride_ak = 1

    stride_bq = 9216
    stride_bk = 48
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 48*idx_m + 21984*idx_q
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/rp/crp3smbg5in6gzfn6rzcvm7ordddns4obtkcnv4nwlmbn5zymjrz.py
# Topologically Sorted Source Nodes: [layer_norm_58], Original ATen: [aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_58 => add_2140, add_2141, convert_element_type_310, convert_element_type_311, mul_1366, mul_1367, rsqrt_58, sub_683, var_mean_58
# Graph fragment:
#   %convert_element_type_310 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%view_65, torch.float32), kwargs = {})
#   %var_mean_58 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_310, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_683 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_310, %getitem_303), kwargs = {})
#   %add_2140 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_302, 1e-05), kwargs = {})
#   %rsqrt_58 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2140,), kwargs = {})
#   %mul_1366 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_683, %rsqrt_58), kwargs = {})
#   %mul_1367 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1366, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w), kwargs = {})
#   %add_2141 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1367, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b), kwargs = {})
#   %convert_element_type_311 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2141, torch.float16), kwargs = {})
triton_red_fused_native_layer_norm_181 = async_compile.triton('triton_red_fused_native_layer_norm_181', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 32768],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_layer_norm_181', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_native_layer_norm_181(in_out_ptr0, in_ptr0, in_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 21984
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_out_ptr0 + (r1 + 21984*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(rmask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(rmask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(rmask & xmask, tmp3_weight_next, tmp3_weight)
    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(
        tmp3_mean, tmp3_m2, tmp3_weight, 1
    )
    tmp3 = tmp3_tmp[:, None]
    tmp4 = tmp4_tmp[:, None]
    tmp5 = tmp5_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp6 = tl.load(in_out_ptr0 + (r1 + 21984*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp15 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp18 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tmp7 - tmp3
        tmp9 = 21984.0
        tmp10 = tmp4 / tmp9
        tmp11 = 1e-05
        tmp12 = tmp10 + tmp11
        tmp13 = libdevice.rsqrt(tmp12)
        tmp14 = tmp8 * tmp13
        tmp16 = tmp15.to(tl.float32)
        tmp17 = tmp14 * tmp16
        tmp19 = tmp18.to(tl.float32)
        tmp20 = tmp17 + tmp19
        tmp21 = tmp20.to(tl.float32)
        tl.store(in_out_ptr0 + (r1 + 21984*x0), tmp21, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/qy/cqyf5ya6pixrcgfdrq7gpppbay5uf6ws3a54d2f6vygscnxi2tmn.py
# Topologically Sorted Source Nodes: [linear_65, layer_norm_59, sigmoid_9, mul_1580, layer_norm_60], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_59 => add_2154, add_2155, convert_element_type_315, convert_element_type_316, mul_1374, mul_1375, rsqrt_59, sub_688, var_mean_59
#   layer_norm_60 => convert_element_type_317, var_mean_60
#   linear_65 => add_tensor_53
#   mul_1580 => mul_1382
#   sigmoid_9 => sigmoid_9
# Graph fragment:
#   %add_tensor_53 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_53, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_315 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_53, torch.float32), kwargs = {})
#   %var_mean_59 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_315, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_688 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_315, %getitem_305), kwargs = {})
#   %add_2154 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_304, 1e-05), kwargs = {})
#   %rsqrt_59 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2154,), kwargs = {})
#   %mul_1374 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_688, %rsqrt_59), kwargs = {})
#   %mul_1375 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1374, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight), kwargs = {})
#   %add_2155 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1375, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias), kwargs = {})
#   %convert_element_type_316 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2155, torch.float16), kwargs = {})
#   %sigmoid_9 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_316,), kwargs = {})
#   %mul_1382 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_53, %sigmoid_9), kwargs = {})
#   %convert_element_type_317 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1382, torch.float32), kwargs = {})
#   %var_mean_60 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_317, [1]), kwargs = {correction: 0, keepdim: True})
triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182 = async_compile.triton('triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'out_ptr2': '*fp32', 'out_ptr3': '*fp32', 'out_ptr4': '*fp32', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr2, out_ptr3, out_ptr4, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 2048
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (r1 + 2048*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    tmp30_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_ptr0 + (r1 + 2048*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp19 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp22 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tmp8 + tmp9
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp11 - tmp5
        tmp13 = 2048.0
        tmp14 = tmp6 / tmp13
        tmp15 = 1e-05
        tmp16 = tmp14 + tmp15
        tmp17 = libdevice.rsqrt(tmp16)
        tmp18 = tmp12 * tmp17
        tmp20 = tmp19.to(tl.float32)
        tmp21 = tmp18 * tmp20
        tmp23 = tmp22.to(tl.float32)
        tmp24 = tmp21 + tmp23
        tmp25 = tmp24.to(tl.float32)
        tmp26 = tl.sigmoid(tmp25)
        tmp27 = tmp10 * tmp26
        tmp28 = tmp27.to(tl.float32)
        tmp29 = tl.broadcast_to(tmp28, [XBLOCK, RBLOCK])
        tmp30_mean_next, tmp30_m2_next, tmp30_weight_next = triton_helpers.welford_reduce(
            tmp29, tmp30_mean, tmp30_m2, tmp30_weight, roffset == 0
        )
        tmp30_mean = tl.where(rmask & xmask, tmp30_mean_next, tmp30_mean)
        tmp30_m2 = tl.where(rmask & xmask, tmp30_m2_next, tmp30_m2)
        tmp30_weight = tl.where(rmask & xmask, tmp30_weight_next, tmp30_weight)
        tl.store(out_ptr2 + (r1 + 2048*x0), tmp28, rmask & xmask)
    tmp30_tmp, tmp31_tmp, tmp32_tmp = triton_helpers.welford(
        tmp30_mean, tmp30_m2, tmp30_weight, 1
    )
    tmp30 = tmp30_tmp[:, None]
    tmp31 = tmp31_tmp[:, None]
    tmp32 = tmp32_tmp[:, None]
    tl.store(out_ptr3 + (x0), tmp30, xmask)
    tl.store(out_ptr4 + (x0), tmp31, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/vq/cvquploogp4tbgpfo2tmnzkychnusqrta7vpldjmw4fseqikl6ov.py
# Topologically Sorted Source Nodes: [cat_default_6, layer_norm_61], Original ATen: [aten.cat, aten.native_layer_norm]
# Source node to ATen node mapping:
#   cat_default_6 => cat_14
#   layer_norm_61 => add_2185, add_2186, convert_element_type_319, convert_element_type_320, mul_1393, mul_1394, rsqrt_61, sub_699, var_mean_61
# Graph fragment:
#   %cat_14 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%mul_812, %mul_809, %convert_element_type_318], 1), kwargs = {})
#   %convert_element_type_319 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%cat_14, torch.float32), kwargs = {})
#   %var_mean_61 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_319, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_699 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_319, %getitem_309), kwargs = {})
#   %add_2185 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_308, 1e-05), kwargs = {})
#   %rsqrt_61 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2185,), kwargs = {})
#   %mul_1393 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_699, %rsqrt_61), kwargs = {})
#   %mul_1394 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1393, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w), kwargs = {})
#   %add_2186 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1394, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b), kwargs = {})
#   %convert_element_type_320 : [num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2186, torch.float16), kwargs = {})
triton_red_fused_cat_native_layer_norm_183 = async_compile.triton('triton_red_fused_cat_native_layer_norm_183', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp32', 'in_ptr3': '*fp32', 'in_ptr4': '*fp32', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_cat_native_layer_norm_183', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 10, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_cat_native_layer_norm_183(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 6354
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp37_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp37_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp37_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = r1
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 1024, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (1024*x0 + (r1)), rmask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 4306, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tmp6 & tmp8
        tmp10 = tl.load(in_ptr1 + (3282*x0 + ((-1024) + r1)), rmask & tmp9 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tmp0 >= tmp7
        tmp12 = tl.full([1, 1], 6354, tl.int64)
        tmp13 = tmp0 < tmp12
        tmp14 = tl.load(in_ptr2 + (2048*x0 + ((-4306) + r1)), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp15 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp16 = tmp14 - tmp15
        tmp17 = tl.load(in_ptr4 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp18 = 2048.0
        tmp19 = tmp17 / tmp18
        tmp20 = 1e-05
        tmp21 = tmp19 + tmp20
        tmp22 = libdevice.rsqrt(tmp21)
        tmp23 = tmp16 * tmp22
        tmp24 = tl.load(in_ptr5 + (tl.broadcast_to((-4306) + r1, [XBLOCK, RBLOCK])), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp25 = tmp24.to(tl.float32)
        tmp26 = tmp23 * tmp25
        tmp27 = tl.load(in_ptr6 + (tl.broadcast_to((-4306) + r1, [XBLOCK, RBLOCK])), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp28 = tmp27.to(tl.float32)
        tmp29 = tmp26 + tmp28
        tmp30 = tmp29.to(tl.float32)
        tmp31 = tl.full(tmp30.shape, 0.0, tmp30.dtype)
        tmp32 = tl.where(tmp11, tmp30, tmp31)
        tmp33 = tl.where(tmp9, tmp10, tmp32)
        tmp34 = tl.where(tmp4, tmp5, tmp33)
        tmp35 = tmp34.to(tl.float32)
        tmp36 = tl.broadcast_to(tmp35, [XBLOCK, RBLOCK])
        tmp37_mean_next, tmp37_m2_next, tmp37_weight_next = triton_helpers.welford_reduce(
            tmp36, tmp37_mean, tmp37_m2, tmp37_weight, roffset == 0
        )
        tmp37_mean = tl.where(rmask & xmask, tmp37_mean_next, tmp37_mean)
        tmp37_m2 = tl.where(rmask & xmask, tmp37_m2_next, tmp37_m2)
        tmp37_weight = tl.where(rmask & xmask, tmp37_weight_next, tmp37_weight)
        tl.store(in_out_ptr0 + (r1 + 6354*x0), tmp34, rmask & xmask)
    tmp37_tmp, tmp38_tmp, tmp39_tmp = triton_helpers.welford(
        tmp37_mean, tmp37_m2, tmp37_weight, 1
    )
    tmp37 = tmp37_tmp[:, None]
    tmp38 = tmp38_tmp[:, None]
    tmp39 = tmp39_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp40 = tl.load(in_out_ptr0 + (r1 + 6354*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp49 = tl.load(in_ptr7 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp52 = tl.load(in_ptr8 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp41 = tmp40.to(tl.float32)
        tmp42 = tmp41 - tmp37
        tmp43 = 6354.0
        tmp44 = tmp38 / tmp43
        tmp45 = 1e-05
        tmp46 = tmp44 + tmp45
        tmp47 = libdevice.rsqrt(tmp46)
        tmp48 = tmp42 * tmp47
        tmp50 = tmp49.to(tl.float32)
        tmp51 = tmp48 * tmp50
        tmp53 = tmp52.to(tl.float32)
        tmp54 = tmp51 + tmp53
        tmp55 = tmp54.to(tl.float32)
        tl.store(in_out_ptr0 + (r1 + 6354*x0), tmp55, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/mb/cmbo6n36dezflxf53slfgaaj4jblu7oqx2f2gihnep7rrtrv5y4a.py
# Topologically Sorted Source Nodes: [linear_66], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_66 => constant_pad_nd_default_18
# Graph fragment:
#   %constant_pad_nd_default_18 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%convert_element_type_320, [0, 6, 0, 0]), kwargs = {})
triton_poi_fused_addmm_184 = async_compile.triton('triton_poi_fused_addmm_184', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[16777216],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_184', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_addmm_184(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 6360)
    x1 = xindex // 6360
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 6354, tl.int64)
    tmp2 = tmp0 < tmp1
    tmp3 = tl.load(in_ptr0 + (x0 + 6354*x1), tmp2 & xmask, other=0.0).to(tl.float32)
    tl.store(out_ptr0 + (x2), tmp3, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/kj/ckj6cflxtjz2lw2agi4oh2wpvfborausiqyntkl7ozxflpdoezuu.py
# Topologically Sorted Source Nodes: [linear_66], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_66 => constant_pad_nd_default_18, mm_default_52
# Graph fragment:
#   %constant_pad_nd_default_18 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%convert_element_type_320, [0, 6, 0, 0]), kwargs = {})
#   %mm_default_52 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%constant_pad_nd_default_18, %constant_pad_nd_default_19), kwargs = {})
triton_tem_fused_addmm_185 = async_compile.triton('triton_tem_fused_addmm_185', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_185', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_185(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 384
    K = 6360
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 6360
    stride_ak = 1
    stride_bk = 384
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 384*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta16 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 128}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/jh/cjhwul4w3ihulwlovp47w6fvh4vgpiqg2izi6rgqw66rzzf3gk3p.py
# Topologically Sorted Source Nodes: [linear_66, layer_norm_62], Original ATen: [aten.addmm, aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_62 => add_2199, add_2200, convert_element_type_324, convert_element_type_325, mul_1401, mul_1402, rsqrt_62, sub_704, var_mean_62
#   linear_66 => add_tensor_52
# Graph fragment:
#   %add_tensor_52 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_52, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_324 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_52, torch.float32), kwargs = {})
#   %var_mean_62 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_324, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_704 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_324, %getitem_311), kwargs = {})
#   %add_2199 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_310, 1e-05), kwargs = {})
#   %rsqrt_62 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2199,), kwargs = {})
#   %mul_1401 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_704, %rsqrt_62), kwargs = {})
#   %mul_1402 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1401, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w), kwargs = {})
#   %add_2200 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1402, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b), kwargs = {})
#   %convert_element_type_325 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2200, torch.float16), kwargs = {})
triton_per_fused_addmm_native_layer_norm_186 = async_compile.triton('triton_per_fused_addmm_native_layer_norm_186', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_addmm_native_layer_norm_186', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_addmm_native_layer_norm_186(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 384
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (r1 + 384*x0), rmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp27 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [RBLOCK])
    tmp6 = tl.where(rmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [RBLOCK])
    tmp9 = tl.where(rmask, tmp7, 0)
    tmp10 = triton_helpers.promote_to_tensor(tl.sum(tmp9, 0))
    tmp11 = tl.full([1], 384, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [RBLOCK])
    tmp18 = tl.where(rmask, tmp16, 0)
    tmp19 = triton_helpers.promote_to_tensor(tl.sum(tmp18, 0))
    tmp20 = tmp3 - tmp13
    tmp21 = 384.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tl.store(in_out_ptr0 + (r1 + 384*x0), tmp33, rmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/c6/cc6rk4rfwlzkego7sv2fe54lnvmjpunezjdkollc2cjbxwrtpekt.py
# Topologically Sorted Source Nodes: [linear_66, layer_norm_62, linear_67], Original ATen: [aten.addmm, aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_62 => add_2199, add_2200, convert_element_type_324, convert_element_type_325, mul_1401, mul_1402, rsqrt_62, sub_704, var_mean_62
#   linear_66 => add_tensor_52
#   linear_67 => mm_default_51
# Graph fragment:
#   %add_tensor_52 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_52, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_324 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_52, torch.float32), kwargs = {})
#   %var_mean_62 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_324, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_704 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_324, %getitem_311), kwargs = {})
#   %add_2199 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_310, 1e-05), kwargs = {})
#   %rsqrt_62 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2199,), kwargs = {})
#   %mul_1401 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_704, %rsqrt_62), kwargs = {})
#   %mul_1402 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1401, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w), kwargs = {})
#   %add_2200 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1402, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b), kwargs = {})
#   %convert_element_type_325 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2200, torch.float16), kwargs = {})
#   %mm_default_51 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%convert_element_type_325, %permute_70), kwargs = {})
triton_tem_fused_addmm_native_layer_norm_187 = async_compile.triton('triton_tem_fused_addmm_native_layer_norm_187', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_native_layer_norm_187', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_native_layer_norm_187(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = ks0
    N = 6354
    K = 384
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 384
    stride_ak = 1
    stride_bk = 1
    stride_bn = 384

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 6354*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/gk/cgkz5lfmbyubmmziwsknwjtw5m4qt7cz2agzchtup6yv4yzcnayd.py
# Topologically Sorted Source Nodes: [linear_67, layer_norm_63, addcmul, layer_norm_64], Original ATen: [aten.addmm, aten.native_layer_norm, aten.addcmul]
# Source node to ATen node mapping:
#   addcmul => add_2224, convert_element_type_331, convert_element_type_332, mul_1415, mul_1416
#   layer_norm_63 => add_2213, add_2214, convert_element_type_329, mul_1409, mul_1410, rsqrt_63, sub_709, var_mean_63
#   layer_norm_64 => var_mean_64
#   linear_67 => add_tensor_51
# Graph fragment:
#   %add_tensor_51 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_51, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_329 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_51, torch.float32), kwargs = {})
#   %var_mean_63 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_329, [1]), kwargs = {correction: 0, keepdim: True})
#   %convert_element_type_331 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_320, torch.float32), kwargs = {})
#   %convert_element_type_332 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%convert_element_type_320, torch.float32), kwargs = {})
#   %mul_1415 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%convert_element_type_332, 1.0), kwargs = {})
#   %sub_709 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_329, %getitem_313), kwargs = {})
#   %add_2213 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_312, 1e-05), kwargs = {})
#   %rsqrt_63 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2213,), kwargs = {})
#   %mul_1409 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_709, %rsqrt_63), kwargs = {})
#   %mul_1410 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1409, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w), kwargs = {})
#   %add_2214 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1410, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b), kwargs = {})
#   %mul_1416 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1415, %add_2214), kwargs = {})
#   %add_2224 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_331, %mul_1416), kwargs = {})
#   %var_mean_64 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%add_2224, [1]), kwargs = {correction: 0, keepdim: True})
triton_red_fused_addcmul_addmm_native_layer_norm_188 = async_compile.triton('triton_red_fused_addcmul_addmm_native_layer_norm_188', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr2': '*fp32', 'out_ptr3': '*fp32', 'out_ptr4': '*fp32', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_addcmul_addmm_native_layer_norm_188', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 7, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_addcmul_addmm_native_layer_norm_188(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr2, out_ptr3, out_ptr4, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 6354
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (r1 + 6354*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    tmp32_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp32_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp32_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_ptr2 + (r1 + 6354*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp12 = tl.load(in_ptr0 + (r1 + 6354*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp13 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp23 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp26 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp9 = tmp8.to(tl.float32)
        tmp10 = 1.0
        tmp11 = tmp9 * tmp10
        tmp14 = tmp12 + tmp13
        tmp15 = tmp14.to(tl.float32)
        tmp16 = tmp15 - tmp5
        tmp17 = 6354.0
        tmp18 = tmp6 / tmp17
        tmp19 = 1e-05
        tmp20 = tmp18 + tmp19
        tmp21 = libdevice.rsqrt(tmp20)
        tmp22 = tmp16 * tmp21
        tmp24 = tmp23.to(tl.float32)
        tmp25 = tmp22 * tmp24
        tmp27 = tmp26.to(tl.float32)
        tmp28 = tmp25 + tmp27
        tmp29 = tmp11 * tmp28
        tmp30 = tmp9 + tmp29
        tmp31 = tl.broadcast_to(tmp30, [XBLOCK, RBLOCK])
        tmp32_mean_next, tmp32_m2_next, tmp32_weight_next = triton_helpers.welford_reduce(
            tmp31, tmp32_mean, tmp32_m2, tmp32_weight, roffset == 0
        )
        tmp32_mean = tl.where(rmask & xmask, tmp32_mean_next, tmp32_mean)
        tmp32_m2 = tl.where(rmask & xmask, tmp32_m2_next, tmp32_m2)
        tmp32_weight = tl.where(rmask & xmask, tmp32_weight_next, tmp32_weight)
        tl.store(out_ptr2 + (r1 + 6354*x0), tmp30, rmask & xmask)
    tmp32_tmp, tmp33_tmp, tmp34_tmp = triton_helpers.welford(
        tmp32_mean, tmp32_m2, tmp32_weight, 1
    )
    tmp32 = tmp32_tmp[:, None]
    tmp33 = tmp33_tmp[:, None]
    tmp34 = tmp34_tmp[:, None]
    tl.store(out_ptr3 + (x0), tmp32, xmask)
    tl.store(out_ptr4 + (x0), tmp33, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/do/cdoewlqouxeuhos73w6p24usxetmp6rwaaokdm5r3wmww3xuuviw.py
# Topologically Sorted Source Nodes: [layer_norm_64, linear_68], Original ATen: [aten.native_layer_norm, aten.addmm]
# Source node to ATen node mapping:
#   layer_norm_64 => add_2228, add_2229, convert_element_type_336, mul_1419, mul_1420, rsqrt_64, sub_714, var_mean_64
#   linear_68 => constant_pad_nd_default_16
# Graph fragment:
#   %var_mean_64 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%add_2224, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_714 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%add_2224, %getitem_315), kwargs = {})
#   %add_2228 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_314, 1e-05), kwargs = {})
#   %rsqrt_64 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2228,), kwargs = {})
#   %mul_1419 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_714, %rsqrt_64), kwargs = {})
#   %mul_1420 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1419, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w), kwargs = {})
#   %add_2229 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1420, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b), kwargs = {})
#   %convert_element_type_336 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2229, torch.float16), kwargs = {})
#   %constant_pad_nd_default_16 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%convert_element_type_336, [0, 6, 0, 0]), kwargs = {})
triton_poi_fused_addmm_native_layer_norm_189 = async_compile.triton('triton_poi_fused_addmm_native_layer_norm_189', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[16777216],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'in_ptr2': '*fp32', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_native_layer_norm_189', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_addmm_native_layer_norm_189(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 6360)
    x1 = xindex // 6360
    x2 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 6354, tl.int64)
    tmp2 = tmp0 < tmp1
    tmp3 = tl.load(in_ptr0 + (x0 + 6354*x1), tmp2 & xmask, other=0.0)
    tmp4 = tl.load(in_ptr1 + (x1), tmp2 & xmask, eviction_policy='evict_last', other=0.0)
    tmp5 = tmp3 - tmp4
    tmp6 = tl.load(in_ptr2 + (x1), tmp2 & xmask, eviction_policy='evict_last', other=0.0)
    tmp7 = 6354.0
    tmp8 = tmp6 / tmp7
    tmp9 = 1e-05
    tmp10 = tmp8 + tmp9
    tmp11 = libdevice.rsqrt(tmp10)
    tmp12 = tmp5 * tmp11
    tmp13 = tl.load(in_ptr3 + (x0), tmp2 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp14 = tmp13.to(tl.float32)
    tmp15 = tmp12 * tmp14
    tmp16 = tl.load(in_ptr4 + (x0), tmp2 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp17 = tmp16.to(tl.float32)
    tmp18 = tmp15 + tmp17
    tmp19 = tmp18.to(tl.float32)
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp2, tmp19, tmp20)
    tl.store(out_ptr0 + (x2), tmp21, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/fz/cfzm5azp5p4y7u6o2hjibsub7qek6e73vwjl2nn4vdumx7ubu5hb.py
# Topologically Sorted Source Nodes: [linear_68, layer_norm_65, sigmoid_10, mul_1607, layer_norm_66], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_65 => add_2242, add_2243, convert_element_type_340, convert_element_type_341, mul_1427, mul_1428, rsqrt_65, sub_719, var_mean_65
#   layer_norm_66 => add_2259, add_2260, convert_element_type_342, convert_element_type_343, mul_1438, mul_1439, rsqrt_66, sub_725, var_mean_66
#   linear_68 => add_tensor_50
#   mul_1607 => mul_1435
#   sigmoid_10 => sigmoid_10
# Graph fragment:
#   %add_tensor_50 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_50, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_340 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_50, torch.float32), kwargs = {})
#   %var_mean_65 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_340, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_719 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_340, %getitem_317), kwargs = {})
#   %add_2242 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_316, 1e-05), kwargs = {})
#   %rsqrt_65 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2242,), kwargs = {})
#   %mul_1427 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_719, %rsqrt_65), kwargs = {})
#   %mul_1428 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1427, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight), kwargs = {})
#   %add_2243 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1428, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias), kwargs = {})
#   %convert_element_type_341 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2243, torch.float16), kwargs = {})
#   %sigmoid_10 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_341,), kwargs = {})
#   %mul_1435 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_50, %sigmoid_10), kwargs = {})
#   %convert_element_type_342 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1435, torch.float32), kwargs = {})
#   %var_mean_66 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_342, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_725 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_342, %getitem_319), kwargs = {})
#   %add_2259 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_318, 1e-05), kwargs = {})
#   %rsqrt_66 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2259,), kwargs = {})
#   %mul_1438 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_725, %rsqrt_66), kwargs = {})
#   %mul_1439 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1438, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w), kwargs = {})
#   %add_2260 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1439, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b), kwargs = {})
#   %convert_element_type_343 : [num_users=3] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2260, torch.float16), kwargs = {})
triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190 = async_compile.triton('triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'out_ptr2': '*fp32', 'out_ptr5': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr2, out_ptr5, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 3072
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    tmp30_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp19 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp22 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tmp8 + tmp9
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp11 - tmp5
        tmp13 = 3072.0
        tmp14 = tmp6 / tmp13
        tmp15 = 1e-05
        tmp16 = tmp14 + tmp15
        tmp17 = libdevice.rsqrt(tmp16)
        tmp18 = tmp12 * tmp17
        tmp20 = tmp19.to(tl.float32)
        tmp21 = tmp18 * tmp20
        tmp23 = tmp22.to(tl.float32)
        tmp24 = tmp21 + tmp23
        tmp25 = tmp24.to(tl.float32)
        tmp26 = tl.sigmoid(tmp25)
        tmp27 = tmp10 * tmp26
        tmp28 = tmp27.to(tl.float32)
        tmp29 = tl.broadcast_to(tmp28, [XBLOCK, RBLOCK])
        tmp30_mean_next, tmp30_m2_next, tmp30_weight_next = triton_helpers.welford_reduce(
            tmp29, tmp30_mean, tmp30_m2, tmp30_weight, roffset == 0
        )
        tmp30_mean = tl.where(rmask & xmask, tmp30_mean_next, tmp30_mean)
        tmp30_m2 = tl.where(rmask & xmask, tmp30_m2_next, tmp30_m2)
        tmp30_weight = tl.where(rmask & xmask, tmp30_weight_next, tmp30_weight)
        tl.store(out_ptr2 + (r1 + 3072*x0), tmp28, rmask & xmask)
    tmp30_tmp, tmp31_tmp, tmp32_tmp = triton_helpers.welford(
        tmp30_mean, tmp30_m2, tmp30_weight, 1
    )
    tmp30 = tmp30_tmp[:, None]
    tmp31 = tmp31_tmp[:, None]
    tmp32 = tmp32_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp33 = tl.load(out_ptr2 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp41 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp44 = tl.load(in_ptr5 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp34 = tmp33 - tmp30
        tmp35 = 3072.0
        tmp36 = tmp31 / tmp35
        tmp37 = 1e-05
        tmp38 = tmp36 + tmp37
        tmp39 = libdevice.rsqrt(tmp38)
        tmp40 = tmp34 * tmp39
        tmp42 = tmp41.to(tl.float32)
        tmp43 = tmp40 * tmp42
        tmp45 = tmp44.to(tl.float32)
        tmp46 = tmp43 + tmp45
        tmp47 = tmp46.to(tl.float32)
        tl.store(out_ptr5 + (r1 + 3072*x0), tmp47, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/dw/cdwyati2j4imcl5yyqxxdysnft6giug6gcy47vibrbcjnse7m33o.py
# Topologically Sorted Source Nodes: [linear_69], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_69 => mm_default_49
# Graph fragment:
#   %mm_default_49 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%convert_element_type_343, %permute_72), kwargs = {})
triton_tem_fused_addmm_191 = async_compile.triton('triton_tem_fused_addmm_191', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_191', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_191(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 1536
    K = 3072
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3072
    stride_ak = 1
    stride_bk = 1
    stride_bn = 3072

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1536*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/j5/cj5vurzkusycwhqswnnxdbrj7bz7egxvzgamridkj26e4jxu3uut.py
# Topologically Sorted Source Nodes: [linear_69, layer_norm_67, sigmoid_11, mul_1618, layer_norm_68], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_67 => add_2273, add_2274, convert_element_type_347, convert_element_type_348, mul_1446, mul_1447, rsqrt_67, sub_730, var_mean_67
#   layer_norm_68 => add_2290, add_2291, convert_element_type_349, convert_element_type_350, mul_1457, mul_1458, rsqrt_68, sub_736, var_mean_68
#   linear_69 => add_tensor_49
#   mul_1618 => mul_1454
#   sigmoid_11 => sigmoid_11
# Graph fragment:
#   %add_tensor_49 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_49, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_347 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_49, torch.float32), kwargs = {})
#   %var_mean_67 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_347, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_730 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_347, %getitem_321), kwargs = {})
#   %add_2273 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_320, 1e-05), kwargs = {})
#   %rsqrt_67 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2273,), kwargs = {})
#   %mul_1446 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_730, %rsqrt_67), kwargs = {})
#   %mul_1447 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1446, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight), kwargs = {})
#   %add_2274 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1447, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias), kwargs = {})
#   %convert_element_type_348 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2274, torch.float16), kwargs = {})
#   %sigmoid_11 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_348,), kwargs = {})
#   %mul_1454 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_49, %sigmoid_11), kwargs = {})
#   %convert_element_type_349 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%mul_1454, torch.float32), kwargs = {})
#   %var_mean_68 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_349, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_736 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_349, %getitem_323), kwargs = {})
#   %add_2290 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_322, 1e-05), kwargs = {})
#   %rsqrt_68 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2290,), kwargs = {})
#   %mul_1457 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_736, %rsqrt_68), kwargs = {})
#   %mul_1458 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1457, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w), kwargs = {})
#   %add_2291 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1458, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b), kwargs = {})
#   %convert_element_type_350 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2291, torch.float16), kwargs = {})
triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192 = async_compile.triton('triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'out_ptr2': '*fp32', 'out_ptr5': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 9, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr2, out_ptr5, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 1536
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (r1 + 1536*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    tmp30_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_ptr0 + (r1 + 1536*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp19 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp22 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tmp8 + tmp9
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp11 - tmp5
        tmp13 = 1536.0
        tmp14 = tmp6 / tmp13
        tmp15 = 1e-05
        tmp16 = tmp14 + tmp15
        tmp17 = libdevice.rsqrt(tmp16)
        tmp18 = tmp12 * tmp17
        tmp20 = tmp19.to(tl.float32)
        tmp21 = tmp18 * tmp20
        tmp23 = tmp22.to(tl.float32)
        tmp24 = tmp21 + tmp23
        tmp25 = tmp24.to(tl.float32)
        tmp26 = tl.sigmoid(tmp25)
        tmp27 = tmp10 * tmp26
        tmp28 = tmp27.to(tl.float32)
        tmp29 = tl.broadcast_to(tmp28, [XBLOCK, RBLOCK])
        tmp30_mean_next, tmp30_m2_next, tmp30_weight_next = triton_helpers.welford_reduce(
            tmp29, tmp30_mean, tmp30_m2, tmp30_weight, roffset == 0
        )
        tmp30_mean = tl.where(rmask & xmask, tmp30_mean_next, tmp30_mean)
        tmp30_m2 = tl.where(rmask & xmask, tmp30_m2_next, tmp30_m2)
        tmp30_weight = tl.where(rmask & xmask, tmp30_weight_next, tmp30_weight)
        tl.store(out_ptr2 + (r1 + 1536*x0), tmp28, rmask & xmask)
    tmp30_tmp, tmp31_tmp, tmp32_tmp = triton_helpers.welford(
        tmp30_mean, tmp30_m2, tmp30_weight, 1
    )
    tmp30 = tmp30_tmp[:, None]
    tmp31 = tmp31_tmp[:, None]
    tmp32 = tmp32_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp33 = tl.load(out_ptr2 + (r1 + 1536*x0), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp41 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp44 = tl.load(in_ptr5 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp34 = tmp33 - tmp30
        tmp35 = 1536.0
        tmp36 = tmp31 / tmp35
        tmp37 = 1e-05
        tmp38 = tmp36 + tmp37
        tmp39 = libdevice.rsqrt(tmp38)
        tmp40 = tmp34 * tmp39
        tmp42 = tmp41.to(tl.float32)
        tmp43 = tmp40 * tmp42
        tmp45 = tmp44.to(tl.float32)
        tmp46 = tmp43 + tmp45
        tmp47 = tmp46.to(tl.float32)
        tl.store(out_ptr5 + (r1 + 1536*x0), tmp47, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/qy/cqyoahw7oozalclm4zxzuf5snr7kgbevajldpci7qf4gusyagtr2.py
# Topologically Sorted Source Nodes: [linear_70, layer_norm_69, add_2100, layer_norm_70, sigmoid_12, mul_1633], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   add_2100 => add_2315
#   layer_norm_69 => add_2304, add_2305, convert_element_type_354, convert_element_type_355, mul_1465, mul_1466, rsqrt_69, sub_741, var_mean_69
#   layer_norm_70 => add_2319, add_2320, convert_element_type_356, convert_element_type_357, mul_1473, mul_1474, rsqrt_70, sub_746, var_mean_70
#   linear_70 => add_tensor_48
#   mul_1633 => mul_1481
#   sigmoid_12 => sigmoid_12
# Graph fragment:
#   %add_tensor_48 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_48, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_354 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_48, torch.float32), kwargs = {})
#   %var_mean_69 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_354, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_741 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_354, %getitem_325), kwargs = {})
#   %add_2304 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_324, 1e-05), kwargs = {})
#   %rsqrt_69 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2304,), kwargs = {})
#   %mul_1465 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_741, %rsqrt_69), kwargs = {})
#   %mul_1466 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1465, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w), kwargs = {})
#   %add_2305 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1466, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b), kwargs = {})
#   %convert_element_type_355 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2305, torch.float16), kwargs = {})
#   %add_2315 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_343, %convert_element_type_355), kwargs = {})
#   %convert_element_type_356 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2315, torch.float32), kwargs = {})
#   %var_mean_70 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_356, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_746 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_356, %getitem_327), kwargs = {})
#   %add_2319 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_326, 1e-05), kwargs = {})
#   %rsqrt_70 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2319,), kwargs = {})
#   %mul_1473 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_746, %rsqrt_70), kwargs = {})
#   %mul_1474 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1473, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight), kwargs = {})
#   %add_2320 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1474, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias), kwargs = {})
#   %convert_element_type_357 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2320, torch.float16), kwargs = {})
#   %sigmoid_12 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_357,), kwargs = {})
#   %mul_1481 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_2315, %sigmoid_12), kwargs = {})
triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193 = async_compile.triton('triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 10, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 3072
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_out_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    tmp30_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp30_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_ptr1 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_out_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp10 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp20 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp23 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tmp9 + tmp10
        tmp12 = tmp11.to(tl.float32)
        tmp13 = tmp12 - tmp5
        tmp14 = 3072.0
        tmp15 = tmp6 / tmp14
        tmp16 = 1e-05
        tmp17 = tmp15 + tmp16
        tmp18 = libdevice.rsqrt(tmp17)
        tmp19 = tmp13 * tmp18
        tmp21 = tmp20.to(tl.float32)
        tmp22 = tmp19 * tmp21
        tmp24 = tmp23.to(tl.float32)
        tmp25 = tmp22 + tmp24
        tmp26 = tmp25.to(tl.float32)
        tmp27 = tmp8 + tmp26
        tmp28 = tmp27.to(tl.float32)
        tmp29 = tl.broadcast_to(tmp28, [XBLOCK, RBLOCK])
        tmp30_mean_next, tmp30_m2_next, tmp30_weight_next = triton_helpers.welford_reduce(
            tmp29, tmp30_mean, tmp30_m2, tmp30_weight, roffset == 0
        )
        tmp30_mean = tl.where(rmask & xmask, tmp30_mean_next, tmp30_mean)
        tmp30_m2 = tl.where(rmask & xmask, tmp30_m2_next, tmp30_m2)
        tmp30_weight = tl.where(rmask & xmask, tmp30_weight_next, tmp30_weight)
        tl.store(in_out_ptr0 + (r1 + 3072*x0), tmp27, rmask & xmask)
    tmp30_tmp, tmp31_tmp, tmp32_tmp = triton_helpers.welford(
        tmp30_mean, tmp30_m2, tmp30_weight, 1
    )
    tmp30 = tmp30_tmp[:, None]
    tmp31 = tmp31_tmp[:, None]
    tmp32 = tmp32_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp33 = tl.load(in_out_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp42 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp45 = tl.load(in_ptr5 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp34 = tmp33.to(tl.float32)
        tmp35 = tmp34 - tmp30
        tmp36 = 3072.0
        tmp37 = tmp31 / tmp36
        tmp38 = 1e-05
        tmp39 = tmp37 + tmp38
        tmp40 = libdevice.rsqrt(tmp39)
        tmp41 = tmp35 * tmp40
        tmp43 = tmp42.to(tl.float32)
        tmp44 = tmp41 * tmp43
        tmp46 = tmp45.to(tl.float32)
        tmp47 = tmp44 + tmp46
        tmp48 = tmp47.to(tl.float32)
        tmp49 = tl.sigmoid(tmp48)
        tmp50 = tmp33 * tmp49
        tl.store(in_out_ptr0 + (r1 + 3072*x0), tmp50, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ze/czevodvef6tpbeslzajh42rz5b4fg3rp5erhnpje7wk2uda6waxh.py
# Topologically Sorted Source Nodes: [linear_72, layer_norm_73, add_2113, add_2138, layer_norm_74, sigmoid_14, mul_1659], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   add_2113 => add_2336
#   add_2138 => add_2385
#   layer_norm_73 => add_2374, add_2375, convert_element_type_368, convert_element_type_369, mul_1507, mul_1508, rsqrt_73, sub_765, var_mean_73
#   layer_norm_74 => add_2389, add_2390, convert_element_type_370, convert_element_type_371, mul_1515, mul_1516, rsqrt_74, sub_770, var_mean_74
#   linear_72 => add_tensor_46
#   mul_1659 => mul_1523
#   sigmoid_14 => sigmoid_14
# Graph fragment:
#   %add_tensor_46 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_46, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_368 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_46, torch.float32), kwargs = {})
#   %var_mean_73 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_368, [1]), kwargs = {correction: 0, keepdim: True})
#   %add_2336 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_343, %mul_1481), kwargs = {})
#   %sub_765 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_368, %getitem_333), kwargs = {})
#   %add_2374 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_332, 1e-05), kwargs = {})
#   %rsqrt_73 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2374,), kwargs = {})
#   %mul_1507 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_765, %rsqrt_73), kwargs = {})
#   %mul_1508 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1507, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w), kwargs = {})
#   %add_2375 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1508, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b), kwargs = {})
#   %convert_element_type_369 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2375, torch.float16), kwargs = {})
#   %add_2385 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_2336, %convert_element_type_369), kwargs = {})
#   %convert_element_type_370 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2385, torch.float32), kwargs = {})
#   %var_mean_74 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_370, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_770 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_370, %getitem_335), kwargs = {})
#   %add_2389 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_334, 1e-05), kwargs = {})
#   %rsqrt_74 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2389,), kwargs = {})
#   %mul_1515 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_770, %rsqrt_74), kwargs = {})
#   %mul_1516 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1515, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight), kwargs = {})
#   %add_2390 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1516, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias), kwargs = {})
#   %convert_element_type_371 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2390, torch.float16), kwargs = {})
#   %sigmoid_14 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_371,), kwargs = {})
#   %mul_1523 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_2385, %sigmoid_14), kwargs = {})
triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194 = async_compile.triton('triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 11, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 3072
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    tmp32_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp32_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp32_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_out_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr2 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp11 = tl.load(in_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp12 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp22 = tl.load(in_ptr3 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp25 = tl.load(in_ptr4 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tmp8 + tmp9
        tmp13 = tmp11 + tmp12
        tmp14 = tmp13.to(tl.float32)
        tmp15 = tmp14 - tmp5
        tmp16 = 3072.0
        tmp17 = tmp6 / tmp16
        tmp18 = 1e-05
        tmp19 = tmp17 + tmp18
        tmp20 = libdevice.rsqrt(tmp19)
        tmp21 = tmp15 * tmp20
        tmp23 = tmp22.to(tl.float32)
        tmp24 = tmp21 * tmp23
        tmp26 = tmp25.to(tl.float32)
        tmp27 = tmp24 + tmp26
        tmp28 = tmp27.to(tl.float32)
        tmp29 = tmp10 + tmp28
        tmp30 = tmp29.to(tl.float32)
        tmp31 = tl.broadcast_to(tmp30, [XBLOCK, RBLOCK])
        tmp32_mean_next, tmp32_m2_next, tmp32_weight_next = triton_helpers.welford_reduce(
            tmp31, tmp32_mean, tmp32_m2, tmp32_weight, roffset == 0
        )
        tmp32_mean = tl.where(rmask & xmask, tmp32_mean_next, tmp32_mean)
        tmp32_m2 = tl.where(rmask & xmask, tmp32_m2_next, tmp32_m2)
        tmp32_weight = tl.where(rmask & xmask, tmp32_weight_next, tmp32_weight)
        tl.store(in_out_ptr0 + (r1 + 3072*x0), tmp29, rmask & xmask)
    tmp32_tmp, tmp33_tmp, tmp34_tmp = triton_helpers.welford(
        tmp32_mean, tmp32_m2, tmp32_weight, 1
    )
    tmp32 = tmp32_tmp[:, None]
    tmp33 = tmp33_tmp[:, None]
    tmp34 = tmp34_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp35 = tl.load(in_out_ptr0 + (r1 + 3072*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp44 = tl.load(in_ptr5 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp47 = tl.load(in_ptr6 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp36 = tmp35.to(tl.float32)
        tmp37 = tmp36 - tmp32
        tmp38 = 3072.0
        tmp39 = tmp33 / tmp38
        tmp40 = 1e-05
        tmp41 = tmp39 + tmp40
        tmp42 = libdevice.rsqrt(tmp41)
        tmp43 = tmp37 * tmp42
        tmp45 = tmp44.to(tl.float32)
        tmp46 = tmp43 * tmp45
        tmp48 = tmp47.to(tl.float32)
        tmp49 = tmp46 + tmp48
        tmp50 = tmp49.to(tl.float32)
        tmp51 = tl.sigmoid(tmp50)
        tmp52 = tmp35 * tmp51
        tl.store(in_out_ptr0 + (r1 + 3072*x0), tmp52, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/wl/cwla3xd56emagnjh6vo5myxhsgmbdkqv7hsbktoghums2jx5dsor.py
# Topologically Sorted Source Nodes: [layer_norm_74, sigmoid_14, mul_1659, linear_73], Original ATen: [aten.native_layer_norm, aten.sigmoid, aten.mul, aten.addmm]
# Source node to ATen node mapping:
#   layer_norm_74 => add_2389, add_2390, convert_element_type_370, convert_element_type_371, mul_1515, mul_1516, rsqrt_74, sub_770, var_mean_74
#   linear_73 => addmm_63
#   mul_1659 => mul_1523
#   sigmoid_14 => sigmoid_14
# Graph fragment:
#   %convert_element_type_370 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2385, torch.float32), kwargs = {})
#   %var_mean_74 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_370, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_770 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_370, %getitem_335), kwargs = {})
#   %add_2389 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_334, 1e-05), kwargs = {})
#   %rsqrt_74 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2389,), kwargs = {})
#   %mul_1515 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_770, %rsqrt_74), kwargs = {})
#   %mul_1516 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1515, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight), kwargs = {})
#   %add_2390 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1516, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias), kwargs = {})
#   %convert_element_type_371 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2390, torch.float16), kwargs = {})
#   %sigmoid_14 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_371,), kwargs = {})
#   %mul_1523 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_2385, %sigmoid_14), kwargs = {})
#   %addmm_63 : [num_users=1] = call_function[target=torch.ops.aten.addmm.default](args = (%submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias, %mul_1523, %permute_76), kwargs = {})
triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_195 = async_compile.triton('triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_195', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_195', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_195(in_ptr0, arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 9216
    K = 3072
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3072
    stride_ak = 1
    stride_bk = 1
    stride_bn = 3072

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 9216*idx_m
    tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, acc.shape)), mask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = acc + tmp0
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), tmp1, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/h4/ch4sm2xz5p7yrolqnolvkwqcd7f5twpxsixupxooyudo7mrd4xwe.py
# Topologically Sorted Source Nodes: [cat_default_5, add_2161, layer_norm_75], Original ATen: [aten.cat, aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_2161 => add_2416
#   cat_default_5 => cat_15
#   layer_norm_75 => add_2421, add_2422, convert_element_type_375, convert_element_type_376, mul_1534, mul_1535, rsqrt_75, sub_780, var_mean_75
# Graph fragment:
#   %cat_15 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %view_61, %addmm_63], 1), kwargs = {})
#   %add_2416 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_279, %view_66), kwargs = {})
#   %convert_element_type_375 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2416, torch.float32), kwargs = {})
#   %var_mean_75 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_375, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_780 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_375, %getitem_337), kwargs = {})
#   %add_2421 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_336, 1e-05), kwargs = {})
#   %rsqrt_75 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2421,), kwargs = {})
#   %mul_1534 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_780, %rsqrt_75), kwargs = {})
#   %mul_1535 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1534, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w), kwargs = {})
#   %add_2422 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1535, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b), kwargs = {})
#   %convert_element_type_376 : [num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2422, torch.float16), kwargs = {})
triton_red_fused_add_cat_native_layer_norm_196 = async_compile.triton('triton_red_fused_add_cat_native_layer_norm_196', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[262144, 256],
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'in_ptr9': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_cat_native_layer_norm_196', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 13, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_add_cat_native_layer_norm_196(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 192
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = (xindex % 102)
    x1 = xindex // 102
    x3 = xindex
    tmp51_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp51_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp51_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp47 = tl.load(in_ptr6 + (13824 + r2 + 192*x0 + 33408*x1), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp0 = r2 + 192*x0
        tmp1 = tl.full([1, 1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1, 1], 192, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (87936*x1 + (r2 + 192*x0)), rmask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp6 = tmp0 >= tmp3
        tmp7 = tl.full([1, 1], 384, tl.int64)
        tmp8 = tmp0 < tmp7
        tmp9 = tmp6 & tmp8
        tmp10 = tl.load(in_ptr1 + (87936*x1 + ((-192) + r2 + 192*x0)), rmask & tmp9 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp11 = tmp0 >= tmp7
        tmp12 = tl.full([1, 1], 576, tl.int64)
        tmp13 = tmp0 < tmp12
        tmp14 = tmp11 & tmp13
        tmp15 = tl.load(in_ptr2 + (87936*x1 + ((-384) + r2 + 192*x0)), rmask & tmp14 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp16 = tmp0 >= tmp12
        tmp17 = tl.full([1, 1], 768, tl.int64)
        tmp18 = tmp0 < tmp17
        tmp19 = tmp16 & tmp18
        tmp20 = tl.load(in_ptr3 + (87936*x1 + ((-576) + r2 + 192*x0)), rmask & tmp19 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp21 = tmp0 >= tmp17
        tmp22 = tl.full([1, 1], 960, tl.int64)
        tmp23 = tmp0 < tmp22
        tmp24 = tmp21 & tmp23
        tmp25 = tl.load(in_ptr4 + (87936*x1 + ((-768) + r2 + 192*x0)), rmask & tmp24 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp26 = tmp0 >= tmp22
        tmp27 = tl.full([1, 1], 1152, tl.int64)
        tmp28 = tmp0 < tmp27
        tmp29 = tmp26 & tmp28
        tmp30 = tl.load(in_ptr5 + (87936*x1 + ((-960) + r2 + 192*x0)), rmask & tmp29 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp31 = tmp0 >= tmp27
        tmp32 = tl.full([1, 1], 10368, tl.int64)
        tmp33 = tmp0 < tmp32
        tmp34 = tmp31 & tmp33
        tmp35 = tl.load(in_ptr6 + (33408*x1 + ((((-1152) + r2 + 192*x0) % 9216))), rmask & tmp34 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp36 = tmp0 >= tmp32
        tmp37 = tl.full([1, 1], 19584, tl.int64)
        tmp38 = tmp0 < tmp37
        tmp39 = tl.load(in_ptr7 + (9216*x1 + ((-10368) + r2 + 192*x0)), rmask & tmp36 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp40 = tl.where(tmp34, tmp35, tmp39)
        tmp41 = tl.where(tmp29, tmp30, tmp40)
        tmp42 = tl.where(tmp24, tmp25, tmp41)
        tmp43 = tl.where(tmp19, tmp20, tmp42)
        tmp44 = tl.where(tmp14, tmp15, tmp43)
        tmp45 = tl.where(tmp9, tmp10, tmp44)
        tmp46 = tl.where(tmp4, tmp5, tmp45)
        tmp48 = tmp47 + tmp46
        tmp49 = tmp48.to(tl.float32)
        tmp50 = tl.broadcast_to(tmp49, [XBLOCK, RBLOCK])
        tmp51_mean_next, tmp51_m2_next, tmp51_weight_next = triton_helpers.welford_reduce(
            tmp50, tmp51_mean, tmp51_m2, tmp51_weight, roffset == 0
        )
        tmp51_mean = tl.where(rmask & xmask, tmp51_mean_next, tmp51_mean)
        tmp51_m2 = tl.where(rmask & xmask, tmp51_m2_next, tmp51_m2)
        tmp51_weight = tl.where(rmask & xmask, tmp51_weight_next, tmp51_weight)
        tl.store(in_out_ptr0 + (r2 + 192*x3), tmp46, rmask & xmask)
    tmp51_tmp, tmp52_tmp, tmp53_tmp = triton_helpers.welford(
        tmp51_mean, tmp51_m2, tmp51_weight, 1
    )
    tmp51 = tmp51_tmp[:, None]
    tmp52 = tmp52_tmp[:, None]
    tmp53 = tmp53_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp54 = tl.load(in_ptr6 + (13824 + r2 + 192*x0 + 33408*x1), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp55 = tl.load(in_out_ptr0 + (r2 + 192*x3), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp65 = tl.load(in_ptr8 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp68 = tl.load(in_ptr9 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp56 = tmp54 + tmp55
        tmp57 = tmp56.to(tl.float32)
        tmp58 = tmp57 - tmp51
        tmp59 = 192.0
        tmp60 = tmp52 / tmp59
        tmp61 = 1e-05
        tmp62 = tmp60 + tmp61
        tmp63 = libdevice.rsqrt(tmp62)
        tmp64 = tmp58 * tmp63
        tmp66 = tmp65.to(tl.float32)
        tmp67 = tmp64 * tmp66
        tmp69 = tmp68.to(tl.float32)
        tmp70 = tmp67 + tmp69
        tmp71 = tmp70.to(tl.float32)
        tl.store(in_out_ptr0 + (r2 + 192*x3), tmp71, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/lr/clr7il6cdpoecfd6kyb3rgakt5ulqxyu522h4v7nw4juj7or4swg.py
# Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.bmm]
# Source node to ATen node mapping:
#   matmul_1 => constant_pad_nd_default_14
# Graph fragment:
#   %constant_pad_nd_default_14 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%view_67, [0, 2, 0, 0, 0, 0]), kwargs = {})
triton_poi_fused_bmm_197 = async_compile.triton('triton_poi_fused_bmm_197', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[16777216],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_bmm_197', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_bmm_197(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = (xindex % 104)
    x1 = ((xindex // 104) % 72)
    x3 = xindex
    tmp0 = x0
    tmp1 = tl.full([1], 102, tl.int64)
    tmp2 = tmp0 < tmp1
    tmp3 = tl.load(in_ptr0 + (x0 + 102*x1), tmp2 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tl.store(out_ptr0 + (x3), tmp3, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/i7/ci7ydjoenp3abh3ictcnji7jpkvfd4xyta55spmiqsu33nx4vi4q.py
# Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.bmm]
# Source node to ATen node mapping:
#   matmul_1 => constant_pad_nd_default_15
# Graph fragment:
#   %constant_pad_nd_default_15 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%view_68, [0, 0, 0, 2, 0, 0]), kwargs = {})
triton_poi_fused_bmm_198 = async_compile.triton('triton_poi_fused_bmm_198', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[67108864],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'out_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_bmm_198', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_bmm_198(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x1 = ((xindex // 192) % 104)
    x2 = xindex // 19968
    x3 = (xindex % 19968)
    x4 = xindex
    tmp0 = x1
    tmp1 = tl.full([1], 102, tl.int64)
    tmp2 = tmp0 < tmp1
    tmp3 = tl.load(in_ptr0 + (x3 + 19584*x2), tmp2 & xmask, other=0.0).to(tl.float32)
    tl.store(out_ptr0 + (x4), tmp3, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/hv/chvnttarlaykhadvf5olbwo33tghg34kynzguz32y6gvw6w3vrjq.py
# Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.bmm]
# Source node to ATen node mapping:
#   matmul_1 => bmm_default_1, constant_pad_nd_default_14, constant_pad_nd_default_15
# Graph fragment:
#   %constant_pad_nd_default_14 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%view_67, [0, 2, 0, 0, 0, 0]), kwargs = {})
#   %constant_pad_nd_default_15 : [num_users=1] = call_function[target=torch.ops.aten.constant_pad_nd.default](args = (%view_68, [0, 0, 0, 2, 0, 0]), kwargs = {})
#   %bmm_default_1 : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%constant_pad_nd_default_14, %constant_pad_nd_default_15), kwargs = {})
triton_tem_fused_bmm_199 = async_compile.triton('triton_tem_fused_bmm_199', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_bmm_199', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_bmm_199(arg_A, arg_B, out_ptr0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = 72
    N = 192
    K = 104

    stride_aq = 7488
    stride_am = 104
    stride_ak = 1

    stride_bq = 19968
    stride_bk = 192
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m + 13824*idx_q
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/wm/cwmnpp6k7rnspxapresfzziop57necnlnnlkxudx3oxhbfht5mjo.py
# Topologically Sorted Source Nodes: [add_2174, layer_norm_76], Original ATen: [aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_2174 => add_2451
#   layer_norm_76 => add_2460, add_2461, convert_element_type_379, convert_element_type_380, mul_1565, mul_1566, rsqrt_76, sub_792, var_mean_76
# Graph fragment:
#   %add_2451 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_69, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b), kwargs = {})
#   %convert_element_type_379 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2451, torch.float32), kwargs = {})
#   %var_mean_76 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_379, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_792 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_379, %getitem_339), kwargs = {})
#   %add_2460 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_338, 1e-05), kwargs = {})
#   %rsqrt_76 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2460,), kwargs = {})
#   %mul_1565 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_792, %rsqrt_76), kwargs = {})
#   %mul_1566 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1565, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w), kwargs = {})
#   %add_2461 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1566, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b), kwargs = {})
#   %convert_element_type_380 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2461, torch.float16), kwargs = {})
triton_per_fused_add_native_layer_norm_200 = async_compile.triton('triton_per_fused_add_native_layer_norm_200', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[262144, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_layer_norm_200', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_add_native_layer_norm_200(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 192
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = rindex < rnumel
    r2 = rindex
    x3 = xindex
    x0 = (xindex % 72)
    tmp0 = tl.load(in_out_ptr0 + (r2 + 192*x3), rmask & xmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp27 = tl.load(in_ptr1 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr2 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
    tmp6 = tl.where(rmask & xmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
    tmp9 = tl.where(rmask & xmask, tmp7, 0)
    tmp10 = tl.sum(tmp9, 1)[:, None]
    tmp11 = tl.full([XBLOCK, 1], 192, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
    tmp18 = tl.where(rmask & xmask, tmp16, 0)
    tmp19 = tl.sum(tmp18, 1)[:, None]
    tmp20 = tmp3 - tmp13
    tmp21 = 192.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tl.store(in_out_ptr0 + (r2 + 192*x3), tmp33, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ce/ccedgliobpxceyigzq725wua3tnkpr5o3jekzq3u3z2s5uprq66b.py
# Topologically Sorted Source Nodes: [linear_default_4], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_default_4 => mm_default_45
# Graph fragment:
#   %mm_default_45 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_71, %permute_78), kwargs = {})
triton_tem_fused_addmm_201 = async_compile.triton('triton_tem_fused_addmm_201', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_201', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_201(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A + 9216
    B = arg_B

    M = ks0
    N = 1536
    K = 4608
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 13824
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4608

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1536*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/nm/cnmkequ3hampm7zgql75l7ikqfrsxs4nqtktjenfsj7uir6ptu6j.py
# Topologically Sorted Source Nodes: [linear_78, layer_norm_85], Original ATen: [aten.addmm, aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_85 => add_2633, add_2634, convert_element_type_412, convert_element_type_413, mul_1673, mul_1674, rsqrt_85, sub_851, var_mean_85
#   linear_78 => add_tensor_42
# Graph fragment:
#   %add_tensor_42 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_42, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias), kwargs = {})
#   %convert_element_type_412 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_42, torch.float32), kwargs = {})
#   %var_mean_85 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_412, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_851 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_412, %getitem_359), kwargs = {})
#   %add_2633 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_358, 1e-05), kwargs = {})
#   %rsqrt_85 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2633,), kwargs = {})
#   %mul_1673 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_851, %rsqrt_85), kwargs = {})
#   %mul_1674 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1673, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w), kwargs = {})
#   %add_2634 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1674, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b), kwargs = {})
#   %convert_element_type_413 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2634, torch.float16), kwargs = {})
triton_red_fused_addmm_native_layer_norm_202 = async_compile.triton('triton_red_fused_addmm_native_layer_norm_202', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_addmm_native_layer_norm_202', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 6, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_addmm_native_layer_norm_202(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 4896
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp5_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp5_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_out_ptr0 + (r1 + 4896*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = tmp2.to(tl.float32)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp5_mean_next, tmp5_m2_next, tmp5_weight_next = triton_helpers.welford_reduce(
            tmp4, tmp5_mean, tmp5_m2, tmp5_weight, roffset == 0
        )
        tmp5_mean = tl.where(rmask & xmask, tmp5_mean_next, tmp5_mean)
        tmp5_m2 = tl.where(rmask & xmask, tmp5_m2_next, tmp5_m2)
        tmp5_weight = tl.where(rmask & xmask, tmp5_weight_next, tmp5_weight)
    tmp5_tmp, tmp6_tmp, tmp7_tmp = triton_helpers.welford(
        tmp5_mean, tmp5_m2, tmp5_weight, 1
    )
    tmp5 = tmp5_tmp[:, None]
    tmp6 = tmp6_tmp[:, None]
    tmp7 = tmp7_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp8 = tl.load(in_out_ptr0 + (r1 + 4896*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp9 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp19 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp22 = tl.load(in_ptr2 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp10 = tmp8 + tmp9
        tmp11 = tmp10.to(tl.float32)
        tmp12 = tmp11 - tmp5
        tmp13 = 4896.0
        tmp14 = tmp6 / tmp13
        tmp15 = 1e-05
        tmp16 = tmp14 + tmp15
        tmp17 = libdevice.rsqrt(tmp16)
        tmp18 = tmp12 * tmp17
        tmp20 = tmp19.to(tl.float32)
        tmp21 = tmp18 * tmp20
        tmp23 = tmp22.to(tl.float32)
        tmp24 = tmp21 + tmp23
        tmp25 = tmp24.to(tl.float32)
        tl.store(in_out_ptr0 + (r1 + 4896*x0), tmp25, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ex/cex65qqmnzxd4mujthpvldncqps6vn5isl3w56sskstdch44unk4.py
# Topologically Sorted Source Nodes: [bmm_2], Original ATen: [aten.bmm]
# Source node to ATen node mapping:
#   bmm_2 => bmm_4
# Graph fragment:
#   %bmm_4 : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%permute_77, %view_72), kwargs = {})
triton_tem_fused_bmm_203 = async_compile.triton('triton_tem_fused_bmm_203', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_bmm_203', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_bmm_203(arg_A, arg_B, out_ptr0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = 192
    N = 48
    K = 102

    stride_aq = 19584
    stride_am = 1
    stride_ak = 192

    stride_bq = 4896
    stride_bk = 48
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 48*idx_m + 9216*idx_q
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ft/cft3wdvx2unknqge7llxjy53337ctrn2d4e56coigpg3v2empvud.py
# Topologically Sorted Source Nodes: [layer_norm_87, bmm_3], Original ATen: [aten.native_layer_norm, aten.bmm]
# Source node to ATen node mapping:
#   bmm_3 => bmm_5
#   layer_norm_87 => add_2672, add_2673, convert_element_type_419, mul_1693, mul_1694, rsqrt_87, sub_863, var_mean_87
# Graph fragment:
#   %var_mean_87 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_418, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_863 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_418, %getitem_363), kwargs = {})
#   %add_2672 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_362, 1e-05), kwargs = {})
#   %rsqrt_87 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2672,), kwargs = {})
#   %mul_1693 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_863, %rsqrt_87), kwargs = {})
#   %mul_1694 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1693, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w), kwargs = {})
#   %add_2673 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1694, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b), kwargs = {})
#   %convert_element_type_419 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2673, torch.float16), kwargs = {})
#   %bmm_5 : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%convert_element_type_376, %convert_element_type_419), kwargs = {})
triton_tem_fused_bmm_native_layer_norm_204 = async_compile.triton('triton_tem_fused_bmm_native_layer_norm_204', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_bmm_native_layer_norm_204', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_bmm_native_layer_norm_204(arg_A, arg_B, out_ptr0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 64
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = 102
    N = 48
    K = 192

    stride_aq = 19584
    stride_am = 192
    stride_ak = 1

    stride_bq = 9216
    stride_bk = 48
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 48*idx_m + 4896*idx_q
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta17 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/da/cdaql77j6kw73ncbxwiafm7f7mpzp3hy7yiktsyrcoyog2h3bgfm.py
# Topologically Sorted Source Nodes: [layer_norm_88], Original ATen: [aten.native_layer_norm]
# Source node to ATen node mapping:
#   layer_norm_88 => add_2693, add_2694, convert_element_type_422, convert_element_type_423, mul_1705, mul_1706, rsqrt_88, sub_869, var_mean_88
# Graph fragment:
#   %convert_element_type_422 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%view_74, torch.float32), kwargs = {})
#   %var_mean_88 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_422, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_869 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_422, %getitem_365), kwargs = {})
#   %add_2693 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_364, 1e-05), kwargs = {})
#   %rsqrt_88 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2693,), kwargs = {})
#   %mul_1705 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_869, %rsqrt_88), kwargs = {})
#   %mul_1706 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1705, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w), kwargs = {})
#   %add_2694 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1706, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b), kwargs = {})
#   %convert_element_type_423 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2694, torch.float16), kwargs = {})
triton_red_fused_native_layer_norm_205 = async_compile.triton('triton_red_fused_native_layer_norm_205', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.reduction(
    size_hints=[2048, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_layer_norm_205', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 2, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_red_fused_native_layer_norm_205(in_out_ptr0, in_ptr0, in_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    rnumel = 4896
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_out_ptr0 + (r1 + 4896*x0), rmask & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])
        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(
            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0
        )
        tmp3_mean = tl.where(rmask & xmask, tmp3_mean_next, tmp3_mean)
        tmp3_m2 = tl.where(rmask & xmask, tmp3_m2_next, tmp3_m2)
        tmp3_weight = tl.where(rmask & xmask, tmp3_weight_next, tmp3_weight)
    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(
        tmp3_mean, tmp3_m2, tmp3_weight, 1
    )
    tmp3 = tmp3_tmp[:, None]
    tmp4 = tmp4_tmp[:, None]
    tmp5 = tmp5_tmp[:, None]
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp6 = tl.load(in_out_ptr0 + (r1 + 4896*x0), rmask & xmask, eviction_policy='evict_first', other=0.0).to(tl.float32)
        tmp15 = tl.load(in_ptr0 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp18 = tl.load(in_ptr1 + (r1), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
        tmp7 = tmp6.to(tl.float32)
        tmp8 = tmp7 - tmp3
        tmp9 = 4896.0
        tmp10 = tmp4 / tmp9
        tmp11 = 1e-05
        tmp12 = tmp10 + tmp11
        tmp13 = libdevice.rsqrt(tmp12)
        tmp14 = tmp8 * tmp13
        tmp16 = tmp15.to(tl.float32)
        tmp17 = tmp14 * tmp16
        tmp19 = tmp18.to(tl.float32)
        tmp20 = tmp17 + tmp19
        tmp21 = tmp20.to(tl.float32)
        tl.store(in_out_ptr0 + (r1 + 4896*x0), tmp21, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/h3/ch3ywjud46ibt6iid7jjvhtwp674sqp6zzbfd3wofnq6jgfjbima.py
# Topologically Sorted Source Nodes: [cat_default_3, add_2440, layer_norm_105], Original ATen: [aten.cat, aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_2440 => add_2969
#   cat_default_3 => cat_19
#   layer_norm_105 => add_2974, add_2975, convert_element_type_487, convert_element_type_488, mul_1873, mul_1874, rsqrt_105, sub_966, var_mean_105
# Graph fragment:
#   %cat_19 : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%clone_2, %clone_3, %clone_4, %clone_5, %clone_6, %clone_7, %view_70, %addmm_77], 1), kwargs = {})
#   %add_2969 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_75, %convert_element_type_376), kwargs = {})
#   %convert_element_type_487 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2969, torch.float32), kwargs = {})
#   %var_mean_105 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_487, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_966 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_487, %getitem_399), kwargs = {})
#   %add_2974 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_398, 1e-05), kwargs = {})
#   %rsqrt_105 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_2974,), kwargs = {})
#   %mul_1873 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_966, %rsqrt_105), kwargs = {})
#   %mul_1874 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_1873, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w), kwargs = {})
#   %add_2975 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_1874, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b), kwargs = {})
#   %convert_element_type_488 : [num_users=4] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_2975, torch.float16), kwargs = {})
triton_per_fused_add_cat_native_layer_norm_206 = async_compile.triton('triton_per_fused_add_cat_native_layer_norm_206', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[262144, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'in_ptr7': '*fp16', 'in_ptr8': '*fp16', 'in_ptr9': '*fp16', 'in_ptr10': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_cat_native_layer_norm_206', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 11, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_add_cat_native_layer_norm_206(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 192
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = rindex < rnumel
    r2 = rindex
    x0 = (xindex % 102)
    x1 = xindex // 102
    x3 = xindex
    tmp47 = tl.load(in_ptr8 + (r2 + 192*x3), rmask & xmask, other=0.0).to(tl.float32)
    tmp73 = tl.load(in_ptr9 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp76 = tl.load(in_ptr10 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp0 = r2 + 192*x0
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 192, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = tl.load(in_ptr0 + (87936*x1 + (r2 + 192*x0)), rmask & tmp4 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp6 = tmp0 >= tmp3
    tmp7 = tl.full([1, 1], 384, tl.int64)
    tmp8 = tmp0 < tmp7
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (87936*x1 + ((-192) + r2 + 192*x0)), rmask & tmp9 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp11 = tmp0 >= tmp7
    tmp12 = tl.full([1, 1], 576, tl.int64)
    tmp13 = tmp0 < tmp12
    tmp14 = tmp11 & tmp13
    tmp15 = tl.load(in_ptr2 + (87936*x1 + ((-384) + r2 + 192*x0)), rmask & tmp14 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp16 = tmp0 >= tmp12
    tmp17 = tl.full([1, 1], 768, tl.int64)
    tmp18 = tmp0 < tmp17
    tmp19 = tmp16 & tmp18
    tmp20 = tl.load(in_ptr3 + (87936*x1 + ((-576) + r2 + 192*x0)), rmask & tmp19 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp21 = tmp0 >= tmp17
    tmp22 = tl.full([1, 1], 960, tl.int64)
    tmp23 = tmp0 < tmp22
    tmp24 = tmp21 & tmp23
    tmp25 = tl.load(in_ptr4 + (87936*x1 + ((-768) + r2 + 192*x0)), rmask & tmp24 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp26 = tmp0 >= tmp22
    tmp27 = tl.full([1, 1], 1152, tl.int64)
    tmp28 = tmp0 < tmp27
    tmp29 = tmp26 & tmp28
    tmp30 = tl.load(in_ptr5 + (87936*x1 + ((-960) + r2 + 192*x0)), rmask & tmp29 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp31 = tmp0 >= tmp27
    tmp32 = tl.full([1, 1], 10368, tl.int64)
    tmp33 = tmp0 < tmp32
    tmp34 = tmp31 & tmp33
    tmp35 = tl.load(in_ptr6 + (13824*x1 + ((((-1152) + r2 + 192*x0) % 9216))), rmask & tmp34 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp36 = tmp0 >= tmp32
    tmp37 = tl.full([1, 1], 19584, tl.int64)
    tmp38 = tmp0 < tmp37
    tmp39 = tl.load(in_ptr7 + (9216*x1 + ((-10368) + r2 + 192*x0)), rmask & tmp36 & xmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp40 = tl.where(tmp34, tmp35, tmp39)
    tmp41 = tl.where(tmp29, tmp30, tmp40)
    tmp42 = tl.where(tmp24, tmp25, tmp41)
    tmp43 = tl.where(tmp19, tmp20, tmp42)
    tmp44 = tl.where(tmp14, tmp15, tmp43)
    tmp45 = tl.where(tmp9, tmp10, tmp44)
    tmp46 = tl.where(tmp4, tmp5, tmp45)
    tmp48 = tmp46 + tmp47
    tmp49 = tmp48.to(tl.float32)
    tmp50 = tl.broadcast_to(tmp49, [XBLOCK, RBLOCK])
    tmp52 = tl.where(rmask & xmask, tmp50, 0)
    tmp53 = tl.broadcast_to(tmp50, [XBLOCK, RBLOCK])
    tmp55 = tl.where(rmask & xmask, tmp53, 0)
    tmp56 = tl.sum(tmp55, 1)[:, None]
    tmp57 = tl.full([XBLOCK, 1], 192, tl.int32)
    tmp58 = tmp57.to(tl.float32)
    tmp59 = tmp56 / tmp58
    tmp60 = tmp50 - tmp59
    tmp61 = tmp60 * tmp60
    tmp62 = tl.broadcast_to(tmp61, [XBLOCK, RBLOCK])
    tmp64 = tl.where(rmask & xmask, tmp62, 0)
    tmp65 = tl.sum(tmp64, 1)[:, None]
    tmp66 = tmp49 - tmp59
    tmp67 = 192.0
    tmp68 = tmp65 / tmp67
    tmp69 = 1e-05
    tmp70 = tmp68 + tmp69
    tmp71 = libdevice.rsqrt(tmp70)
    tmp72 = tmp66 * tmp71
    tmp74 = tmp73.to(tl.float32)
    tmp75 = tmp72 * tmp74
    tmp77 = tmp76.to(tl.float32)
    tmp78 = tmp75 + tmp77
    tmp79 = tmp78.to(tl.float32)
    tl.store(in_out_ptr0 + (r2 + 192*x3), tmp79, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/7d/c7d4qmulgha3dzywiodg5dzmmhhpwriyde5vbtriw6rvn66ktg2y.py
# Topologically Sorted Source Nodes: [matmul_3], Original ATen: [aten.bmm]
# Source node to ATen node mapping:
#   matmul_3 => bmm_9
# Graph fragment:
#   %bmm_9 : [num_users=1] = call_function[target=torch.ops.aten.bmm.default](args = (%view_85, %view_86), kwargs = {})
triton_tem_fused_bmm_207 = async_compile.triton('triton_tem_fused_bmm_207', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_bmm_207', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_bmm_207(arg_A, arg_B, out_ptr0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = False
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 32
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = 24
    N = 192
    K = 102

    stride_aq = 0
    stride_am = 102
    stride_ak = 1

    stride_bq = 19584
    stride_bk = 192
    stride_bn = 1

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N

    rk = tl.arange(0, BLOCK_K)

    idx_q = tl.program_id(1)  # batch dimension for BMM
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak + idx_q*stride_aq)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn + idx_q*stride_bq)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_q = tl.program_id(1)  # batch dimension for BMM
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 192*idx_m + 4608*idx_q
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta18 = {'GROUP_M': 8, 'EVEN_K': False, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 32, 'BLOCK_N': 128, 'BLOCK_K': 64}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/jc/cjc5ujktphgm6fw4s6kzv6v7tybpuptyjtww2ihwiee2fuotxmh3.py
# Topologically Sorted Source Nodes: [add_2732, layer_norm_136], Original ATen: [aten.add, aten.native_layer_norm]
# Source node to ATen node mapping:
#   add_2732 => add_3557
#   layer_norm_136 => add_3566, add_3567, convert_element_type_603, convert_element_type_604, mul_2243, mul_2244, rsqrt_136, sub_1164, var_mean_136
# Graph fragment:
#   %add_3557 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%view_87, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b), kwargs = {})
#   %convert_element_type_603 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_3557, torch.float32), kwargs = {})
#   %var_mean_136 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_603, [2]), kwargs = {correction: 0, keepdim: True})
#   %sub_1164 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_603, %getitem_463), kwargs = {})
#   %add_3566 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_462, 1e-05), kwargs = {})
#   %rsqrt_136 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_3566,), kwargs = {})
#   %mul_2243 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_1164, %rsqrt_136), kwargs = {})
#   %mul_2244 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_2243, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w), kwargs = {})
#   %add_3567 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_2244, %submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b), kwargs = {})
#   %convert_element_type_604 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_3567, torch.float16), kwargs = {})
triton_per_fused_add_native_layer_norm_208 = async_compile.triton('triton_per_fused_add_native_layer_norm_208', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[65536, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_layer_norm_208', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_add_native_layer_norm_208(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr):
    rnumel = 192
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    roffset = 0
    rmask = rindex < rnumel
    r2 = rindex
    x3 = xindex
    x0 = (xindex % 24)
    tmp0 = tl.load(in_out_ptr0 + (r2 + 192*x3), rmask & xmask, other=0.0).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp27 = tl.load(in_ptr1 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp30 = tl.load(in_ptr2 + (r2), rmask, eviction_policy='evict_last', other=0.0).to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
    tmp6 = tl.where(rmask & xmask, tmp4, 0)
    tmp7 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
    tmp9 = tl.where(rmask & xmask, tmp7, 0)
    tmp10 = tl.sum(tmp9, 1)[:, None]
    tmp11 = tl.full([XBLOCK, 1], 192, tl.int32)
    tmp12 = tmp11.to(tl.float32)
    tmp13 = tmp10 / tmp12
    tmp14 = tmp4 - tmp13
    tmp15 = tmp14 * tmp14
    tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
    tmp18 = tl.where(rmask & xmask, tmp16, 0)
    tmp19 = tl.sum(tmp18, 1)[:, None]
    tmp20 = tmp3 - tmp13
    tmp21 = 192.0
    tmp22 = tmp19 / tmp21
    tmp23 = 1e-05
    tmp24 = tmp22 + tmp23
    tmp25 = libdevice.rsqrt(tmp24)
    tmp26 = tmp20 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 * tmp28
    tmp31 = tmp30.to(tl.float32)
    tmp32 = tmp29 + tmp31
    tmp33 = tmp32.to(tl.float32)
    tl.store(in_out_ptr0 + (r2 + 192*x3), tmp33, rmask & xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/w7/cw7y6ltdzgifqbhglufwwggb7qgc3arq7bdpmw3qi2exnjajbfcu.py
# Topologically Sorted Source Nodes: [linear_default_6], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_default_6 => mm_default_19
# Graph fragment:
#   %mm_default_19 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%view_88, %permute_108), kwargs = {})
triton_tem_fused_addmm_209 = async_compile.triton('triton_tem_fused_addmm_209', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_209', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_209(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 64
    A = arg_A
    B = arg_B

    M = ks0
    N = 1536
    K = 4608
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 4608
    stride_ak = 1
    stride_bk = 1
    stride_bn = 4608

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 1536*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/g2/cg2pt2rjgw4tp6najntzjgxhhz5yq7j4eb6xea6lznezosqc5y64.py
# Topologically Sorted Source Nodes: [linear_118], Original ATen: [aten.addmm]
# Source node to ATen node mapping:
#   linear_118 => mm_default_6
# Graph fragment:
#   %mm_default_6 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%mul_2534, %permute_121), kwargs = {})
triton_tem_fused_addmm_210 = async_compile.triton('triton_tem_fused_addmm_210', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_210', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_210(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 512
    K = 3072
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 3072
    stride_ak = 1
    stride_bk = 1
    stride_bn = 3072

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 512*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')
meta19 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 128}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/2d/c2do44tewzigsf6cctorxgts7tt3am2h6dkbffzujzrjbkfndpza.py
# Topologically Sorted Source Nodes: [linear_118, relu_default_1, nan_to_num_default_1], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num]
# Source node to ATen node mapping:
#   linear_118 => add_tensor_6
#   nan_to_num_default_1 => convert_element_type_711, convert_element_type_712, eq_1266, eq_1267, isnan_2, scalar_tensor_6, scalar_tensor_7, scalar_tensor_8, where_6, where_7, where_8
#   relu_default_1 => relu_1
# Graph fragment:
#   %add_tensor_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_6, %submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b), kwargs = {})
#   %relu_1 : [num_users=4] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor_6,), kwargs = {})
#   %convert_element_type_712 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_1, torch.float32), kwargs = {})
#   %eq_1267 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_712, inf), kwargs = {})
#   %scalar_tensor_8 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_711 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_1, torch.float32), kwargs = {})
#   %eq_1266 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_711, -inf), kwargs = {})
#   %scalar_tensor_7 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan_2 : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%relu_1,), kwargs = {})
#   %scalar_tensor_6 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where_6 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan_2, %scalar_tensor_6, %relu_1), kwargs = {})
#   %where_7 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1266, %scalar_tensor_7, %where_6), kwargs = {})
#   %where_8 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1267, %scalar_tensor_8, %where_7), kwargs = {})
triton_poi_fused_addmm_nan_to_num_relu_211 = async_compile.triton('triton_poi_fused_addmm_nan_to_num_relu_211', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[1048576],
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_nan_to_num_relu_211', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_addmm_nan_to_num_relu_211(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 512)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tl.full([1], 0, tl.int32)
    tmp4 = triton_helpers.maximum(tmp3, tmp2)
    tmp5 = tmp4.to(tl.float32)
    tmp6 = float("inf")
    tmp7 = tmp5 == tmp6
    tmp8 = float("-inf")
    tmp9 = tmp5 == tmp8
    tmp10 = libdevice.isnan(tmp4).to(tl.int1)
    tmp11 = 0.0
    tmp12 = tl.where(tmp10, tmp11, tmp4)
    tmp13 = -65504.0
    tmp14 = tl.where(tmp9, tmp13, tmp12)
    tmp15 = 65504.0
    tmp16 = tl.where(tmp7, tmp15, tmp14)
    tl.store(in_out_ptr0 + (x2), tmp16, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/oa/coahag7xkhqxxgr2g6g4dewlvwdk56jrgt45un3b762alqm356xa.py
# Topologically Sorted Source Nodes: [linear_118, relu_default_1, nan_to_num_default_1, linear_119], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num]
# Source node to ATen node mapping:
#   linear_118 => add_tensor_6
#   linear_119 => mm_default_5
#   nan_to_num_default_1 => convert_element_type_711, convert_element_type_712, eq_1266, eq_1267, isnan_2, scalar_tensor_6, scalar_tensor_7, scalar_tensor_8, where_6, where_7, where_8
#   relu_default_1 => relu_1
# Graph fragment:
#   %add_tensor_6 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_6, %submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b), kwargs = {})
#   %relu_1 : [num_users=4] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor_6,), kwargs = {})
#   %convert_element_type_712 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_1, torch.float32), kwargs = {})
#   %eq_1267 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_712, inf), kwargs = {})
#   %scalar_tensor_8 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_711 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_1, torch.float32), kwargs = {})
#   %eq_1266 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_711, -inf), kwargs = {})
#   %scalar_tensor_7 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan_2 : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%relu_1,), kwargs = {})
#   %scalar_tensor_6 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where_6 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan_2, %scalar_tensor_6, %relu_1), kwargs = {})
#   %where_7 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1266, %scalar_tensor_7, %where_6), kwargs = {})
#   %where_8 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1267, %scalar_tensor_8, %where_7), kwargs = {})
#   %mm_default_5 : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%where_8, %permute_122), kwargs = {})
triton_tem_fused_addmm_nan_to_num_relu_212 = async_compile.triton('triton_tem_fused_addmm_nan_to_num_relu_212', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=3,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_nan_to_num_relu_212', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_nan_to_num_relu_212(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 128
    BLOCK_N : tl.constexpr = 128
    BLOCK_K : tl.constexpr = 32
    A = arg_A
    B = arg_B

    M = ks0
    N = 3072
    K = 512
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 512
    stride_ak = 1
    stride_bk = 1
    stride_bn = 512

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_n + 3072*idx_m
    tl.store(out_ptr0 + (tl.broadcast_to(xindex, acc.shape)), acc, mask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/53/c53y4ccnl5daw6h6ao6eldjn3o3r73mvxgydnhxz4jd64vov33sk.py
# Topologically Sorted Source Nodes: [linear_119, relu_default_2, nan_to_num_default_2, add_2989], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num, aten.add]
# Source node to ATen node mapping:
#   add_2989 => add_4072
#   linear_119 => add_tensor_5
#   nan_to_num_default_2 => convert_element_type_716, convert_element_type_717, eq_1271, eq_1272, isnan_3, scalar_tensor_10, scalar_tensor_11, scalar_tensor_9, where_10, where_11, where_9
#   relu_default_2 => relu_2
# Graph fragment:
#   %add_tensor_5 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_5, %submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b), kwargs = {})
#   %relu_2 : [num_users=4] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor_5,), kwargs = {})
#   %convert_element_type_717 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_2, torch.float32), kwargs = {})
#   %eq_1272 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_717, inf), kwargs = {})
#   %scalar_tensor_11 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_716 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_2, torch.float32), kwargs = {})
#   %eq_1271 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_716, -inf), kwargs = {})
#   %scalar_tensor_10 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan_3 : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%relu_2,), kwargs = {})
#   %scalar_tensor_9 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where_9 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan_3, %scalar_tensor_9, %relu_2), kwargs = {})
#   %where_10 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1271, %scalar_tensor_10, %where_9), kwargs = {})
#   %where_11 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1272, %scalar_tensor_11, %where_10), kwargs = {})
#   %add_4072 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_2534, %where_11), kwargs = {})
triton_poi_fused_add_addmm_nan_to_num_relu_213 = async_compile.triton('triton_poi_fused_add_addmm_nan_to_num_relu_213', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_addmm_nan_to_num_relu_213', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_addmm_nan_to_num_relu_213(in_out_ptr0, in_ptr0, in_ptr1, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 3072)
    tmp0 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tmp1 + tmp2
    tmp4 = tl.full([1], 0, tl.int32)
    tmp5 = triton_helpers.maximum(tmp4, tmp3)
    tmp6 = tmp5.to(tl.float32)
    tmp7 = float("inf")
    tmp8 = tmp6 == tmp7
    tmp9 = float("-inf")
    tmp10 = tmp6 == tmp9
    tmp11 = libdevice.isnan(tmp5).to(tl.int1)
    tmp12 = 0.0
    tmp13 = tl.where(tmp11, tmp12, tmp5)
    tmp14 = -65504.0
    tmp15 = tl.where(tmp10, tmp14, tmp13)
    tmp16 = 65504.0
    tmp17 = tl.where(tmp8, tmp16, tmp15)
    tmp18 = tmp0 + tmp17
    tl.store(in_out_ptr0 + (x2), tmp18, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/ip/cipgkvayk3y4rjxb2tlbehpksxjvx72zsmbsjcspwxpmnzltzpjr.py
# Topologically Sorted Source Nodes: [full_like, mul_2264, add_3008, linear_121, relu_default_4, nan_to_num_default_4, mul_2273, add_3018], Original ATen: [aten.full_like, aten.mul, aten.add, aten.addmm, aten.relu, aten.nan_to_num]
# Source node to ATen node mapping:
#   add_3008 => add_4094
#   add_3018 => add_4107
#   full_like => full
#   linear_121 => add_tensor_3
#   mul_2264 => mul_2559
#   mul_2273 => mul_2570
#   nan_to_num_default_4 => convert_element_type_726, convert_element_type_727, eq_1285, eq_1286, isnan_5, scalar_tensor_15, scalar_tensor_16, scalar_tensor_17, where_15, where_16, where_17
#   relu_default_4 => relu_4
# Graph fragment:
#   %full : [num_users=2] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_9, 3072], 0.0), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %mul_2559 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%full, %add_4072), kwargs = {})
#   %add_4094 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_2534, %mul_2559), kwargs = {})
#   %add_tensor_3 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_3, %submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b), kwargs = {})
#   %relu_4 : [num_users=4] = call_function[target=torch.ops.aten.relu.default](args = (%add_tensor_3,), kwargs = {})
#   %convert_element_type_727 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_4, torch.float32), kwargs = {})
#   %eq_1286 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_727, inf), kwargs = {})
#   %scalar_tensor_17 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %convert_element_type_726 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%relu_4, torch.float32), kwargs = {})
#   %eq_1285 : [num_users=1] = call_function[target=torch.ops.aten.eq.Scalar](args = (%convert_element_type_726, -inf), kwargs = {})
#   %scalar_tensor_16 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (-65504.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %isnan_5 : [num_users=1] = call_function[target=torch.ops.aten.isnan.default](args = (%relu_4,), kwargs = {})
#   %scalar_tensor_15 : [num_users=1] = call_function[target=torch.ops.aten.scalar_tensor.default](args = (0.0,), kwargs = {dtype: torch.float16, layout: torch.strided, device: cuda:0})
#   %where_15 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%isnan_5, %scalar_tensor_15, %relu_4), kwargs = {})
#   %where_16 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1285, %scalar_tensor_16, %where_15), kwargs = {})
#   %where_17 : [num_users=1] = call_function[target=torch.ops.aten.where.self](args = (%eq_1286, %scalar_tensor_17, %where_16), kwargs = {})
#   %mul_2570 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%where_17, %full), kwargs = {})
#   %add_4107 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_4094, %mul_2570), kwargs = {})
triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214 = async_compile.triton('triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 4, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 3072)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp5 = tl.load(in_ptr1 + (x2), xmask).to(tl.float32)
    tmp6 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp2 = 0.0
    tmp3 = tmp2 * tmp1
    tmp4 = tmp0 + tmp3
    tmp7 = tmp5 + tmp6
    tmp8 = tl.full([1], 0, tl.int32)
    tmp9 = triton_helpers.maximum(tmp8, tmp7)
    tmp10 = tmp9.to(tl.float32)
    tmp11 = float("inf")
    tmp12 = tmp10 == tmp11
    tmp13 = float("-inf")
    tmp14 = tmp10 == tmp13
    tmp15 = libdevice.isnan(tmp9).to(tl.int1)
    tmp16 = tl.where(tmp15, tmp2, tmp9)
    tmp17 = -65504.0
    tmp18 = tl.where(tmp14, tmp17, tmp16)
    tmp19 = 65504.0
    tmp20 = tl.where(tmp12, tmp19, tmp18)
    tmp21 = tmp20 * tmp2
    tmp22 = tmp4 + tmp21
    tl.store(in_out_ptr0 + (x2), tmp22, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/fj/cfjod3prartkmy32lzpmd5vlmng5i6g35s5itfik4pyxlthpg47q.py
# Topologically Sorted Source Nodes: [linear_122, sigmoid_45, mul_2282], Original ATen: [aten.addmm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   linear_122 => add_tensor_2
#   mul_2282 => mul_2579
#   sigmoid_45 => sigmoid_45
# Graph fragment:
#   %add_tensor_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_2, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b), kwargs = {})
#   %sigmoid_45 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_tensor_2,), kwargs = {})
#   %mul_2579 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_4107, %sigmoid_45), kwargs = {})
triton_poi_fused_addmm_mul_sigmoid_215 = async_compile.triton('triton_poi_fused_addmm_mul_sigmoid_215', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[8388608],
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_addmm_mul_sigmoid_215', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 3, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_addmm_mul_sigmoid_215(in_out_ptr0, in_ptr0, in_ptr1, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = (xindex % 3072)
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (x2), xmask).to(tl.float32)
    tmp2 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tmp1 + tmp2
    tmp4 = tl.sigmoid(tmp3)
    tmp5 = tmp0 * tmp4
    tl.store(in_out_ptr0 + (x2), tmp5, xmask)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/hr/chrm2xkghwdsshbmcuj33c4ebof6tdow47qwjn6bkf3r4wewcgol.py
# Topologically Sorted Source Nodes: [linear_123, layer_norm_165, sigmoid_46, mul_2291], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_165 => add_4123, add_4124, convert_element_type_734, convert_element_type_735, mul_2584, mul_2585, rsqrt_165, sub_1353, var_mean_165
#   linear_123 => add_tensor_1
#   mul_2291 => mul_2592
#   sigmoid_46 => sigmoid_46
# Graph fragment:
#   %add_tensor_1 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_1, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b), kwargs = {})
#   %convert_element_type_734 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_1, torch.float32), kwargs = {})
#   %var_mean_165 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_734, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_1353 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_734, %getitem_521), kwargs = {})
#   %add_4123 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_520, 1e-05), kwargs = {})
#   %rsqrt_165 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_4123,), kwargs = {})
#   %mul_2584 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_1353, %rsqrt_165), kwargs = {})
#   %mul_2585 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_2584, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale), kwargs = {})
#   %add_4124 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_2585, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_735 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_4124, torch.float16), kwargs = {})
#   %sigmoid_46 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_735,), kwargs = {})
#   %mul_2592 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_1, %sigmoid_46), kwargs = {})
triton_per_fused_addmm_mul_native_layer_norm_sigmoid_216 = async_compile.triton('triton_per_fused_addmm_mul_native_layer_norm_sigmoid_216', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.persistent_reduction(
    size_hints=[2048, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {'in_out_ptr0': '*fp16', 'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'xnumel': 'i32', 'rnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_addmm_mul_native_layer_norm_sigmoid_216', 'mutated_arg_names': ['in_out_ptr0'], 'optimize_mem': True, 'no_x_dim': True, 'num_load': 4, 'num_reduction': 4, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True}
)
@triton.jit
def triton_per_fused_addmm_mul_native_layer_norm_sigmoid_216(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, xnumel, rnumel):
    XBLOCK: tl.constexpr = 1
    rnumel = 512
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = tl.full([RBLOCK], True, tl.int1)
    rindex = tl.arange(0, RBLOCK)[:]
    roffset = 0
    rmask = tl.full([RBLOCK], True, tl.int1)
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (r1 + 512*x0), None).to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp24 = tl.load(in_ptr1 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp27 = tl.load(in_ptr2 + (r1), None, eviction_policy='evict_last').to(tl.float32)
    tmp2 = tmp0 + tmp1
    tmp3 = tmp2.to(tl.float32)
    tmp4 = tl.broadcast_to(tmp3, [RBLOCK])
    tmp6 = tl.broadcast_to(tmp4, [RBLOCK])
    tmp8 = triton_helpers.promote_to_tensor(tl.sum(tmp6, 0))
    tmp9 = tl.full([1], 512, tl.int32)
    tmp10 = tmp9.to(tl.float32)
    tmp11 = tmp8 / tmp10
    tmp12 = tmp4 - tmp11
    tmp13 = tmp12 * tmp12
    tmp14 = tl.broadcast_to(tmp13, [RBLOCK])
    tmp16 = triton_helpers.promote_to_tensor(tl.sum(tmp14, 0))
    tmp17 = tmp3 - tmp11
    tmp18 = 512.0
    tmp19 = tmp16 / tmp18
    tmp20 = 1e-05
    tmp21 = tmp19 + tmp20
    tmp22 = libdevice.rsqrt(tmp21)
    tmp23 = tmp17 * tmp22
    tmp25 = tmp24.to(tl.float32)
    tmp26 = tmp23 * tmp25
    tmp28 = tmp27.to(tl.float32)
    tmp29 = tmp26 + tmp28
    tmp30 = tmp29.to(tl.float32)
    tmp31 = tl.sigmoid(tmp30)
    tmp32 = tmp2 * tmp31
    tl.store(in_out_ptr0 + (r1 + 512*x0), tmp32, None)
''', device_str='cuda')


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/wj/cwjl3gfadasi7itvssukgcjmbp7peimtk4q67gditvgdcf2t3nzi.py
# Topologically Sorted Source Nodes: [linear_123, layer_norm_165, sigmoid_46, mul_2291, linear_124], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
# Source node to ATen node mapping:
#   layer_norm_165 => add_4123, add_4124, convert_element_type_734, convert_element_type_735, mul_2584, mul_2585, rsqrt_165, sub_1353, var_mean_165
#   linear_123 => add_tensor_1
#   linear_124 => mm_default
#   mul_2291 => mul_2592
#   sigmoid_46 => sigmoid_46
# Graph fragment:
#   %add_tensor_1 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default_1, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b), kwargs = {})
#   %convert_element_type_734 : [num_users=2] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_tensor_1, torch.float32), kwargs = {})
#   %var_mean_165 : [num_users=2] = call_function[target=torch.ops.aten.var_mean.correction](args = (%convert_element_type_734, [1]), kwargs = {correction: 0, keepdim: True})
#   %sub_1353 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%convert_element_type_734, %getitem_521), kwargs = {})
#   %add_4123 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%getitem_520, 1e-05), kwargs = {})
#   %rsqrt_165 : [num_users=1] = call_function[target=torch.ops.aten.rsqrt.default](args = (%add_4123,), kwargs = {})
#   %mul_2584 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sub_1353, %rsqrt_165), kwargs = {})
#   %mul_2585 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%mul_2584, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale), kwargs = {})
#   %add_4124 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_2585, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias), kwargs = {})
#   %convert_element_type_735 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%add_4124, torch.float16), kwargs = {})
#   %sigmoid_46 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%convert_element_type_735,), kwargs = {})
#   %mul_2592 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add_tensor_1, %sigmoid_46), kwargs = {})
#   %mm_default : [num_users=1] = call_function[target=torch.ops.aten.mm.default](args = (%mul_2592, %permute_127), kwargs = {})
triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_217 = async_compile.triton('triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_217', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties

@triton_heuristics.template(
    num_stages=4,
    num_warps=4,
    triton_meta={'signature': {'arg_A': '*fp16', 'arg_B': '*fp16', 'out_ptr0': '*fp16', 'ks0': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
    inductor_meta={'kernel_name': 'triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_217', 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
)
@triton.jit
def triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_217(arg_A, arg_B, out_ptr0, ks0):
    GROUP_M : tl.constexpr = 8
    EVEN_K : tl.constexpr = True
    ALLOW_TF32 : tl.constexpr = False
    ACC_TYPE : tl.constexpr = tl.float32
    B_PROLOGUE_CAST_TYPE : tl.constexpr = None
    BLOCK_M : tl.constexpr = 64
    BLOCK_N : tl.constexpr = 16
    BLOCK_K : tl.constexpr = 128
    A = arg_A
    B = arg_B

    M = ks0
    N = 1
    K = 512
    if M * N == 0:
        # early exit due to zero-size input(s)
        return
    stride_am = 512
    stride_ak = 1
    stride_bk = 1
    stride_bn = 512

    # based on triton.ops.matmul
    pid = tl.program_id(0)
    grid_m = (M + BLOCK_M - 1) // BLOCK_M
    grid_n = (N + BLOCK_N - 1) // BLOCK_N

    # re-order program ID for better L2 performance
    width = GROUP_M * grid_n
    group_id = pid // width
    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
    pid_m = group_id * GROUP_M + (pid % group_size)
    pid_n = (pid % width) // (group_size)

    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
    else:
        ram = rm % M
    if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
        rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
    else:
        rbn = rn % N
    rk = tl.arange(0, BLOCK_K)
    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
    for k in range(K, 0, -BLOCK_K):
        if EVEN_K:
            a = tl.load(A)
            b = tl.load(B)
        else:
            a = tl.load(A, mask=rk[None, :] < k, other=0.)
            b = tl.load(B, mask=rk[:, None] < k, other=0.)
        if B_PROLOGUE_CAST_TYPE is not None:
            b = b.to(B_PROLOGUE_CAST_TYPE)
        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
        A += BLOCK_K * stride_ak
        B += BLOCK_K * stride_bk

    # rematerialize rm and rn to save registers
    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    idx_m = rm[:, None]
    idx_n = rn[None, :]
    mask = (idx_m < M) & (idx_n < N)

    # inductor generates a suffix
    xindex = idx_m + idx_n
    tl.store(out_ptr0 + (tl.broadcast_to(idx_m, acc.shape)), acc, mask)
''', device_str='cuda')
meta20 = {'GROUP_M': 8, 'EVEN_K': True, 'ALLOW_TF32': False, 'ACC_TYPE': 'tl.float32', 'B_PROLOGUE_CAST_TYPE': None, 'BLOCK_M': 64, 'BLOCK_N': 16, 'BLOCK_K': 128}


# kernel path: /tmp/torchinductor_guorachel/tmpyhywxbq9/3q/c3qfulgrs6omdjdr23prkvrz2xnn4elxil5xyymr7lzhogbzm64z.py
# Topologically Sorted Source Nodes: [submod_1, linear_124, sum_3, sum_5, add_605, sum_10, add_708, sum_11, add_712, sum_9, add_722, sum_7, add_735, sum_6, add_742, sum_4, add_749, sum_8, add_762, sum_12, add_772, sum_2, add_789, sum_1, add_804, add_808], Original ATen: [aten.full_like, aten.addmm, aten.add, aten.sigmoid, aten.gt, aten._to_copy, aten.mul, aten.sub, aten.logit, aten.sum]
# Source node to ATen node mapping:
#   add_605 => add_631
#   add_708 => add_990
#   add_712 => add_994
#   add_722 => add_1012
#   add_735 => add_1040
#   add_742 => add_1050
#   add_749 => add_1057
#   add_762 => add_1078
#   add_772 => add_1088
#   add_789 => add_1105
#   add_804 => add_1140
#   add_808 => add_1144
#   linear_124 => add_tensor
#   submod_1 => add_4143, add_4156, add_4187, clamp_max_1, clamp_min_1, convert_element_type_739, convert_element_type_740, convert_element_type_741, div, full_1, full_2, gt, log, mul_2605, mul_2608, mul_2610, sigmoid_47, sigmoid_48, sub_1363, sub_1372
#   sum_1 => sum_1
#   sum_10 => sum_10
#   sum_11 => sum_11
#   sum_12 => sum_12
#   sum_2 => sum_2
#   sum_3 => sum_3
#   sum_4 => sum_4
#   sum_5 => sum_5
#   sum_6 => sum_6
#   sum_7 => sum_7
#   sum_8 => sum_8
#   sum_9 => sum_9
# Graph fragment:
#   %full_2 : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_9, 1], 1.0), kwargs = {dtype: torch.float32, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %add_tensor : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mm_default, %submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b), kwargs = {})
#   %add_4143 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_tensor, %submod_1_main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_positive_weight_calibration_bias), kwargs = {})
#   %sigmoid_47 : [num_users=3] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_4143,), kwargs = {})
#   %full_1 : [num_users=1] = call_function[target=torch.ops.aten.full.default](args = ([%sym_size_int_9, 1], 0.0), kwargs = {dtype: torch.float32, layout: torch.strided, device: cuda:0, pin_memory: False})
#   %gt : [num_users=1] = call_function[target=torch.ops.aten.gt.Tensor](args = (%sigmoid_47, %full_1), kwargs = {})
#   %convert_element_type_741 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%gt, torch.float32), kwargs = {})
#   %mul_2605 : [num_users=2] = call_function[target=torch.ops.aten.mul.Tensor](args = (%full_2, %convert_element_type_741), kwargs = {})
#   %sub_1372 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%submod_1__tensor_constant1, %mul_2605), kwargs = {})
#   %mul_2610 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_47, %sub_1372), kwargs = {})
#   %convert_element_type_739 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%sigmoid_47, torch.float32), kwargs = {})
#   %clamp_min_1 : [num_users=1] = call_function[target=torch.ops.aten.clamp_min.default](args = (%convert_element_type_739, 1e-06), kwargs = {})
#   %clamp_max_1 : [num_users=2] = call_function[target=torch.ops.aten.clamp_max.default](args = (%clamp_min_1, 0.999999), kwargs = {})
#   %sub_1363 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (1, %clamp_max_1), kwargs = {})
#   %div : [num_users=1] = call_function[target=torch.ops.aten.div.Tensor](args = (%clamp_max_1, %sub_1363), kwargs = {})
#   %log : [num_users=1] = call_function[target=torch.ops.aten.log.default](args = (%div,), kwargs = {})
#   %convert_element_type_740 : [num_users=1] = call_function[target=torch.ops.prims.convert_element_type.default](args = (%log, torch.float16), kwargs = {})
#   %sum_3 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_10, [1], True), kwargs = {})
#   %sum_5 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_12, [1], True), kwargs = {})
#   %add_631 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%sum_3, %sum_5), kwargs = {})
#   %sum_10 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_17, [1], True), kwargs = {})
#   %add_990 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_631, %sum_10), kwargs = {})
#   %sum_11 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_18, [1], True), kwargs = {})
#   %add_994 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_990, %sum_11), kwargs = {})
#   %sum_9 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_16, [1], True), kwargs = {})
#   %add_1012 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_994, %sum_9), kwargs = {})
#   %sum_7 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_14, [1], True), kwargs = {})
#   %add_1040 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1012, %sum_7), kwargs = {})
#   %sum_6 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_13, [1], True), kwargs = {})
#   %add_1050 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1040, %sum_6), kwargs = {})
#   %sum_4 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_11, [1], True), kwargs = {})
#   %add_1057 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1050, %sum_4), kwargs = {})
#   %sum_8 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_15, [1], True), kwargs = {})
#   %add_1078 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1057, %sum_8), kwargs = {})
#   %sum_12 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_60, [1], True), kwargs = {})
#   %add_1088 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1078, %sum_12), kwargs = {})
#   %sum_2 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_9, [1], True), kwargs = {})
#   %add_1105 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1088, %sum_2), kwargs = {})
#   %sum_1 : [num_users=1] = call_function[target=torch.ops.aten.sum.dim_IntList](args = (%getitem_8, [1], True), kwargs = {})
#   %add_1140 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1105, %sum_1), kwargs = {})
#   %add_1144 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_1140, %submod_0_main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias), kwargs = {})
#   %add_4156 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%convert_element_type_740, %add_1144), kwargs = {})
#   %sigmoid_48 : [num_users=1] = call_function[target=torch.ops.aten.sigmoid.default](args = (%add_4156,), kwargs = {})
#   %mul_2608 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sigmoid_48, %mul_2605), kwargs = {})
#   %add_4187 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul_2610, %mul_2608), kwargs = {})
triton_poi_fused__to_copy_add_addmm_full_like_gt_logit_mul_sigmoid_sub_sum_218 = async_compile.triton('triton_poi_fused__to_copy_add_addmm_full_like_gt_logit_mul_sigmoid_sub_sum_218', '''
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor.runtime import triton_helpers, triton_heuristics
from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
triton_helpers.set_driver_to_gpu()

@triton_heuristics.pointwise(
    size_hints=[2048],
    filename=__file__,
    triton_meta={'signature': {'in_ptr0': '*fp16', 'in_ptr1': '*fp16', 'in_ptr2': '*fp16', 'in_ptr3': '*fp16', 'in_ptr4': '*fp16', 'in_ptr5': '*fp16', 'in_ptr6': '*fp16', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132, warp_size=32), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused__to_copy_add_addmm_full_like_gt_logit_mul_sigmoid_sub_sum_218', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'num_load': 53, 'num_reduction': 0, 'backend_hash': 'DF75C1AF293916715AA8CF69E2B06BA77907F904A716773E01682ABA87E684ED', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': True, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': True, 'is_fbcode': True},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused__to_copy_add_addmm_full_like_gt_logit_mul_sigmoid_sub_sum_218(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (536 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp1 = tl.load(in_ptr0 + (537 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp3 = tl.load(in_ptr0 + (538 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp5 = tl.load(in_ptr0 + (539 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp7 = tl.load(in_ptr0 + (544 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp8 = tl.load(in_ptr0 + (545 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp10 = tl.load(in_ptr0 + (546 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp12 = tl.load(in_ptr0 + (547 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp15 = tl.load(in_ptr0 + (564 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp16 = tl.load(in_ptr0 + (565 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp18 = tl.load(in_ptr0 + (566 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp20 = tl.load(in_ptr0 + (567 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp23 = tl.load(in_ptr0 + (568 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp24 = tl.load(in_ptr0 + (569 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp26 = tl.load(in_ptr0 + (570 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp28 = tl.load(in_ptr0 + (571 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp31 = tl.load(in_ptr0 + (560 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp32 = tl.load(in_ptr0 + (561 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp34 = tl.load(in_ptr0 + (562 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp36 = tl.load(in_ptr0 + (563 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp39 = tl.load(in_ptr0 + (552 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp40 = tl.load(in_ptr0 + (553 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp42 = tl.load(in_ptr0 + (554 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp44 = tl.load(in_ptr0 + (555 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp47 = tl.load(in_ptr0 + (548 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp48 = tl.load(in_ptr0 + (549 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp50 = tl.load(in_ptr0 + (550 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp52 = tl.load(in_ptr0 + (551 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp55 = tl.load(in_ptr0 + (540 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp56 = tl.load(in_ptr0 + (541 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp58 = tl.load(in_ptr0 + (542 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp60 = tl.load(in_ptr0 + (543 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp63 = tl.load(in_ptr0 + (556 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp64 = tl.load(in_ptr0 + (557 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp66 = tl.load(in_ptr0 + (558 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp68 = tl.load(in_ptr0 + (559 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp71 = tl.load(in_ptr1 + (172 + 30516*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp72 = tl.load(in_ptr1 + (173 + 30516*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp74 = tl.load(in_ptr1 + (174 + 30516*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp76 = tl.load(in_ptr1 + (175 + 30516*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp79 = tl.load(in_ptr0 + (532 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp80 = tl.load(in_ptr0 + (533 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp82 = tl.load(in_ptr0 + (534 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp84 = tl.load(in_ptr0 + (535 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp87 = tl.load(in_ptr2 + (x0), xmask).to(tl.float32)
    tmp88 = tl.load(in_ptr3 + (0)).to(tl.float32)
    tmp89 = tl.broadcast_to(tmp88, [XBLOCK])
    tmp91 = tl.load(in_ptr4 + (0)).to(tl.float32)
    tmp92 = tl.broadcast_to(tmp91, [XBLOCK])
    tmp105 = tl.load(in_ptr0 + (528 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp106 = tl.load(in_ptr0 + (529 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp108 = tl.load(in_ptr0 + (530 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp110 = tl.load(in_ptr0 + (531 + 36284*x0), xmask, eviction_policy='evict_last').to(tl.float32)
    tmp113 = tl.load(in_ptr5 + (0)).to(tl.float32)
    tmp114 = tl.broadcast_to(tmp113, [XBLOCK])
    tmp117 = tl.load(in_ptr6 + (0)).to(tl.float32)
    tmp118 = tl.broadcast_to(tmp117, [XBLOCK])
    tmp2 = tmp0 + tmp1
    tmp4 = tmp2 + tmp3
    tmp6 = tmp4 + tmp5
    tmp9 = tmp7 + tmp8
    tmp11 = tmp9 + tmp10
    tmp13 = tmp11 + tmp12
    tmp14 = tmp6 + tmp13
    tmp17 = tmp15 + tmp16
    tmp19 = tmp17 + tmp18
    tmp21 = tmp19 + tmp20
    tmp22 = tmp14 + tmp21
    tmp25 = tmp23 + tmp24
    tmp27 = tmp25 + tmp26
    tmp29 = tmp27 + tmp28
    tmp30 = tmp22 + tmp29
    tmp33 = tmp31 + tmp32
    tmp35 = tmp33 + tmp34
    tmp37 = tmp35 + tmp36
    tmp38 = tmp30 + tmp37
    tmp41 = tmp39 + tmp40
    tmp43 = tmp41 + tmp42
    tmp45 = tmp43 + tmp44
    tmp46 = tmp38 + tmp45
    tmp49 = tmp47 + tmp48
    tmp51 = tmp49 + tmp50
    tmp53 = tmp51 + tmp52
    tmp54 = tmp46 + tmp53
    tmp57 = tmp55 + tmp56
    tmp59 = tmp57 + tmp58
    tmp61 = tmp59 + tmp60
    tmp62 = tmp54 + tmp61
    tmp65 = tmp63 + tmp64
    tmp67 = tmp65 + tmp66
    tmp69 = tmp67 + tmp68
    tmp70 = tmp62 + tmp69
    tmp73 = tmp71 + tmp72
    tmp75 = tmp73 + tmp74
    tmp77 = tmp75 + tmp76
    tmp78 = tmp70 + tmp77
    tmp81 = tmp79 + tmp80
    tmp83 = tmp81 + tmp82
    tmp85 = tmp83 + tmp84
    tmp86 = tmp78 + tmp85
    tmp90 = tmp87 + tmp89
    tmp93 = tmp90 + tmp92
    tmp94 = tl.sigmoid(tmp93)
    tmp95 = tmp94.to(tl.float32)
    tmp96 = 1e-06
    tmp97 = triton_helpers.maximum(tmp95, tmp96)
    tmp98 = 0.999999
    tmp99 = triton_helpers.minimum(tmp97, tmp98)
    tmp100 = 1.0
    tmp101 = tmp100 - tmp99
    tmp102 = tmp99 / tmp101
    tmp103 = tl_math.log(tmp102)
    tmp104 = tmp103.to(tl.float32)
    tmp107 = tmp105 + tmp106
    tmp109 = tmp107 + tmp108
    tmp111 = tmp109 + tmp110
    tmp112 = tmp86 + tmp111
    tmp115 = tmp112 + tmp114
    tmp116 = tmp104 + tmp115
    tmp119 = tmp118.to(tl.float32)
    tmp120 = 0.0
    tmp121 = tmp95 > tmp120
    tmp122 = tmp121.to(tl.float32)
    tmp123 = tmp100 * tmp122
    tmp124 = tmp119 - tmp123
    tmp125 = tmp95 * tmp124
    tmp126 = tl.sigmoid(tmp116)
    tmp127 = tmp126.to(tl.float32)
    tmp128 = tmp127 * tmp123
    tmp129 = tmp125 + tmp128
    tl.store(out_ptr0 + (x0), tmp129, xmask)
''', device_str='cuda')


async_compile.wait(globals())
del async_compile

def call(args):
    arg606_1, arg607_1, arg608_1, arg609_1, arg610_1, arg611_1, arg612_1, arg613_1, arg614_1 = args
    args.clear()
    arg606_1_size = arg606_1.size()
    s0 = arg606_1_size[0]
    assert_size_stride(arg606_1, (s0, 3890), (3890, 1))
    assert_size_stride(arg607_1, (s0, 2688), (2688, 1))
    assert_size_stride(arg608_1, (s0, 384), (384, 1))
    assert_size_stride(arg609_1, (s0, 384), (384, 1))
    assert_size_stride(arg610_1, (s0, 17932), (17932, 1))
    assert_size_stride(arg611_1, (s0, 17584), (17584, 1))
    assert_size_stride(arg612_1, (s0, 30516), (30516, 1))
    assert_size_stride(arg613_1, (s0, 200, 64), (12800, 64, 1))
    assert_size_stride(arg614_1, (s0, 200, 64), (12800, 64, 1))

    for kernel in globals().values():
        if isinstance(kernel, torch._inductor.runtime.triton_heuristics.CachingAutotuner):
            kernel.cuda_kernel_saved = False
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        buf0 = empty_strided_cuda((s0, 36284), (36284, 1), torch.float16)
        # Topologically Sorted Source Nodes: [cat_default_12, permute_pooled_embs_auto_grad], Original ATen: [aten.cat, fbgemm.permute_pooled_embs_auto_grad]
        triton_poi_fused_cat_permute_pooled_embs_auto_grad_11_xnumel = 36284*s0
        stream0 = get_raw_stream(0)
        triton_poi_fused_cat_permute_pooled_embs_auto_grad_11.run(arg608_1, arg609_1, arg610_1, arg611_1, buf0, triton_poi_fused_cat_permute_pooled_embs_auto_grad_11_xnumel, grid=grid(triton_poi_fused_cat_permute_pooled_embs_auto_grad_11_xnumel), stream=stream0)
        del arg608_1
        del arg609_1
        del arg610_1
        del arg611_1
        # Topologically Sorted Source Nodes: [cat_default_12, permute_pooled_embs_auto_grad], Original ATen: [aten.cat, fbgemm.permute_pooled_embs_auto_grad]
        buf1 = torch.ops.fbgemm.permute_pooled_embs_auto_grad.default(buf0, _FOLDED_CONST_submod_0_cat_fusion_gpu__offset_dim_list, _FOLDED_CONST_submod_0_cat_fusion_gpu__permute, _FOLDED_CONST_submod_0_cat_fusion_gpu__inv_offset_dim_list, _FOLDED_CONST_submod_0_cat_fusion_gpu__inv_permute)
        del buf0
        buf2 = buf1
        del buf1
        # Topologically Sorted Source Nodes: [permute_pooled_embs_auto_grad_1], Original ATen: [fbgemm.permute_pooled_embs_auto_grad]
        buf3 = torch.ops.fbgemm.permute_pooled_embs_auto_grad.default(arg612_1, _FOLDED_CONST_submod_0_cat_fusion_cpu__offset_dim_list, _FOLDED_CONST_submod_0_cat_fusion_cpu__permute, _FOLDED_CONST_submod_0_cat_fusion_cpu__inv_offset_dim_list, _FOLDED_CONST_submod_0_cat_fusion_cpu__inv_permute)
        del arg612_1
        buf4 = buf3
        del buf3
        buf5 = empty_strided_cuda((s0, 192), (192, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_12.run(arg606_1, _FOLDED_CONST_permute, buf5, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf388 = empty_strided_cuda((s0, 87936), (87936, 1), torch.float16)
        buf353 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 69120)  # alias
        # Topologically Sorted Source Nodes: [linear, layer_norm], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf5, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias, buf353, s0, 192, grid=grid(s0), stream=stream0)
        buf9 = buf5; del buf5  # reuse
        # Topologically Sorted Source Nodes: [linear_1], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_14.run(arg606_1, _FOLDED_CONST_permute_1, buf9, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf357 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 69888)  # alias
        # Topologically Sorted Source Nodes: [linear_1, layer_norm_1], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf9, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias, buf357, s0, 192, grid=grid(s0), stream=stream0)
        buf13 = buf9; del buf9  # reuse
        # Topologically Sorted Source Nodes: [linear_2], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_15.run(arg606_1, _FOLDED_CONST_permute_2, buf13, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta1), stream=stream0)
        buf361 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 70656)  # alias
        # Topologically Sorted Source Nodes: [linear_2, layer_norm_2], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf13, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias, buf361, s0, 192, grid=grid(s0), stream=stream0)
        buf17 = buf13; del buf13  # reuse
        # Topologically Sorted Source Nodes: [linear_3], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_16.run(arg606_1, _FOLDED_CONST_permute_3, buf17, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta1), stream=stream0)
        buf362 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 70848)  # alias
        # Topologically Sorted Source Nodes: [linear_3, layer_norm_3], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf17, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias, buf362, s0, 192, grid=grid(s0), stream=stream0)
        buf21 = buf17; del buf17  # reuse
        # Topologically Sorted Source Nodes: [linear_4], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_17.run(arg607_1, _FOLDED_CONST_permute_4, buf21, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta2), stream=stream0)
        buf351 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 68736)  # alias
        # Topologically Sorted Source Nodes: [linear_4, layer_norm_4], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf21, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias, buf351, s0, 192, grid=grid(s0), stream=stream0)
        buf25 = buf21; del buf21  # reuse
        # Topologically Sorted Source Nodes: [linear_5], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_18.run(arg607_1, _FOLDED_CONST_permute_5, buf25, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta3), stream=stream0)
        buf352 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 68928)  # alias
        # Topologically Sorted Source Nodes: [linear_5, layer_norm_5], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf25, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias, buf352, s0, 192, grid=grid(s0), stream=stream0)
        buf29 = buf25; del buf25  # reuse
        # Topologically Sorted Source Nodes: [linear_6], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_19.run(arg607_1, _FOLDED_CONST_permute_6, buf29, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf354 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 69312)  # alias
        # Topologically Sorted Source Nodes: [linear_6, layer_norm_6], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf29, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias, buf354, s0, 192, grid=grid(s0), stream=stream0)
        buf33 = buf29; del buf29  # reuse
        # Topologically Sorted Source Nodes: [linear_7], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_20.run(arg607_1, _FOLDED_CONST_permute_7, buf33, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta4), stream=stream0)
        buf355 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 69504)  # alias
        # Topologically Sorted Source Nodes: [linear_7, layer_norm_7], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf33, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias, buf355, s0, 192, grid=grid(s0), stream=stream0)
        buf37 = buf33; del buf33  # reuse
        # Topologically Sorted Source Nodes: [linear_8], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_21.run(arg607_1, _FOLDED_CONST_permute_8, buf37, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf356 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 69696)  # alias
        # Topologically Sorted Source Nodes: [linear_8, layer_norm_8], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf37, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias, buf356, s0, 192, grid=grid(s0), stream=stream0)
        buf41 = buf37; del buf37  # reuse
        # Topologically Sorted Source Nodes: [linear_9], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_22.run(arg607_1, _FOLDED_CONST_permute_9, buf41, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf358 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 70080)  # alias
        # Topologically Sorted Source Nodes: [linear_9, layer_norm_9], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf41, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias, buf358, s0, 192, grid=grid(s0), stream=stream0)
        buf45 = buf41; del buf41  # reuse
        # Topologically Sorted Source Nodes: [linear_10], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_23.run(arg607_1, _FOLDED_CONST_permute_10, buf45, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf359 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 70272)  # alias
        # Topologically Sorted Source Nodes: [linear_10, layer_norm_10], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf45, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias, buf359, s0, 192, grid=grid(s0), stream=stream0)
        buf49 = buf45; del buf45  # reuse
        # Topologically Sorted Source Nodes: [linear_11], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_24.run(arg607_1, _FOLDED_CONST_permute_11, buf49, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf360 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 70464)  # alias
        # Topologically Sorted Source Nodes: [linear_11, layer_norm_11], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf49, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias, buf360, s0, 192, grid=grid(s0), stream=stream0)
        buf53 = buf49; del buf49  # reuse
        # Topologically Sorted Source Nodes: [linear_12], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_25.run(arg607_1, _FOLDED_CONST_permute_12, buf53, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta5), stream=stream0)
        buf363 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 71040)  # alias
        # Topologically Sorted Source Nodes: [linear_12, layer_norm_12], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf53, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias, buf363, s0, 192, grid=grid(s0), stream=stream0)
        buf57 = buf53; del buf53  # reuse
        # Topologically Sorted Source Nodes: [linear_13], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_26.run(arg607_1, _FOLDED_CONST_permute_13, buf57, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta5), stream=stream0)
        buf364 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 71232)  # alias
        # Topologically Sorted Source Nodes: [linear_13, layer_norm_13], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf57, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias, buf364, s0, 192, grid=grid(s0), stream=stream0)
        buf61 = buf57; del buf57  # reuse
        # Topologically Sorted Source Nodes: [linear_14], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_27.run(arg607_1, _FOLDED_CONST_permute_14, buf61, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf365 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 71424)  # alias
        # Topologically Sorted Source Nodes: [linear_14, layer_norm_14], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf61, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias, buf365, s0, 192, grid=grid(s0), stream=stream0)
        buf65 = buf61; del buf61  # reuse
        # Topologically Sorted Source Nodes: [linear_15], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_28.run(arg607_1, _FOLDED_CONST_permute_15, buf65, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta2), stream=stream0)
        buf366 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 71616)  # alias
        # Topologically Sorted Source Nodes: [linear_15, layer_norm_15], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf65, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias, buf366, s0, 192, grid=grid(s0), stream=stream0)
        buf69 = buf65; del buf65  # reuse
        # Topologically Sorted Source Nodes: [linear_16], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_29.run(arg607_1, _FOLDED_CONST_permute_16, buf69, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta6), stream=stream0)
        buf367 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 71808)  # alias
        # Topologically Sorted Source Nodes: [linear_16, layer_norm_16], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf69, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias, buf367, s0, 192, grid=grid(s0), stream=stream0)
        buf73 = buf69; del buf69  # reuse
        # Topologically Sorted Source Nodes: [linear_17], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_30.run(arg607_1, _FOLDED_CONST_permute_17, buf73, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta2), stream=stream0)
        buf368 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 72000)  # alias
        # Topologically Sorted Source Nodes: [linear_17, layer_norm_17], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf73, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias, buf368, s0, 192, grid=grid(s0), stream=stream0)
        buf77 = buf73; del buf73  # reuse
        # Topologically Sorted Source Nodes: [linear_18], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_31.run(arg607_1, _FOLDED_CONST_permute_18, buf77, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta6), stream=stream0)
        buf369 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 72192)  # alias
        # Topologically Sorted Source Nodes: [linear_18, layer_norm_18], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf77, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias, buf369, s0, 192, grid=grid(s0), stream=stream0)
        buf81 = buf77; del buf77  # reuse
        # Topologically Sorted Source Nodes: [linear_19], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_32.run(arg607_1, _FOLDED_CONST_permute_19, buf81, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf370 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 72384)  # alias
        # Topologically Sorted Source Nodes: [linear_19, layer_norm_19], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf81, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias, buf370, s0, 192, grid=grid(s0), stream=stream0)
        buf85 = buf81; del buf81  # reuse
        # Topologically Sorted Source Nodes: [linear_20], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_33.run(arg607_1, _FOLDED_CONST_permute_20, buf85, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf371 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 72576)  # alias
        # Topologically Sorted Source Nodes: [linear_20, layer_norm_20], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf85, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias, buf371, s0, 192, grid=grid(s0), stream=stream0)
        buf89 = buf85; del buf85  # reuse
        # Topologically Sorted Source Nodes: [linear_21], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_34.run(arg607_1, _FOLDED_CONST_permute_21, buf89, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf372 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 72768)  # alias
        # Topologically Sorted Source Nodes: [linear_21, layer_norm_21], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf89, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias, buf372, s0, 192, grid=grid(s0), stream=stream0)
        buf93 = buf89; del buf89  # reuse
        # Topologically Sorted Source Nodes: [linear_22], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_35.run(arg607_1, _FOLDED_CONST_permute_22, buf93, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta1), stream=stream0)
        buf373 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 72960)  # alias
        # Topologically Sorted Source Nodes: [linear_22, layer_norm_22], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf93, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias, buf373, s0, 192, grid=grid(s0), stream=stream0)
        buf97 = buf93; del buf93  # reuse
        # Topologically Sorted Source Nodes: [linear_23], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_36.run(arg607_1, _FOLDED_CONST_permute_23, buf97, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta7), stream=stream0)
        buf374 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 73152)  # alias
        # Topologically Sorted Source Nodes: [linear_23, layer_norm_23], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf97, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias, buf374, s0, 192, grid=grid(s0), stream=stream0)
        buf101 = buf97; del buf97  # reuse
        # Topologically Sorted Source Nodes: [linear_24], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_37.run(arg607_1, _FOLDED_CONST_permute_24, buf101, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta8), stream=stream0)
        buf375 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 73344)  # alias
        # Topologically Sorted Source Nodes: [linear_24, layer_norm_24], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf101, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias, buf375, s0, 192, grid=grid(s0), stream=stream0)
        buf105 = buf101; del buf101  # reuse
        # Topologically Sorted Source Nodes: [linear_25], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_38.run(arg607_1, _FOLDED_CONST_permute_25, buf105, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf376 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 73536)  # alias
        # Topologically Sorted Source Nodes: [linear_25, layer_norm_25], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf105, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias, buf376, s0, 192, grid=grid(s0), stream=stream0)
        buf109 = buf105; del buf105  # reuse
        # Topologically Sorted Source Nodes: [linear_26], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_39.run(arg607_1, _FOLDED_CONST_permute_26, buf109, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta6), stream=stream0)
        buf377 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 73728)  # alias
        # Topologically Sorted Source Nodes: [linear_26, layer_norm_26], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf109, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias, buf377, s0, 192, grid=grid(s0), stream=stream0)
        buf113 = buf109; del buf109  # reuse
        # Topologically Sorted Source Nodes: [linear_27], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_40.run(arg607_1, _FOLDED_CONST_permute_27, buf113, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta6), stream=stream0)
        buf378 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 73920)  # alias
        # Topologically Sorted Source Nodes: [linear_27, layer_norm_27], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf113, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias, buf378, s0, 192, grid=grid(s0), stream=stream0)
        buf117 = buf113; del buf113  # reuse
        # Topologically Sorted Source Nodes: [linear_28], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_41.run(arg607_1, _FOLDED_CONST_permute_28, buf117, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta4), stream=stream0)
        buf379 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 74112)  # alias
        # Topologically Sorted Source Nodes: [linear_28, layer_norm_28], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf117, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias, buf379, s0, 192, grid=grid(s0), stream=stream0)
        buf121 = buf117; del buf117  # reuse
        # Topologically Sorted Source Nodes: [linear_29], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_42.run(arg607_1, _FOLDED_CONST_permute_29, buf121, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta9), stream=stream0)
        buf380 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 74304)  # alias
        # Topologically Sorted Source Nodes: [linear_29, layer_norm_29], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf121, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias, buf380, s0, 192, grid=grid(s0), stream=stream0)
        buf125 = buf121; del buf121  # reuse
        # Topologically Sorted Source Nodes: [linear_30], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_43.run(arg607_1, _FOLDED_CONST_permute_30, buf125, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf381 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 74496)  # alias
        # Topologically Sorted Source Nodes: [linear_30, layer_norm_30], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf125, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias, buf381, s0, 192, grid=grid(s0), stream=stream0)
        buf129 = buf125; del buf125  # reuse
        # Topologically Sorted Source Nodes: [linear_31], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_44.run(arg607_1, _FOLDED_CONST_permute_31, buf129, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta1), stream=stream0)
        buf382 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 74688)  # alias
        # Topologically Sorted Source Nodes: [linear_31, layer_norm_31], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf129, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias, buf382, s0, 192, grid=grid(s0), stream=stream0)
        buf133 = buf129; del buf129  # reuse
        # Topologically Sorted Source Nodes: [linear_32], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_45.run(arg607_1, _FOLDED_CONST_permute_32, buf133, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta1), stream=stream0)
        buf383 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 74880)  # alias
        # Topologically Sorted Source Nodes: [linear_32, layer_norm_32], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf133, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias, buf383, s0, 192, grid=grid(s0), stream=stream0)
        buf137 = buf133; del buf133  # reuse
        # Topologically Sorted Source Nodes: [linear_33], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_46.run(arg607_1, _FOLDED_CONST_permute_33, buf137, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta1), stream=stream0)
        buf384 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 75072)  # alias
        # Topologically Sorted Source Nodes: [linear_33, layer_norm_33], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf137, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias, buf384, s0, 192, grid=grid(s0), stream=stream0)
        buf141 = buf137; del buf137  # reuse
        # Topologically Sorted Source Nodes: [linear_34], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_47.run(arg607_1, _FOLDED_CONST_permute_34, buf141, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta0), stream=stream0)
        buf385 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 75264)  # alias
        # Topologically Sorted Source Nodes: [linear_34, layer_norm_34], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf141, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias, buf385, s0, 192, grid=grid(s0), stream=stream0)
        buf145 = buf141; del buf141  # reuse
        # Topologically Sorted Source Nodes: [linear_35], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_48.run(arg607_1, _FOLDED_CONST_permute_35, buf145, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 192, meta2), stream=stream0)
        del arg607_1
        buf386 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 75456)  # alias
        # Topologically Sorted Source Nodes: [linear_35, layer_norm_35], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_13.run(buf145, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias, buf386, s0, 192, grid=grid(s0), stream=stream0)
        del buf145
        buf149 = empty_strided_cuda((s0, 6056), (6056, 1), torch.float16)
        # Topologically Sorted Source Nodes: [cat_default_11, linear_default], Original ATen: [aten.cat, aten.addmm]
        triton_poi_fused_addmm_cat_49_xnumel = 6056*s0
        triton_poi_fused_addmm_cat_49.run(arg606_1, buf149, triton_poi_fused_addmm_cat_49_xnumel, grid=grid(triton_poi_fused_addmm_cat_49_xnumel), stream=stream0)
        buf150 = empty_strided_cuda((s0, 256), (256, 1), torch.float16)
        # Topologically Sorted Source Nodes: [cat_default_11, linear_default], Original ATen: [aten.cat, aten.addmm]
        triton_tem_fused_addmm_cat_50.run(buf149, _FOLDED_CONST_constant_pad_nd_default_23, buf150, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 256, meta9), stream=stream0)
        del buf149
        buf186 = buf150; del buf150  # reuse
        # Topologically Sorted Source Nodes: [linear_default, layer_norm_36, sigmoid, mul_477], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_51.run(buf186, _FOLDED_CONST_cat_3, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias, s0, 256, grid=grid(s0), stream=stream0)
        buf169 = empty_strided_cuda((s0, 700), (700, 1), torch.float16)
        buf154 = reinterpret_tensor(buf169, (s0, 36), (700, 1), 0)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_52_xnumel = 36*s0
        triton_poi_fused_cat_52.run(buf2, buf154, triton_poi_fused_cat_52_xnumel, grid=grid(triton_poi_fused_cat_52_xnumel), stream=stream0)
        buf155 = reinterpret_tensor(buf169, (s0, 28), (700, 1), 36)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_53_xnumel = 28*s0
        triton_poi_fused_cat_53.run(buf4, buf155, triton_poi_fused_cat_53_xnumel, grid=grid(triton_poi_fused_cat_53_xnumel), stream=stream0)
        buf156 = reinterpret_tensor(buf169, (s0, 228), (700, 1), 64)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_54_xnumel = 228*s0
        triton_poi_fused_cat_54.run(buf2, buf156, triton_poi_fused_cat_54_xnumel, grid=grid(triton_poi_fused_cat_54_xnumel), stream=stream0)
        buf157 = reinterpret_tensor(buf169, (s0, 16), (700, 1), 292)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_55_xnumel = 16*s0
        triton_poi_fused_cat_55.run(buf4, buf157, triton_poi_fused_cat_55_xnumel, grid=grid(triton_poi_fused_cat_55_xnumel), stream=stream0)
        buf158 = reinterpret_tensor(buf169, (s0, 32), (700, 1), 308)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_56_xnumel = 32*s0
        triton_poi_fused_cat_56.run(buf2, buf158, triton_poi_fused_cat_56_xnumel, grid=grid(triton_poi_fused_cat_56_xnumel), stream=stream0)
        buf159 = reinterpret_tensor(buf169, (s0, 32), (700, 1), 340)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_57_xnumel = 32*s0
        triton_poi_fused_cat_57.run(buf4, buf159, triton_poi_fused_cat_57_xnumel, grid=grid(triton_poi_fused_cat_57_xnumel), stream=stream0)
        buf160 = reinterpret_tensor(buf169, (s0, 32), (700, 1), 372)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_58_xnumel = 32*s0
        triton_poi_fused_cat_58.run(buf2, buf160, triton_poi_fused_cat_58_xnumel, grid=grid(triton_poi_fused_cat_58_xnumel), stream=stream0)
        buf161 = reinterpret_tensor(buf169, (s0, 32), (700, 1), 404)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_59_xnumel = 32*s0
        triton_poi_fused_cat_59.run(buf4, buf161, triton_poi_fused_cat_59_xnumel, grid=grid(triton_poi_fused_cat_59_xnumel), stream=stream0)
        buf162 = reinterpret_tensor(buf169, (s0, 80), (700, 1), 436)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_60_xnumel = 80*s0
        triton_poi_fused_cat_60.run(buf2, buf162, triton_poi_fused_cat_60_xnumel, grid=grid(triton_poi_fused_cat_60_xnumel), stream=stream0)
        buf163 = reinterpret_tensor(buf169, (s0, 16), (700, 1), 516)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_61_xnumel = 16*s0
        triton_poi_fused_cat_61.run(buf4, buf163, triton_poi_fused_cat_61_xnumel, grid=grid(triton_poi_fused_cat_61_xnumel), stream=stream0)
        buf164 = reinterpret_tensor(buf169, (s0, 32), (700, 1), 532)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_62_xnumel = 32*s0
        triton_poi_fused_cat_62.run(buf2, buf164, triton_poi_fused_cat_62_xnumel, grid=grid(triton_poi_fused_cat_62_xnumel), stream=stream0)
        buf165 = reinterpret_tensor(buf169, (s0, 32), (700, 1), 564)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_63_xnumel = 32*s0
        triton_poi_fused_cat_63.run(buf4, buf165, triton_poi_fused_cat_63_xnumel, grid=grid(triton_poi_fused_cat_63_xnumel), stream=stream0)
        buf166 = reinterpret_tensor(buf169, (s0, 16), (700, 1), 596)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_64_xnumel = 16*s0
        triton_poi_fused_cat_64.run(buf2, buf166, triton_poi_fused_cat_64_xnumel, grid=grid(triton_poi_fused_cat_64_xnumel), stream=stream0)
        buf167 = reinterpret_tensor(buf169, (s0, 16), (700, 1), 612)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_65_xnumel = 16*s0
        triton_poi_fused_cat_65.run(buf4, buf167, triton_poi_fused_cat_65_xnumel, grid=grid(triton_poi_fused_cat_65_xnumel), stream=stream0)
        buf168 = reinterpret_tensor(buf169, (s0, 72), (700, 1), 628)  # alias
        # Topologically Sorted Source Nodes: [cat_default_9], Original ATen: [aten.cat]
        triton_poi_fused_cat_66_xnumel = 72*s0
        triton_poi_fused_cat_66.run(buf2, buf168, triton_poi_fused_cat_66_xnumel, grid=grid(triton_poi_fused_cat_66_xnumel), stream=stream0)
        del buf154
        del buf155
        del buf156
        del buf157
        del buf158
        del buf159
        del buf160
        del buf161
        del buf162
        del buf163
        del buf164
        del buf165
        del buf166
        del buf167
        del buf168
        buf170 = empty_strided_cuda((s0, 512), (512, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_default_1], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_67.run(_FOLDED_CONST_cat_6, buf169, _FOLDED_CONST_permute_37, buf170, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 512, meta7), stream=stream0)
        del buf169
        buf195 = empty_strided_cuda((s0, 256), (256, 1), torch.float16)
        # Topologically Sorted Source Nodes: [contiguous_2, layer_norm_37, sigmoid_1, mul_508], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_68.run(buf170, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias, buf195, s0, 256, grid=grid(s0), stream=stream0)
        buf202 = empty_strided_cuda((s0, 200, 64), (12800, 64, 1), torch.float16)
        # Topologically Sorted Source Nodes: [add_779, layer_norm_38], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_69_xnumel = 200*s0
        triton_per_fused_add_native_layer_norm_69.run(arg613_1, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias, buf202, triton_per_fused_add_native_layer_norm_69_xnumel, 64, grid=grid(triton_per_fused_add_native_layer_norm_69_xnumel), stream=stream0)
        del arg613_1
        buf212 = empty_strided_cuda((s0, 200, 64), (12800, 64, 1), torch.float16)
        # Topologically Sorted Source Nodes: [add_784, layer_norm_39], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_69_xnumel = 200*s0
        triton_per_fused_add_native_layer_norm_69.run(arg614_1, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias, buf212, triton_per_fused_add_native_layer_norm_69_xnumel, 64, grid=grid(triton_per_fused_add_native_layer_norm_69_xnumel), stream=stream0)
        del arg614_1
        buf200 = empty_strided_cuda((s0, 32, 64), (2048, 64, 1), torch.float16)
        # Topologically Sorted Source Nodes: [repeat, layer_norm_40], Original ATen: [aten.repeat, aten.native_layer_norm]
        triton_per_fused_native_layer_norm_repeat_70_xnumel = 32*s0
        triton_per_fused_native_layer_norm_repeat_70.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias, buf200, triton_per_fused_native_layer_norm_repeat_70_xnumel, 64, grid=grid(triton_per_fused_native_layer_norm_repeat_70_xnumel), stream=stream0)
        buf210 = empty_strided_cuda((s0, 32, 64), (2048, 64, 1), torch.float16)
        # Topologically Sorted Source Nodes: [repeat_1, layer_norm_41], Original ATen: [aten.repeat, aten.native_layer_norm]
        triton_per_fused_native_layer_norm_repeat_70_xnumel = 32*s0
        triton_per_fused_native_layer_norm_repeat_70.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias, buf210, triton_per_fused_native_layer_norm_repeat_70_xnumel, 64, grid=grid(triton_per_fused_native_layer_norm_repeat_70_xnumel), stream=stream0)
        buf187 = empty_strided_cuda((s0, 3026), (3026, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_default, layer_norm_36, sigmoid, mul_477, linear_42], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_71.run(buf186, _FOLDED_CONST_permute_40, buf187, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 3026, meta10), stream=stream0)
        buf191 = empty_strided_cuda((s0, 3282), (3282, 1), torch.float16)
        buf188 = reinterpret_tensor(buf191, (s0, 3026), (3282, 1), 256)  # alias
        # Topologically Sorted Source Nodes: [linear_42, sigmoid_2, mul_497], Original ATen: [aten.addmm, aten.sigmoid, aten.mul]
        triton_poi_fused_addmm_mul_sigmoid_72_xnumel = 3026*s0
        triton_poi_fused_addmm_mul_sigmoid_72.run(arg606_1, buf187, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b, buf188, triton_poi_fused_addmm_mul_sigmoid_72_xnumel, grid=grid(triton_poi_fused_addmm_mul_sigmoid_72_xnumel), stream=stream0)
        del arg606_1
        del buf187
        buf189 = buf186; del buf186  # reuse
        # Topologically Sorted Source Nodes: [linear_42, sigmoid_2, mul_497, linear_43], Original ATen: [aten.addmm, aten.sigmoid, aten.mul]
        triton_tem_fused_addmm_mul_sigmoid_73.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b, buf188, _FOLDED_CONST_permute_41, buf189, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 256, meta0), stream=stream0)
        buf190 = reinterpret_tensor(buf191, (s0, 256), (3282, 1), 0)  # alias
        # Topologically Sorted Source Nodes: [cat_default_8], Original ATen: [aten.cat]
        triton_poi_fused_cat_74_xnumel = 256*s0
        triton_poi_fused_cat_74.run(buf189, buf190, triton_poi_fused_cat_74_xnumel, grid=grid(triton_poi_fused_cat_74_xnumel), stream=stream0)
        del buf189
        buf230 = empty_strided_cuda((s0, 3282), (3282, 1), torch.float16)
        # Topologically Sorted Source Nodes: [layer_norm_42, sigmoid_3, mul_567], Original ATen: [aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_mul_native_layer_norm_sigmoid_75.run(buf191, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias, buf230, s0, 3282, grid=grid(s0), stream=stream0)
        del buf188
        del buf190
        del buf191
        buf196 = empty_strided_cuda((s0, 1024), (1024, 1), torch.float16)
        # Topologically Sorted Source Nodes: [contiguous_2, layer_norm_37, sigmoid_1, mul_508, linear_46], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul, aten.addmm]
        triton_tem_fused_addmm_clone_mul_native_layer_norm_sigmoid_76.run(buf195, _FOLDED_CONST_permute_44, buf196, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1024, meta5), stream=stream0)
        buf457 = buf196; del buf196  # reuse
        # Topologically Sorted Source Nodes: [linear_46, layer_norm_43, sigmoid_4, mul_570], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_77.run(buf457, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias, s0, 1024, grid=grid(s0), stream=stream0)
        buf201 = empty_strided_cuda((32*s0, 64), (64, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_47], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_78.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias, buf200, _FOLDED_CONST_permute_45, buf201, s0, grid=torch._inductor.kernel.mm_common.mm_grid(32*s0, 64, meta11), stream=stream0)
        del buf200
        buf203 = empty_strided_cuda((200*s0, 64), (64, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_44], Original ATen: [aten.mm]
        triton_tem_fused_mm_79.run(buf202, _FOLDED_CONST_permute_42, buf203, s0, grid=torch._inductor.kernel.mm_common.mm_grid(200*s0, 64, meta1), stream=stream0)
        # Topologically Sorted Source Nodes: [scaled_dot_product_attention], Original ATen: [aten._scaled_dot_product_flash_attention]
        buf204 = torch.ops.aten._scaled_dot_product_flash_attention.default(reinterpret_tensor(buf201, (s0, 1, 32, 64), (2048, 64, 64, 1), 0), reinterpret_tensor(buf203, (s0, 1, 200, 64), (12800, 64, 64, 1), 0), reinterpret_tensor(buf202, (s0, 1, 200, 64), (12800, 64, 64, 1), 0), scale=0.125)
        del buf202
        buf205 = buf204[0]
        del buf204
        buf211 = buf201; del buf201  # reuse
        # Topologically Sorted Source Nodes: [linear_48], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_78.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias, buf210, _FOLDED_CONST_permute_46, buf211, s0, grid=torch._inductor.kernel.mm_common.mm_grid(32*s0, 64, meta11), stream=stream0)
        buf213 = buf203; del buf203  # reuse
        # Topologically Sorted Source Nodes: [linear_45], Original ATen: [aten.mm]
        triton_tem_fused_mm_79.run(buf212, _FOLDED_CONST_permute_43, buf213, s0, grid=torch._inductor.kernel.mm_common.mm_grid(200*s0, 64, meta1), stream=stream0)
        # Topologically Sorted Source Nodes: [scaled_dot_product_attention_1], Original ATen: [aten._scaled_dot_product_flash_attention]
        buf214 = torch.ops.aten._scaled_dot_product_flash_attention.default(reinterpret_tensor(buf211, (s0, 1, 32, 64), (2048, 64, 64, 1), 0), reinterpret_tensor(buf213, (s0, 1, 200, 64), (12800, 64, 64, 1), 0), reinterpret_tensor(buf212, (s0, 1, 200, 64), (12800, 64, 64, 1), 0), scale=0.125)
        del buf212
        del buf213
        buf215 = buf214[0]
        del buf214
        buf236 = reinterpret_tensor(buf211, (s0, 32, 64), (2048, 64, 1), 0); del buf211  # reuse
        # Topologically Sorted Source Nodes: [repeat, add_976, layer_norm_44], Original ATen: [aten.repeat, aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_repeat_80_xnumel = 32*s0
        triton_per_fused_add_native_layer_norm_repeat_80.run(buf205, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias, buf236, triton_per_fused_add_native_layer_norm_repeat_80_xnumel, 64, grid=grid(triton_per_fused_add_native_layer_norm_repeat_80_xnumel), stream=stream0)
        buf226 = buf210; del buf210  # reuse
        # Topologically Sorted Source Nodes: [repeat_1, add_985, layer_norm_45], Original ATen: [aten.repeat, aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_repeat_80_xnumel = 32*s0
        triton_per_fused_add_native_layer_norm_repeat_80.run(buf215, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias, buf226, triton_per_fused_add_native_layer_norm_repeat_80_xnumel, 64, grid=grid(triton_per_fused_add_native_layer_norm_repeat_80_xnumel), stream=stream0)
        buf227 = empty_strided_cuda((32*s0, 128), (128, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_56], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_81.run(buf226, _FOLDED_CONST_permute_57, buf227, s0, grid=torch._inductor.kernel.mm_common.mm_grid(32*s0, 128, meta12), stream=stream0)
        buf228 = reinterpret_tensor(buf227, (s0, 32, 128), (4096, 128, 1), 0); del buf227  # reuse
        # Topologically Sorted Source Nodes: [gelu_1], Original ATen: [aten.gelu]
        triton_poi_fused_gelu_82_xnumel = 4096*s0
        triton_poi_fused_gelu_82.run(buf228, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias, triton_poi_fused_gelu_82_xnumel, grid=grid(triton_poi_fused_gelu_82_xnumel), stream=stream0)
        buf229 = reinterpret_tensor(buf226, (32*s0, 64), (64, 1), 0); del buf226  # reuse
        # Topologically Sorted Source Nodes: [linear_58], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_83.run(buf228, _FOLDED_CONST_permute_59, buf229, s0, grid=torch._inductor.kernel.mm_common.mm_grid(32*s0, 64, meta1), stream=stream0)
        buf231 = empty_strided_cuda((s0, 3288), (3288, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_default_2], Original ATen: [aten.addmm]
        triton_poi_fused_addmm_84_xnumel = 3288*s0
        triton_poi_fused_addmm_84.run(buf230, buf231, triton_poi_fused_addmm_84_xnumel, grid=grid(triton_poi_fused_addmm_84_xnumel), stream=stream0)
        buf232 = empty_strided_cuda((s0, 1152), (1152, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_default_2], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_85.run(_FOLDED_CONST_cat_9, buf231, _FOLDED_CONST_constant_pad_nd_default_21, buf232, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1152, meta13), stream=stream0)
        del buf231
        buf233 = buf195; del buf195  # reuse
        # Topologically Sorted Source Nodes: [contiguous_3, relu_default, nan_to_num_default], Original ATen: [aten.clone, aten.relu, aten.nan_to_num]
        triton_poi_fused_clone_nan_to_num_relu_86_xnumel = 256*s0
        triton_poi_fused_clone_nan_to_num_relu_86.run(buf170, buf233, triton_poi_fused_clone_nan_to_num_relu_86_xnumel, grid=grid(triton_poi_fused_clone_nan_to_num_relu_86_xnumel), stream=stream0)
        buf234 = empty_strided_cuda((s0, 960), (960, 1), torch.float16)
        # Topologically Sorted Source Nodes: [contiguous_3, relu_default, nan_to_num_default, linear_40], Original ATen: [aten.clone, aten.relu, aten.nan_to_num, aten.addmm]
        triton_tem_fused_addmm_clone_nan_to_num_relu_87.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b, buf233, _FOLDED_CONST_permute_38, buf234, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 960, meta14), stream=stream0)
        del buf233
        buf235 = empty_strided_cuda((s0, 1536), (1536, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_41], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_88.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b, buf234, _FOLDED_CONST_permute_39, buf235, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        del buf234
        buf237 = reinterpret_tensor(buf228, (32*s0, 128), (128, 1), 0); del buf228  # reuse
        # Topologically Sorted Source Nodes: [linear_55], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_81.run(buf236, _FOLDED_CONST_permute_56, buf237, s0, grid=torch._inductor.kernel.mm_common.mm_grid(32*s0, 128, meta12), stream=stream0)
        buf238 = reinterpret_tensor(buf237, (s0, 32, 128), (4096, 128, 1), 0); del buf237  # reuse
        # Topologically Sorted Source Nodes: [gelu], Original ATen: [aten.gelu]
        triton_poi_fused_gelu_82_xnumel = 4096*s0
        triton_poi_fused_gelu_82.run(buf238, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias, triton_poi_fused_gelu_82_xnumel, grid=grid(triton_poi_fused_gelu_82_xnumel), stream=stream0)
        buf239 = reinterpret_tensor(buf236, (32*s0, 64), (64, 1), 0); del buf236  # reuse
        # Topologically Sorted Source Nodes: [linear_57], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_83.run(buf238, _FOLDED_CONST_permute_58, buf239, s0, grid=torch._inductor.kernel.mm_common.mm_grid(32*s0, 64, meta1), stream=stream0)
        buf273 = reinterpret_tensor(buf238, (64, s0, 64), (64*s0, 64, 1), 0); del buf238  # reuse
        buf240 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 0)  # alias
        buf241 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 64*s0)  # alias
        buf242 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 128*s0)  # alias
        buf243 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 192*s0)  # alias
        buf244 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 256*s0)  # alias
        buf245 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 320*s0)  # alias
        buf246 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 384*s0)  # alias
        buf247 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 448*s0)  # alias
        buf248 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 512*s0)  # alias
        buf249 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 576*s0)  # alias
        buf250 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 640*s0)  # alias
        buf251 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 704*s0)  # alias
        buf252 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 768*s0)  # alias
        buf253 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 832*s0)  # alias
        buf254 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 896*s0)  # alias
        buf255 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 960*s0)  # alias
        buf256 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1024*s0)  # alias
        buf257 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1088*s0)  # alias
        buf258 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1152*s0)  # alias
        buf259 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1216*s0)  # alias
        buf260 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1280*s0)  # alias
        buf261 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1344*s0)  # alias
        buf262 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1408*s0)  # alias
        buf263 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1472*s0)  # alias
        buf264 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1536*s0)  # alias
        buf265 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1600*s0)  # alias
        buf266 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1664*s0)  # alias
        buf267 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1728*s0)  # alias
        buf268 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1792*s0)  # alias
        buf269 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1856*s0)  # alias
        buf270 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1920*s0)  # alias
        buf271 = reinterpret_tensor(buf273, (1, s0, 64), (64*s0, 64, 1), 1984*s0)  # alias
        # Topologically Sorted Source Nodes: [stack_default], Original ATen: [aten.cat]
        triton_poi_fused_cat_89_xnumel = 64*s0
        triton_poi_fused_cat_89.run(buf215, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb, buf229, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias, buf240, buf241, buf242, buf243, buf244, buf245, buf246, buf247, buf248, buf249, buf250, buf251, buf252, buf253, buf254, buf255, buf256, buf257, buf258, buf259, buf260, buf261, buf262, buf263, buf264, buf265, buf266, buf267, buf268, buf269, buf270, buf271, triton_poi_fused_cat_89_xnumel, grid=grid(triton_poi_fused_cat_89_xnumel), stream=stream0)
        del buf215
        del buf229
        buf272 = reinterpret_tensor(buf273, (32, s0, 64), (64*s0, 64, 1), 2048*s0)  # alias
        # Topologically Sorted Source Nodes: [stack_default], Original ATen: [aten.cat]
        triton_poi_fused_cat_90_xnumel = 2048*s0
        triton_poi_fused_cat_90.run(buf205, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb, buf239, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias, buf272, s0, triton_poi_fused_cat_90_xnumel, grid=grid(triton_poi_fused_cat_90_xnumel), stream=stream0)
        del buf205
        buf274 = empty_strided_cuda((64, s0, 64), (64*s0, 64, 1), torch.float16)
        # Topologically Sorted Source Nodes: [nan_to_num, clamp], Original ATen: [aten.nan_to_num, aten.clamp]
        triton_poi_fused_clamp_nan_to_num_91_xnumel = 4096*s0
        triton_poi_fused_clamp_nan_to_num_91.run(buf273, buf274, triton_poi_fused_clamp_nan_to_num_91_xnumel, grid=grid(triton_poi_fused_clamp_nan_to_num_91_xnumel), stream=stream0)
        del buf240
        del buf241
        del buf242
        del buf243
        del buf244
        del buf245
        del buf246
        del buf247
        del buf248
        del buf249
        del buf250
        del buf251
        del buf252
        del buf253
        del buf254
        del buf255
        del buf256
        del buf257
        del buf258
        del buf259
        del buf260
        del buf261
        del buf262
        del buf263
        del buf264
        del buf265
        del buf266
        del buf267
        del buf268
        del buf269
        del buf270
        del buf271
        del buf272
        del buf273
        buf275 = empty_strided_cuda((64, s0, 192), (192*s0, 192, 1), torch.float16)
        # Topologically Sorted Source Nodes: [nan_to_num, clamp, baddbmm], Original ATen: [aten.nan_to_num, aten.clamp, aten.baddbmm]
        triton_tem_fused_baddbmm_clamp_nan_to_num_92.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias, buf274, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight, buf275, s0, grid=torch._inductor.kernel.bmm.bmm_grid(64, s0, 192, meta5), stream=stream0)
        del buf274
        buf276 = reinterpret_tensor(buf388, (s0, 2880), (87936, 1), 0)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_93_xnumel = 2880*s0
        triton_poi_fused_cat_93.run(buf2, buf276, triton_poi_fused_cat_93_xnumel, grid=grid(triton_poi_fused_cat_93_xnumel), stream=stream0)
        buf277 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 2880)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_94_xnumel = 192*s0
        triton_poi_fused_cat_94.run(buf4, buf277, triton_poi_fused_cat_94_xnumel, grid=grid(triton_poi_fused_cat_94_xnumel), stream=stream0)
        buf278 = reinterpret_tensor(buf388, (s0, 3648), (87936, 1), 3072)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_95_xnumel = 3648*s0
        triton_poi_fused_cat_95.run(buf2, buf278, triton_poi_fused_cat_95_xnumel, grid=grid(triton_poi_fused_cat_95_xnumel), stream=stream0)
        buf279 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 6720)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_96_xnumel = 192*s0
        triton_poi_fused_cat_96.run(buf4, buf279, triton_poi_fused_cat_96_xnumel, grid=grid(triton_poi_fused_cat_96_xnumel), stream=stream0)
        buf280 = reinterpret_tensor(buf388, (s0, 768), (87936, 1), 6912)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_97_xnumel = 768*s0
        triton_poi_fused_cat_97.run(buf2, buf280, triton_poi_fused_cat_97_xnumel, grid=grid(triton_poi_fused_cat_97_xnumel), stream=stream0)
        buf281 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 7680)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_98_xnumel = 192*s0
        triton_poi_fused_cat_98.run(buf4, buf281, triton_poi_fused_cat_98_xnumel, grid=grid(triton_poi_fused_cat_98_xnumel), stream=stream0)
        buf282 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 7872)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_99_xnumel = 192*s0
        triton_poi_fused_cat_99.run(buf2, buf282, triton_poi_fused_cat_99_xnumel, grid=grid(triton_poi_fused_cat_99_xnumel), stream=stream0)
        buf283 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 8064)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_100_xnumel = 384*s0
        triton_poi_fused_cat_100.run(buf4, buf283, triton_poi_fused_cat_100_xnumel, grid=grid(triton_poi_fused_cat_100_xnumel), stream=stream0)
        buf284 = reinterpret_tensor(buf388, (s0, 1152), (87936, 1), 8448)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_101_xnumel = 1152*s0
        triton_poi_fused_cat_101.run(buf2, buf284, triton_poi_fused_cat_101_xnumel, grid=grid(triton_poi_fused_cat_101_xnumel), stream=stream0)
        buf285 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 9600)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_102_xnumel = 192*s0
        triton_poi_fused_cat_102.run(buf4, buf285, triton_poi_fused_cat_102_xnumel, grid=grid(triton_poi_fused_cat_102_xnumel), stream=stream0)
        buf286 = reinterpret_tensor(buf388, (s0, 960), (87936, 1), 9792)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_103_xnumel = 960*s0
        triton_poi_fused_cat_103.run(buf2, buf286, triton_poi_fused_cat_103_xnumel, grid=grid(triton_poi_fused_cat_103_xnumel), stream=stream0)
        buf287 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 10752)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_104_xnumel = 192*s0
        triton_poi_fused_cat_104.run(buf4, buf287, triton_poi_fused_cat_104_xnumel, grid=grid(triton_poi_fused_cat_104_xnumel), stream=stream0)
        buf288 = reinterpret_tensor(buf388, (s0, 576), (87936, 1), 10944)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_105_xnumel = 576*s0
        triton_poi_fused_cat_105.run(buf2, buf288, triton_poi_fused_cat_105_xnumel, grid=grid(triton_poi_fused_cat_105_xnumel), stream=stream0)
        buf289 = reinterpret_tensor(buf388, (s0, 768), (87936, 1), 11520)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_106_xnumel = 768*s0
        triton_poi_fused_cat_106.run(buf4, buf289, triton_poi_fused_cat_106_xnumel, grid=grid(triton_poi_fused_cat_106_xnumel), stream=stream0)
        buf290 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 12288)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_107_xnumel = 192*s0
        triton_poi_fused_cat_107.run(buf2, buf290, triton_poi_fused_cat_107_xnumel, grid=grid(triton_poi_fused_cat_107_xnumel), stream=stream0)
        buf291 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 12480)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_108_xnumel = 192*s0
        triton_poi_fused_cat_108.run(buf4, buf291, triton_poi_fused_cat_108_xnumel, grid=grid(triton_poi_fused_cat_108_xnumel), stream=stream0)
        buf292 = reinterpret_tensor(buf388, (s0, 768), (87936, 1), 12672)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_109_xnumel = 768*s0
        triton_poi_fused_cat_109.run(buf2, buf292, triton_poi_fused_cat_109_xnumel, grid=grid(triton_poi_fused_cat_109_xnumel), stream=stream0)
        buf293 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 13440)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_110_xnumel = 384*s0
        triton_poi_fused_cat_110.run(buf4, buf293, triton_poi_fused_cat_110_xnumel, grid=grid(triton_poi_fused_cat_110_xnumel), stream=stream0)
        buf294 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 13824)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_111_xnumel = 384*s0
        triton_poi_fused_cat_111.run(buf2, buf294, triton_poi_fused_cat_111_xnumel, grid=grid(triton_poi_fused_cat_111_xnumel), stream=stream0)
        buf295 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 14208)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_112_xnumel = 192*s0
        triton_poi_fused_cat_112.run(buf4, buf295, triton_poi_fused_cat_112_xnumel, grid=grid(triton_poi_fused_cat_112_xnumel), stream=stream0)
        buf296 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 14400)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_113_xnumel = 384*s0
        triton_poi_fused_cat_113.run(buf2, buf296, triton_poi_fused_cat_113_xnumel, grid=grid(triton_poi_fused_cat_113_xnumel), stream=stream0)
        buf297 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 14784)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_114_xnumel = 192*s0
        triton_poi_fused_cat_114.run(buf4, buf297, triton_poi_fused_cat_114_xnumel, grid=grid(triton_poi_fused_cat_114_xnumel), stream=stream0)
        buf298 = reinterpret_tensor(buf388, (s0, 1152), (87936, 1), 14976)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_115_xnumel = 1152*s0
        triton_poi_fused_cat_115.run(buf2, buf298, triton_poi_fused_cat_115_xnumel, grid=grid(triton_poi_fused_cat_115_xnumel), stream=stream0)
        buf299 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 16128)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_116_xnumel = 192*s0
        triton_poi_fused_cat_116.run(buf4, buf299, triton_poi_fused_cat_116_xnumel, grid=grid(triton_poi_fused_cat_116_xnumel), stream=stream0)
        buf300 = reinterpret_tensor(buf388, (s0, 768), (87936, 1), 16320)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_117_xnumel = 768*s0
        triton_poi_fused_cat_117.run(buf2, buf300, triton_poi_fused_cat_117_xnumel, grid=grid(triton_poi_fused_cat_117_xnumel), stream=stream0)
        buf301 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 17088)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_118_xnumel = 192*s0
        triton_poi_fused_cat_118.run(buf4, buf301, triton_poi_fused_cat_118_xnumel, grid=grid(triton_poi_fused_cat_118_xnumel), stream=stream0)
        buf302 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 17280)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_119_xnumel = 384*s0
        triton_poi_fused_cat_119.run(buf2, buf302, triton_poi_fused_cat_119_xnumel, grid=grid(triton_poi_fused_cat_119_xnumel), stream=stream0)
        buf303 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 17664)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_120_xnumel = 192*s0
        triton_poi_fused_cat_120.run(buf4, buf303, triton_poi_fused_cat_120_xnumel, grid=grid(triton_poi_fused_cat_120_xnumel), stream=stream0)
        buf304 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 17856)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_121_xnumel = 192*s0
        triton_poi_fused_cat_121.run(buf2, buf304, triton_poi_fused_cat_121_xnumel, grid=grid(triton_poi_fused_cat_121_xnumel), stream=stream0)
        buf305 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 18048)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_122_xnumel = 384*s0
        triton_poi_fused_cat_122.run(buf4, buf305, triton_poi_fused_cat_122_xnumel, grid=grid(triton_poi_fused_cat_122_xnumel), stream=stream0)
        buf306 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 18432)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_123_xnumel = 384*s0
        triton_poi_fused_cat_123.run(buf2, buf306, triton_poi_fused_cat_123_xnumel, grid=grid(triton_poi_fused_cat_123_xnumel), stream=stream0)
        buf307 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 18816)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_124_xnumel = 192*s0
        triton_poi_fused_cat_124.run(buf4, buf307, triton_poi_fused_cat_124_xnumel, grid=grid(triton_poi_fused_cat_124_xnumel), stream=stream0)
        buf308 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 19008)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_125_xnumel = 192*s0
        triton_poi_fused_cat_125.run(buf2, buf308, triton_poi_fused_cat_125_xnumel, grid=grid(triton_poi_fused_cat_125_xnumel), stream=stream0)
        buf309 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 19200)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_126_xnumel = 192*s0
        triton_poi_fused_cat_126.run(buf4, buf309, triton_poi_fused_cat_126_xnumel, grid=grid(triton_poi_fused_cat_126_xnumel), stream=stream0)
        buf310 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 19392)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_127_xnumel = 384*s0
        triton_poi_fused_cat_127.run(buf2, buf310, triton_poi_fused_cat_127_xnumel, grid=grid(triton_poi_fused_cat_127_xnumel), stream=stream0)
        buf311 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 19776)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_128_xnumel = 384*s0
        triton_poi_fused_cat_128.run(buf4, buf311, triton_poi_fused_cat_128_xnumel, grid=grid(triton_poi_fused_cat_128_xnumel), stream=stream0)
        buf312 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 20160)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_129_xnumel = 384*s0
        triton_poi_fused_cat_129.run(buf2, buf312, triton_poi_fused_cat_129_xnumel, grid=grid(triton_poi_fused_cat_129_xnumel), stream=stream0)
        buf313 = reinterpret_tensor(buf388, (s0, 13056), (87936, 1), 20544)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_130_xnumel = 13056*s0
        triton_poi_fused_cat_130.run(buf4, buf313, triton_poi_fused_cat_130_xnumel, grid=grid(triton_poi_fused_cat_130_xnumel), stream=stream0)
        buf314 = reinterpret_tensor(buf388, (s0, 768), (87936, 1), 33600)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_131_xnumel = 768*s0
        triton_poi_fused_cat_131.run(buf2, buf314, triton_poi_fused_cat_131_xnumel, grid=grid(triton_poi_fused_cat_131_xnumel), stream=stream0)
        buf315 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 34368)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_132_xnumel = 192*s0
        triton_poi_fused_cat_132.run(buf4, buf315, triton_poi_fused_cat_132_xnumel, grid=grid(triton_poi_fused_cat_132_xnumel), stream=stream0)
        buf316 = reinterpret_tensor(buf388, (s0, 1152), (87936, 1), 34560)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_133_xnumel = 1152*s0
        triton_poi_fused_cat_133.run(buf2, buf316, triton_poi_fused_cat_133_xnumel, grid=grid(triton_poi_fused_cat_133_xnumel), stream=stream0)
        buf317 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 35712)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_134_xnumel = 192*s0
        triton_poi_fused_cat_134.run(buf4, buf317, triton_poi_fused_cat_134_xnumel, grid=grid(triton_poi_fused_cat_134_xnumel), stream=stream0)
        buf318 = reinterpret_tensor(buf388, (s0, 576), (87936, 1), 35904)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_135_xnumel = 576*s0
        triton_poi_fused_cat_135.run(buf2, buf318, triton_poi_fused_cat_135_xnumel, grid=grid(triton_poi_fused_cat_135_xnumel), stream=stream0)
        buf319 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 36480)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_136_xnumel = 192*s0
        triton_poi_fused_cat_136.run(buf4, buf319, triton_poi_fused_cat_136_xnumel, grid=grid(triton_poi_fused_cat_136_xnumel), stream=stream0)
        buf320 = reinterpret_tensor(buf388, (s0, 2880), (87936, 1), 36672)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_137_xnumel = 2880*s0
        triton_poi_fused_cat_137.run(buf2, buf320, triton_poi_fused_cat_137_xnumel, grid=grid(triton_poi_fused_cat_137_xnumel), stream=stream0)
        buf321 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 39552)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_138_xnumel = 384*s0
        triton_poi_fused_cat_138.run(buf4, buf321, triton_poi_fused_cat_138_xnumel, grid=grid(triton_poi_fused_cat_138_xnumel), stream=stream0)
        buf322 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 39936)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_139_xnumel = 384*s0
        triton_poi_fused_cat_139.run(buf2, buf322, triton_poi_fused_cat_139_xnumel, grid=grid(triton_poi_fused_cat_139_xnumel), stream=stream0)
        buf323 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 40320)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_140_xnumel = 192*s0
        triton_poi_fused_cat_140.run(buf4, buf323, triton_poi_fused_cat_140_xnumel, grid=grid(triton_poi_fused_cat_140_xnumel), stream=stream0)
        buf324 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 40512)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_141_xnumel = 192*s0
        triton_poi_fused_cat_141.run(buf2, buf324, triton_poi_fused_cat_141_xnumel, grid=grid(triton_poi_fused_cat_141_xnumel), stream=stream0)
        buf325 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 40704)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_142_xnumel = 192*s0
        triton_poi_fused_cat_142.run(buf4, buf325, triton_poi_fused_cat_142_xnumel, grid=grid(triton_poi_fused_cat_142_xnumel), stream=stream0)
        buf326 = reinterpret_tensor(buf388, (s0, 1920), (87936, 1), 40896)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_143_xnumel = 1920*s0
        triton_poi_fused_cat_143.run(buf2, buf326, triton_poi_fused_cat_143_xnumel, grid=grid(triton_poi_fused_cat_143_xnumel), stream=stream0)
        buf327 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 42816)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_144_xnumel = 384*s0
        triton_poi_fused_cat_144.run(buf4, buf327, triton_poi_fused_cat_144_xnumel, grid=grid(triton_poi_fused_cat_144_xnumel), stream=stream0)
        buf328 = reinterpret_tensor(buf388, (s0, 9792), (87936, 1), 43200)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_145_xnumel = 9792*s0
        triton_poi_fused_cat_145.run(buf2, buf328, triton_poi_fused_cat_145_xnumel, grid=grid(triton_poi_fused_cat_145_xnumel), stream=stream0)
        buf329 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 52992)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_146_xnumel = 192*s0
        triton_poi_fused_cat_146.run(buf4, buf329, triton_poi_fused_cat_146_xnumel, grid=grid(triton_poi_fused_cat_146_xnumel), stream=stream0)
        buf330 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 53184)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_147_xnumel = 192*s0
        triton_poi_fused_cat_147.run(buf2, buf330, triton_poi_fused_cat_147_xnumel, grid=grid(triton_poi_fused_cat_147_xnumel), stream=stream0)
        buf331 = reinterpret_tensor(buf388, (s0, 768), (87936, 1), 53376)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_148_xnumel = 768*s0
        triton_poi_fused_cat_148.run(buf4, buf331, triton_poi_fused_cat_148_xnumel, grid=grid(triton_poi_fused_cat_148_xnumel), stream=stream0)
        buf332 = reinterpret_tensor(buf388, (s0, 576), (87936, 1), 54144)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_149_xnumel = 576*s0
        triton_poi_fused_cat_149.run(buf2, buf332, triton_poi_fused_cat_149_xnumel, grid=grid(triton_poi_fused_cat_149_xnumel), stream=stream0)
        buf333 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 54720)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_150_xnumel = 192*s0
        triton_poi_fused_cat_150.run(buf4, buf333, triton_poi_fused_cat_150_xnumel, grid=grid(triton_poi_fused_cat_150_xnumel), stream=stream0)
        buf334 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 54912)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_151_xnumel = 192*s0
        triton_poi_fused_cat_151.run(buf2, buf334, triton_poi_fused_cat_151_xnumel, grid=grid(triton_poi_fused_cat_151_xnumel), stream=stream0)
        buf335 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 55104)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_152_xnumel = 384*s0
        triton_poi_fused_cat_152.run(buf4, buf335, triton_poi_fused_cat_152_xnumel, grid=grid(triton_poi_fused_cat_152_xnumel), stream=stream0)
        buf336 = reinterpret_tensor(buf388, (s0, 384), (87936, 1), 55488)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_153_xnumel = 384*s0
        triton_poi_fused_cat_153.run(buf2, buf336, triton_poi_fused_cat_153_xnumel, grid=grid(triton_poi_fused_cat_153_xnumel), stream=stream0)
        buf337 = reinterpret_tensor(buf388, (s0, 2496), (87936, 1), 55872)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_154_xnumel = 2496*s0
        triton_poi_fused_cat_154.run(buf4, buf337, triton_poi_fused_cat_154_xnumel, grid=grid(triton_poi_fused_cat_154_xnumel), stream=stream0)
        buf338 = reinterpret_tensor(buf388, (s0, 576), (87936, 1), 58368)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_155_xnumel = 576*s0
        triton_poi_fused_cat_155.run(buf2, buf338, triton_poi_fused_cat_155_xnumel, grid=grid(triton_poi_fused_cat_155_xnumel), stream=stream0)
        buf339 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 58944)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_156_xnumel = 192*s0
        triton_poi_fused_cat_156.run(buf4, buf339, triton_poi_fused_cat_156_xnumel, grid=grid(triton_poi_fused_cat_156_xnumel), stream=stream0)
        buf340 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 59136)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_157_xnumel = 192*s0
        triton_poi_fused_cat_157.run(buf2, buf340, triton_poi_fused_cat_157_xnumel, grid=grid(triton_poi_fused_cat_157_xnumel), stream=stream0)
        buf341 = reinterpret_tensor(buf388, (s0, 768), (87936, 1), 59328)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_158_xnumel = 768*s0
        triton_poi_fused_cat_158.run(buf4, buf341, triton_poi_fused_cat_158_xnumel, grid=grid(triton_poi_fused_cat_158_xnumel), stream=stream0)
        buf342 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 60096)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_159_xnumel = 192*s0
        triton_poi_fused_cat_159.run(buf2, buf342, triton_poi_fused_cat_159_xnumel, grid=grid(triton_poi_fused_cat_159_xnumel), stream=stream0)
        buf343 = reinterpret_tensor(buf388, (s0, 5760), (87936, 1), 60288)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_160_xnumel = 5760*s0
        triton_poi_fused_cat_160.run(buf4, buf343, triton_poi_fused_cat_160_xnumel, grid=grid(triton_poi_fused_cat_160_xnumel), stream=stream0)
        buf344 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 66048)  # alias
        # Topologically Sorted Source Nodes: [contiguous_4], Original ATen: [aten.clone]
        triton_poi_fused_clone_161_xnumel = 192*s0
        triton_poi_fused_clone_161.run(buf232, buf344, triton_poi_fused_clone_161_xnumel, grid=grid(triton_poi_fused_clone_161_xnumel), stream=stream0)
        buf345 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 66240)  # alias
        # Topologically Sorted Source Nodes: [contiguous_5], Original ATen: [aten.clone]
        triton_poi_fused_clone_162_xnumel = 192*s0
        triton_poi_fused_clone_162.run(buf232, buf345, triton_poi_fused_clone_162_xnumel, grid=grid(triton_poi_fused_clone_162_xnumel), stream=stream0)
        buf346 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 66432)  # alias
        # Topologically Sorted Source Nodes: [contiguous_6], Original ATen: [aten.clone]
        triton_poi_fused_clone_163_xnumel = 192*s0
        triton_poi_fused_clone_163.run(buf232, buf346, triton_poi_fused_clone_163_xnumel, grid=grid(triton_poi_fused_clone_163_xnumel), stream=stream0)
        buf347 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 66624)  # alias
        # Topologically Sorted Source Nodes: [contiguous_7], Original ATen: [aten.clone]
        triton_poi_fused_clone_164_xnumel = 192*s0
        triton_poi_fused_clone_164.run(buf232, buf347, triton_poi_fused_clone_164_xnumel, grid=grid(triton_poi_fused_clone_164_xnumel), stream=stream0)
        buf348 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 66816)  # alias
        # Topologically Sorted Source Nodes: [contiguous_8], Original ATen: [aten.clone]
        triton_poi_fused_clone_165_xnumel = 192*s0
        triton_poi_fused_clone_165.run(buf232, buf348, triton_poi_fused_clone_165_xnumel, grid=grid(triton_poi_fused_clone_165_xnumel), stream=stream0)
        buf349 = reinterpret_tensor(buf388, (s0, 192), (87936, 1), 67008)  # alias
        # Topologically Sorted Source Nodes: [contiguous_9], Original ATen: [aten.clone]
        triton_poi_fused_clone_166_xnumel = 192*s0
        triton_poi_fused_clone_166.run(buf232, buf349, triton_poi_fused_clone_166_xnumel, grid=grid(triton_poi_fused_clone_166_xnumel), stream=stream0)
        del buf232
        buf350 = reinterpret_tensor(buf388, (s0, 1536), (87936, 1), 67200)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_167_xnumel = 1536*s0
        triton_poi_fused_cat_167.run(buf235, buf350, triton_poi_fused_cat_167_xnumel, grid=grid(triton_poi_fused_cat_167_xnumel), stream=stream0)
        buf387 = reinterpret_tensor(buf388, (s0, 12288), (87936, 1), 75648)  # alias
        # Topologically Sorted Source Nodes: [cat_default_7], Original ATen: [aten.cat]
        triton_poi_fused_cat_168_xnumel = 12288*s0
        triton_poi_fused_cat_168.run(buf275, buf387, s0, triton_poi_fused_cat_168_xnumel, grid=grid(triton_poi_fused_cat_168_xnumel), stream=stream0)
        del buf275
        del buf276
        del buf277
        del buf278
        del buf279
        del buf280
        del buf281
        del buf282
        del buf283
        del buf284
        del buf285
        del buf286
        del buf287
        del buf288
        del buf289
        del buf290
        del buf291
        del buf292
        del buf293
        del buf294
        del buf295
        del buf296
        del buf297
        del buf298
        del buf299
        del buf300
        del buf301
        del buf302
        del buf303
        del buf304
        del buf305
        del buf306
        del buf307
        del buf308
        del buf309
        del buf310
        del buf311
        del buf312
        del buf313
        del buf314
        del buf315
        del buf316
        del buf317
        del buf318
        del buf319
        del buf320
        del buf321
        del buf322
        del buf323
        del buf324
        del buf325
        del buf326
        del buf327
        del buf328
        del buf329
        del buf330
        del buf331
        del buf332
        del buf333
        del buf334
        del buf335
        del buf336
        del buf337
        del buf338
        del buf339
        del buf340
        del buf341
        del buf342
        del buf343
        del buf350
        del buf351
        del buf352
        del buf353
        del buf354
        del buf355
        del buf356
        del buf357
        del buf358
        del buf359
        del buf360
        del buf361
        del buf362
        del buf363
        del buf364
        del buf365
        del buf366
        del buf367
        del buf368
        del buf369
        del buf370
        del buf371
        del buf372
        del buf373
        del buf374
        del buf375
        del buf376
        del buf377
        del buf378
        del buf379
        del buf380
        del buf381
        del buf382
        del buf383
        del buf384
        del buf385
        del buf386
        del buf387
        buf389 = empty_strided_cuda((s0, 174, 192), (33408, 192, 1), torch.float16)
        # Topologically Sorted Source Nodes: [matmul], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_169.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w, buf388, buf389, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 174, 192, meta8), stream=stream0)
        buf393 = buf389; del buf389  # reuse
        # Topologically Sorted Source Nodes: [add_1891, layer_norm_46], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_170_xnumel = 174*s0
        triton_per_fused_add_native_layer_norm_170.run(buf393, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b, triton_per_fused_add_native_layer_norm_170_xnumel, 192, grid=grid(triton_per_fused_add_native_layer_norm_170_xnumel), stream=stream0)
        buf394 = buf235; del buf235  # reuse
        # Topologically Sorted Source Nodes: [linear_default_3], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_171.run(buf393, _FOLDED_CONST_permute_63, buf394, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf409 = empty_strided_cuda((s0, 768), (768, 1), torch.float16)
        # Topologically Sorted Source Nodes: [contiguous_10, layer_norm_47, sigmoid_5, mul_1509, layer_norm_49], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_172.run(buf394, _FOLDED_CONST_cat_13, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b, buf409, s0, 768, grid=grid(s0), stream=stream0)
        buf414 = empty_strided_cuda((s0, 768), (768, 1), torch.float16)
        # Topologically Sorted Source Nodes: [contiguous_11, layer_norm_48, sigmoid_6, mul_1514, layer_norm_50], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_173.run(buf394, _FOLDED_CONST_cat_13, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b, buf414, s0, 768, grid=grid(s0), stream=stream0)
        buf410 = empty_strided_cuda((s0, 768), (768, 1), torch.float16)
        # Topologically Sorted Source Nodes: [layer_norm_49, linear_61], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf409, _FOLDED_CONST_permute_64, buf410, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf427 = buf409; del buf409  # reuse
        # Topologically Sorted Source Nodes: [linear_61, layer_norm_51, sigmoid_7, mul_1531, layer_norm_53], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf410, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b, buf427, s0, 768, grid=grid(s0), stream=stream0)
        buf415 = buf410; del buf410  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_50, linear_62], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf414, _FOLDED_CONST_permute_65, buf415, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf432 = buf414; del buf414  # reuse
        # Topologically Sorted Source Nodes: [linear_62, layer_norm_52, sigmoid_8, mul_1536, layer_norm_54], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf415, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b, buf432, s0, 768, grid=grid(s0), stream=stream0)
        buf428 = empty_strided_cuda((s0, 21984), (21984, 1), torch.float16)
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf427, _FOLDED_CONST_permute_66, out=buf428)
        buf437 = buf428; del buf428  # reuse
        # Topologically Sorted Source Nodes: [linear_63, layer_norm_55], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_176.run(buf437, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, s0, 21984, grid=grid(s0), stream=stream0)
        buf433 = empty_strided_cuda((s0, 9216), (9216, 1), torch.float16)
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf432, _FOLDED_CONST_permute_67, out=buf433)
        buf434 = empty_strided_cuda((s0, 1), (1, s0), torch.float32)
        buf435 = empty_strided_cuda((s0, 1), (1, s0), torch.float32)
        # Topologically Sorted Source Nodes: [linear_64, layer_norm_56], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_177.run(buf433, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf434, buf435, s0, 9216, grid=grid(s0), stream=stream0)
        buf438 = empty_strided_cuda((s0, 192, 48), (9216, 48, 1), torch.float16)
        # Topologically Sorted Source Nodes: [bmm], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_178.run(buf388, buf437, buf438, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 192, 48, meta15), stream=stream0)
        buf443 = empty_strided_cuda((s0, 192, 48), (9216, 48, 1), torch.float16)
        # Topologically Sorted Source Nodes: [add_2006, layer_norm_57], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_179_xnumel = 192*s0
        triton_per_fused_add_native_layer_norm_179.run(buf433, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf434, buf435, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, buf438, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b, buf443, triton_per_fused_add_native_layer_norm_179_xnumel, 48, grid=grid(triton_per_fused_add_native_layer_norm_179_xnumel), stream=stream0)
        buf444 = reinterpret_tensor(buf437, (s0, 458, 48), (21984, 48, 1), 0); del buf437  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_57, bmm_1], Original ATen: [aten.native_layer_norm, aten.bmm]
        triton_tem_fused_bmm_native_layer_norm_180.run(buf388, buf443, buf444, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 458, 48, meta5), stream=stream0)
        buf448 = reinterpret_tensor(buf444, (s0, 21984), (21984, 1), 0); del buf444  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_58], Original ATen: [aten.native_layer_norm]
        triton_red_fused_native_layer_norm_181.run(buf448, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b, s0, 21984, grid=grid(s0), stream=stream0)
        buf449 = reinterpret_tensor(buf239, (s0, 2048), (2048, 1), 0); del buf239  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf448, _FOLDED_CONST_permute_68, out=buf449)
        del buf448
        buf453 = empty_strided_cuda((s0, 2048), (2048, 1), torch.float32)
        buf454 = buf435; del buf435  # reuse
        buf455 = buf434; del buf434  # reuse
        # Topologically Sorted Source Nodes: [linear_65, layer_norm_59, sigmoid_9, mul_1580, layer_norm_60], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182.run(buf449, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias, buf453, buf454, buf455, s0, 2048, grid=grid(s0), stream=stream0)
        buf458 = empty_strided_cuda((s0, 6354), (6354, 1), torch.float16)
        buf462 = buf458; del buf458  # reuse
        # Topologically Sorted Source Nodes: [cat_default_6, layer_norm_61], Original ATen: [aten.cat, aten.native_layer_norm]
        triton_red_fused_cat_native_layer_norm_183.run(buf462, buf457, buf230, buf453, buf454, buf455, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b, s0, 6354, grid=grid(s0), stream=stream0)
        buf463 = empty_strided_cuda((s0, 6360), (6360, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_66], Original ATen: [aten.addmm]
        triton_poi_fused_addmm_184_xnumel = 6360*s0
        triton_poi_fused_addmm_184.run(buf462, buf463, triton_poi_fused_addmm_184_xnumel, grid=grid(triton_poi_fused_addmm_184_xnumel), stream=stream0)
        buf464 = empty_strided_cuda((s0, 384), (384, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_66], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_185.run(buf463, _FOLDED_CONST_constant_pad_nd_default_19, buf464, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 384, meta16), stream=stream0)
        buf468 = buf464; del buf464  # reuse
        # Topologically Sorted Source Nodes: [linear_66, layer_norm_62], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_186.run(buf468, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, s0, 384, grid=grid(s0), stream=stream0)
        buf469 = empty_strided_cuda((s0, 6354), (6354, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_66, layer_norm_62, linear_67], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_tem_fused_addmm_native_layer_norm_187.run(buf468, _FOLDED_CONST_permute_70, buf469, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 6354, meta10), stream=stream0)
        buf473 = empty_strided_cuda((s0, 6354), (6354, 1), torch.float32)
        buf474 = buf455; del buf455  # reuse
        buf475 = buf454; del buf454  # reuse
        # Topologically Sorted Source Nodes: [linear_67, layer_norm_63, addcmul, layer_norm_64], Original ATen: [aten.addmm, aten.native_layer_norm, aten.addcmul]
        triton_red_fused_addcmul_addmm_native_layer_norm_188.run(buf469, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias, buf462, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b, buf473, buf474, buf475, s0, 6354, grid=grid(s0), stream=stream0)
        buf477 = buf463; del buf463  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_64, linear_68], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_poi_fused_addmm_native_layer_norm_189_xnumel = 6360*s0
        triton_poi_fused_addmm_native_layer_norm_189.run(buf473, buf474, buf475, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b, buf477, triton_poi_fused_addmm_native_layer_norm_189_xnumel, grid=grid(triton_poi_fused_addmm_native_layer_norm_189_xnumel), stream=stream0)
        buf478 = empty_strided_cuda((s0, 3072), (3072, 1), torch.float16)
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf477, _FOLDED_CONST_constant_pad_nd_default_17, out=buf478)
        buf482 = empty_strided_cuda((s0, 3072), (3072, 1), torch.float32)
        buf486 = empty_strided_cuda((s0, 3072), (3072, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_68, layer_norm_65, sigmoid_10, mul_1607, layer_norm_66], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190.run(buf478, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b, buf482, buf486, s0, 3072, grid=grid(s0), stream=stream0)
        buf487 = buf394; del buf394  # reuse
        # Topologically Sorted Source Nodes: [linear_69], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf486, _FOLDED_CONST_permute_72, buf487, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf491 = empty_strided_cuda((s0, 1536), (1536, 1), torch.float32)
        buf495 = empty_strided_cuda((s0, 1536), (1536, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_69, layer_norm_67, sigmoid_11, mul_1618, layer_norm_68], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf487, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b, buf491, buf495, s0, 1536, grid=grid(s0), stream=stream0)
        buf496 = buf478; del buf478  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf495, _FOLDED_CONST_permute_73, out=buf496)
        buf500 = buf496; del buf496  # reuse
        buf504 = buf500; del buf500  # reuse
        # Topologically Sorted Source Nodes: [linear_70, layer_norm_69, add_2100, layer_norm_70, sigmoid_12, mul_1633], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193.run(buf504, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias, buf486, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf505 = buf495; del buf495  # reuse
        # Topologically Sorted Source Nodes: [linear_71], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf504, _FOLDED_CONST_permute_74, buf505, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf509 = buf491; del buf491  # reuse
        buf513 = buf487; del buf487  # reuse
        # Topologically Sorted Source Nodes: [linear_71, layer_norm_71, sigmoid_13, mul_1644, layer_norm_72], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf505, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b, buf509, buf513, s0, 1536, grid=grid(s0), stream=stream0)
        buf514 = empty_strided_cuda((s0, 3072), (3072, 1), torch.float16)
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf513, _FOLDED_CONST_permute_75, out=buf514)
        buf518 = buf486; del buf486  # reuse
        buf522 = buf518; del buf518  # reuse
        # Topologically Sorted Source Nodes: [linear_72, layer_norm_73, add_2113, add_2138, layer_norm_74, sigmoid_14, mul_1659], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194.run(buf522, buf514, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias, buf504, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf523 = reinterpret_tensor(buf443, (s0, 9216), (9216, 1), 0); del buf443  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_74, sigmoid_14, mul_1659, linear_73], Original ATen: [aten.native_layer_norm, aten.sigmoid, aten.mul, aten.addmm]
        triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_195.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias, buf522, _FOLDED_CONST_permute_76, buf523, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 9216, meta12), stream=stream0)
        buf524 = empty_strided_cuda((s0, 19584), (19584, 1), torch.float16)
        buf528 = reinterpret_tensor(buf524, (s0, 102, 192), (19584, 192, 1), 0); del buf524  # reuse
        # Topologically Sorted Source Nodes: [cat_default_5, add_2161, layer_norm_75], Original ATen: [aten.cat, aten.add, aten.native_layer_norm]
        triton_red_fused_add_cat_native_layer_norm_196_xnumel = 102*s0
        triton_red_fused_add_cat_native_layer_norm_196.run(buf528, buf344, buf345, buf346, buf347, buf348, buf349, buf393, buf523, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b, triton_red_fused_add_cat_native_layer_norm_196_xnumel, 192, grid=grid(triton_red_fused_add_cat_native_layer_norm_196_xnumel), stream=stream0)
        del buf393
        buf529 = empty_strided_cuda((s0, 72, 104), (7488, 104, 1), torch.float16)
        # Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.bmm]
        triton_poi_fused_bmm_197_xnumel = 7488*s0
        triton_poi_fused_bmm_197.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w, buf529, triton_poi_fused_bmm_197_xnumel, grid=grid(triton_poi_fused_bmm_197_xnumel), stream=stream0)
        buf530 = empty_strided_cuda((s0, 104, 192), (19968, 192, 1), torch.float16)
        # Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.bmm]
        triton_poi_fused_bmm_198_xnumel = 19968*s0
        triton_poi_fused_bmm_198.run(buf528, buf530, triton_poi_fused_bmm_198_xnumel, grid=grid(triton_poi_fused_bmm_198_xnumel), stream=stream0)
        buf531 = empty_strided_cuda((s0, 72, 192), (13824, 192, 1), torch.float16)
        # Topologically Sorted Source Nodes: [matmul_1], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_199.run(buf529, buf530, buf531, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 72, 192, meta13), stream=stream0)
        buf535 = buf531; del buf531  # reuse
        # Topologically Sorted Source Nodes: [add_2174, layer_norm_76], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_200_xnumel = 72*s0
        triton_per_fused_add_native_layer_norm_200.run(buf535, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b, triton_per_fused_add_native_layer_norm_200_xnumel, 192, grid=grid(triton_per_fused_add_native_layer_norm_200_xnumel), stream=stream0)
        buf536 = buf513; del buf513  # reuse
        # Topologically Sorted Source Nodes: [linear_default_4], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_201.run(buf535, _FOLDED_CONST_permute_78, buf536, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf551 = buf432; del buf432  # reuse
        # Topologically Sorted Source Nodes: [contiguous_12, layer_norm_77, sigmoid_15, mul_1708, layer_norm_79], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_172.run(buf536, _FOLDED_CONST_cat_17, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b, buf551, s0, 768, grid=grid(s0), stream=stream0)
        buf556 = buf427; del buf427  # reuse
        # Topologically Sorted Source Nodes: [contiguous_13, layer_norm_78, sigmoid_16, mul_1713, layer_norm_80], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_173.run(buf536, _FOLDED_CONST_cat_17, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b, buf556, s0, 768, grid=grid(s0), stream=stream0)
        buf552 = buf415; del buf415  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_79, linear_76], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf551, _FOLDED_CONST_permute_79, buf552, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf569 = buf551; del buf551  # reuse
        # Topologically Sorted Source Nodes: [linear_76, layer_norm_81, sigmoid_17, mul_1730, layer_norm_83], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf552, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b, buf569, s0, 768, grid=grid(s0), stream=stream0)
        buf557 = buf552; del buf552  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_80, linear_77], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf556, _FOLDED_CONST_permute_80, buf557, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf574 = buf556; del buf556  # reuse
        # Topologically Sorted Source Nodes: [linear_77, layer_norm_82, sigmoid_18, mul_1735, layer_norm_84], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf557, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b, buf574, s0, 768, grid=grid(s0), stream=stream0)
        buf570 = empty_strided_cuda((s0, 4896), (4896, 1), torch.float16)
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf569, _FOLDED_CONST_permute_81, out=buf570)
        buf579 = buf570; del buf570  # reuse
        # Topologically Sorted Source Nodes: [linear_78, layer_norm_85], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_202.run(buf579, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, s0, 4896, grid=grid(s0), stream=stream0)
        buf575 = buf523; del buf523  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf574, _FOLDED_CONST_permute_82, out=buf575)
        buf576 = buf475; del buf475  # reuse
        buf577 = buf474; del buf474  # reuse
        # Topologically Sorted Source Nodes: [linear_79, layer_norm_86], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_177.run(buf575, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf576, buf577, s0, 9216, grid=grid(s0), stream=stream0)
        buf580 = buf438; del buf438  # reuse
        # Topologically Sorted Source Nodes: [bmm_2], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_203.run(buf528, buf579, buf580, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 192, 48, meta15), stream=stream0)
        buf585 = reinterpret_tensor(buf433, (s0, 192, 48), (9216, 48, 1), 0); del buf433  # reuse
        # Topologically Sorted Source Nodes: [add_2285, layer_norm_87], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_179_xnumel = 192*s0
        triton_per_fused_add_native_layer_norm_179.run(buf575, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf576, buf577, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, buf580, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b, buf585, triton_per_fused_add_native_layer_norm_179_xnumel, 48, grid=grid(triton_per_fused_add_native_layer_norm_179_xnumel), stream=stream0)
        buf586 = reinterpret_tensor(buf579, (s0, 102, 48), (4896, 48, 1), 0); del buf579  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_87, bmm_3], Original ATen: [aten.native_layer_norm, aten.bmm]
        triton_tem_fused_bmm_native_layer_norm_204.run(buf528, buf585, buf586, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 102, 48, meta17), stream=stream0)
        buf590 = reinterpret_tensor(buf586, (s0, 4896), (4896, 1), 0); del buf586  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_88], Original ATen: [aten.native_layer_norm]
        triton_red_fused_native_layer_norm_205.run(buf590, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b, s0, 4896, grid=grid(s0), stream=stream0)
        buf591 = buf449; del buf449  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf590, _FOLDED_CONST_permute_83, out=buf591)
        buf595 = buf453; del buf453  # reuse
        buf596 = buf577; del buf577  # reuse
        buf597 = buf576; del buf576  # reuse
        # Topologically Sorted Source Nodes: [linear_80, layer_norm_89, sigmoid_19, mul_1779, layer_norm_90], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182.run(buf591, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias, buf595, buf596, buf597, s0, 2048, grid=grid(s0), stream=stream0)
        buf599 = buf469; del buf469  # reuse
        buf603 = buf599; del buf599  # reuse
        # Topologically Sorted Source Nodes: [cat_default_4, layer_norm_91], Original ATen: [aten.cat, aten.native_layer_norm]
        triton_red_fused_cat_native_layer_norm_183.run(buf603, buf457, buf230, buf595, buf596, buf597, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b, s0, 6354, grid=grid(s0), stream=stream0)
        buf604 = buf477; del buf477  # reuse
        # Topologically Sorted Source Nodes: [linear_81], Original ATen: [aten.addmm]
        triton_poi_fused_addmm_184_xnumel = 6360*s0
        triton_poi_fused_addmm_184.run(buf603, buf604, triton_poi_fused_addmm_184_xnumel, grid=grid(triton_poi_fused_addmm_184_xnumel), stream=stream0)
        buf605 = buf468; del buf468  # reuse
        # Topologically Sorted Source Nodes: [linear_81], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_185.run(buf604, _FOLDED_CONST_constant_pad_nd_default_13, buf605, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 384, meta16), stream=stream0)
        buf609 = buf605; del buf605  # reuse
        # Topologically Sorted Source Nodes: [linear_81, layer_norm_92], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_186.run(buf609, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, s0, 384, grid=grid(s0), stream=stream0)
        buf610 = buf462; del buf462  # reuse
        # Topologically Sorted Source Nodes: [linear_81, layer_norm_92, linear_82], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_tem_fused_addmm_native_layer_norm_187.run(buf609, _FOLDED_CONST_permute_85, buf610, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 6354, meta10), stream=stream0)
        buf614 = buf473; del buf473  # reuse
        buf615 = buf597; del buf597  # reuse
        buf616 = buf596; del buf596  # reuse
        # Topologically Sorted Source Nodes: [linear_82, layer_norm_93, addcmul_1, layer_norm_94], Original ATen: [aten.addmm, aten.native_layer_norm, aten.addcmul]
        triton_red_fused_addcmul_addmm_native_layer_norm_188.run(buf610, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias, buf603, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b, buf614, buf615, buf616, s0, 6354, grid=grid(s0), stream=stream0)
        buf618 = buf604; del buf604  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_94, linear_83], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_poi_fused_addmm_native_layer_norm_189_xnumel = 6360*s0
        triton_poi_fused_addmm_native_layer_norm_189.run(buf614, buf615, buf616, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b, buf618, triton_poi_fused_addmm_native_layer_norm_189_xnumel, grid=grid(triton_poi_fused_addmm_native_layer_norm_189_xnumel), stream=stream0)
        buf619 = buf522; del buf522  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf618, _FOLDED_CONST_constant_pad_nd_default_11, out=buf619)
        buf623 = buf482; del buf482  # reuse
        buf627 = buf514; del buf514  # reuse
        # Topologically Sorted Source Nodes: [linear_83, layer_norm_95, sigmoid_20, mul_1806, layer_norm_96], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190.run(buf619, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b, buf623, buf627, s0, 3072, grid=grid(s0), stream=stream0)
        buf628 = buf536; del buf536  # reuse
        # Topologically Sorted Source Nodes: [linear_84], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf627, _FOLDED_CONST_permute_87, buf628, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf632 = buf509; del buf509  # reuse
        buf636 = buf505; del buf505  # reuse
        # Topologically Sorted Source Nodes: [linear_84, layer_norm_97, sigmoid_21, mul_1817, layer_norm_98], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf628, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b, buf632, buf636, s0, 1536, grid=grid(s0), stream=stream0)
        buf637 = buf619; del buf619  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf636, _FOLDED_CONST_permute_88, out=buf637)
        buf641 = buf637; del buf637  # reuse
        buf645 = buf641; del buf641  # reuse
        # Topologically Sorted Source Nodes: [linear_85, layer_norm_99, add_2379, layer_norm_100, sigmoid_22, mul_1832], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193.run(buf645, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias, buf627, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf646 = buf636; del buf636  # reuse
        # Topologically Sorted Source Nodes: [linear_86], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf645, _FOLDED_CONST_permute_89, buf646, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf650 = buf632; del buf632  # reuse
        buf654 = buf628; del buf628  # reuse
        # Topologically Sorted Source Nodes: [linear_86, layer_norm_101, sigmoid_23, mul_1843, layer_norm_102], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf646, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b, buf650, buf654, s0, 1536, grid=grid(s0), stream=stream0)
        buf655 = buf504; del buf504  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf654, _FOLDED_CONST_permute_90, out=buf655)
        buf659 = buf627; del buf627  # reuse
        buf663 = buf659; del buf659  # reuse
        # Topologically Sorted Source Nodes: [linear_87, layer_norm_103, add_2392, add_2417, layer_norm_104, sigmoid_24, mul_1858], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194.run(buf663, buf655, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias, buf645, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf664 = reinterpret_tensor(buf585, (s0, 9216), (9216, 1), 0); del buf585  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_104, sigmoid_24, mul_1858, linear_88], Original ATen: [aten.native_layer_norm, aten.sigmoid, aten.mul, aten.addmm]
        triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_195.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias, buf663, _FOLDED_CONST_permute_91, buf664, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 9216, meta12), stream=stream0)
        buf665 = empty_strided_cuda((s0, 19584), (19584, 1), torch.float16)
        buf669 = reinterpret_tensor(buf665, (s0, 102, 192), (19584, 192, 1), 0); del buf665  # reuse
        # Topologically Sorted Source Nodes: [cat_default_3, add_2440, layer_norm_105], Original ATen: [aten.cat, aten.add, aten.native_layer_norm]
        triton_per_fused_add_cat_native_layer_norm_206_xnumel = 102*s0
        triton_per_fused_add_cat_native_layer_norm_206.run(buf669, buf344, buf345, buf346, buf347, buf348, buf349, buf535, buf664, buf528, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b, triton_per_fused_add_cat_native_layer_norm_206_xnumel, 192, grid=grid(triton_per_fused_add_cat_native_layer_norm_206_xnumel), stream=stream0)
        buf670 = buf529; del buf529  # reuse
        # Topologically Sorted Source Nodes: [matmul_2], Original ATen: [aten.bmm]
        triton_poi_fused_bmm_197_xnumel = 7488*s0
        triton_poi_fused_bmm_197.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w, buf670, triton_poi_fused_bmm_197_xnumel, grid=grid(triton_poi_fused_bmm_197_xnumel), stream=stream0)
        buf671 = buf530; del buf530  # reuse
        # Topologically Sorted Source Nodes: [matmul_2], Original ATen: [aten.bmm]
        triton_poi_fused_bmm_198_xnumel = 19968*s0
        triton_poi_fused_bmm_198.run(buf669, buf671, triton_poi_fused_bmm_198_xnumel, grid=grid(triton_poi_fused_bmm_198_xnumel), stream=stream0)
        buf672 = buf535; del buf535  # reuse
        # Topologically Sorted Source Nodes: [matmul_2], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_199.run(buf670, buf671, buf672, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 72, 192, meta13), stream=stream0)
        del buf670
        del buf671
        buf676 = buf672; del buf672  # reuse
        # Topologically Sorted Source Nodes: [add_2453, layer_norm_106], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_200_xnumel = 72*s0
        triton_per_fused_add_native_layer_norm_200.run(buf676, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b, triton_per_fused_add_native_layer_norm_200_xnumel, 192, grid=grid(triton_per_fused_add_native_layer_norm_200_xnumel), stream=stream0)
        buf677 = buf654; del buf654  # reuse
        # Topologically Sorted Source Nodes: [linear_default_5], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_201.run(buf676, _FOLDED_CONST_permute_93, buf677, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf692 = buf574; del buf574  # reuse
        # Topologically Sorted Source Nodes: [contiguous_14, layer_norm_107, sigmoid_25, mul_1907, layer_norm_109], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_172.run(buf677, _FOLDED_CONST_cat_21, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b, buf692, s0, 768, grid=grid(s0), stream=stream0)
        buf697 = buf569; del buf569  # reuse
        # Topologically Sorted Source Nodes: [contiguous_15, layer_norm_108, sigmoid_26, mul_1912, layer_norm_110], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_173.run(buf677, _FOLDED_CONST_cat_21, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b, buf697, s0, 768, grid=grid(s0), stream=stream0)
        buf693 = buf557; del buf557  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_109, linear_91], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf692, _FOLDED_CONST_permute_94, buf693, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf710 = buf692; del buf692  # reuse
        # Topologically Sorted Source Nodes: [linear_91, layer_norm_111, sigmoid_27, mul_1929, layer_norm_113], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf693, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b, buf710, s0, 768, grid=grid(s0), stream=stream0)
        buf698 = buf693; del buf693  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_110, linear_92], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf697, _FOLDED_CONST_permute_95, buf698, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf715 = buf697; del buf697  # reuse
        # Topologically Sorted Source Nodes: [linear_92, layer_norm_112, sigmoid_28, mul_1934, layer_norm_114], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf698, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b, buf715, s0, 768, grid=grid(s0), stream=stream0)
        buf711 = buf590; del buf590  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf710, _FOLDED_CONST_permute_96, out=buf711)
        buf720 = buf711; del buf711  # reuse
        # Topologically Sorted Source Nodes: [linear_93, layer_norm_115], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_202.run(buf720, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, s0, 4896, grid=grid(s0), stream=stream0)
        buf716 = buf664; del buf664  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf715, _FOLDED_CONST_permute_97, out=buf716)
        buf717 = buf616; del buf616  # reuse
        buf718 = buf615; del buf615  # reuse
        # Topologically Sorted Source Nodes: [linear_94, layer_norm_116], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_177.run(buf716, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf717, buf718, s0, 9216, grid=grid(s0), stream=stream0)
        buf721 = buf580; del buf580  # reuse
        # Topologically Sorted Source Nodes: [bmm_4], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_203.run(buf669, buf720, buf721, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 192, 48, meta15), stream=stream0)
        buf726 = reinterpret_tensor(buf575, (s0, 192, 48), (9216, 48, 1), 0); del buf575  # reuse
        # Topologically Sorted Source Nodes: [add_2564, layer_norm_117], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_179_xnumel = 192*s0
        triton_per_fused_add_native_layer_norm_179.run(buf716, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf717, buf718, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, buf721, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b, buf726, triton_per_fused_add_native_layer_norm_179_xnumel, 48, grid=grid(triton_per_fused_add_native_layer_norm_179_xnumel), stream=stream0)
        buf727 = reinterpret_tensor(buf720, (s0, 102, 48), (4896, 48, 1), 0); del buf720  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_117, bmm_5], Original ATen: [aten.native_layer_norm, aten.bmm]
        triton_tem_fused_bmm_native_layer_norm_204.run(buf669, buf726, buf727, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 102, 48, meta17), stream=stream0)
        buf731 = reinterpret_tensor(buf727, (s0, 4896), (4896, 1), 0); del buf727  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_118], Original ATen: [aten.native_layer_norm]
        triton_red_fused_native_layer_norm_205.run(buf731, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b, s0, 4896, grid=grid(s0), stream=stream0)
        buf732 = buf591; del buf591  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf731, _FOLDED_CONST_permute_98, out=buf732)
        buf736 = buf595; del buf595  # reuse
        buf737 = buf718; del buf718  # reuse
        buf738 = buf717; del buf717  # reuse
        # Topologically Sorted Source Nodes: [linear_95, layer_norm_119, sigmoid_29, mul_1978, layer_norm_120], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182.run(buf732, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias, buf736, buf737, buf738, s0, 2048, grid=grid(s0), stream=stream0)
        buf740 = buf610; del buf610  # reuse
        buf744 = buf740; del buf740  # reuse
        # Topologically Sorted Source Nodes: [cat_default_2, layer_norm_121], Original ATen: [aten.cat, aten.native_layer_norm]
        triton_red_fused_cat_native_layer_norm_183.run(buf744, buf457, buf230, buf736, buf737, buf738, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b, s0, 6354, grid=grid(s0), stream=stream0)
        buf745 = buf618; del buf618  # reuse
        # Topologically Sorted Source Nodes: [linear_96], Original ATen: [aten.addmm]
        triton_poi_fused_addmm_184_xnumel = 6360*s0
        triton_poi_fused_addmm_184.run(buf744, buf745, triton_poi_fused_addmm_184_xnumel, grid=grid(triton_poi_fused_addmm_184_xnumel), stream=stream0)
        buf746 = buf609; del buf609  # reuse
        # Topologically Sorted Source Nodes: [linear_96], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_185.run(buf745, _FOLDED_CONST_constant_pad_nd_default_7, buf746, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 384, meta16), stream=stream0)
        buf750 = buf746; del buf746  # reuse
        # Topologically Sorted Source Nodes: [linear_96, layer_norm_122], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_186.run(buf750, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, s0, 384, grid=grid(s0), stream=stream0)
        buf751 = buf603; del buf603  # reuse
        # Topologically Sorted Source Nodes: [linear_96, layer_norm_122, linear_97], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_tem_fused_addmm_native_layer_norm_187.run(buf750, _FOLDED_CONST_permute_100, buf751, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 6354, meta10), stream=stream0)
        buf755 = buf614; del buf614  # reuse
        buf756 = buf738; del buf738  # reuse
        buf757 = buf737; del buf737  # reuse
        # Topologically Sorted Source Nodes: [linear_97, layer_norm_123, addcmul_2, layer_norm_124], Original ATen: [aten.addmm, aten.native_layer_norm, aten.addcmul]
        triton_red_fused_addcmul_addmm_native_layer_norm_188.run(buf751, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias, buf744, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b, buf755, buf756, buf757, s0, 6354, grid=grid(s0), stream=stream0)
        buf759 = buf745; del buf745  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_124, linear_98], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_poi_fused_addmm_native_layer_norm_189_xnumel = 6360*s0
        triton_poi_fused_addmm_native_layer_norm_189.run(buf755, buf756, buf757, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b, buf759, triton_poi_fused_addmm_native_layer_norm_189_xnumel, grid=grid(triton_poi_fused_addmm_native_layer_norm_189_xnumel), stream=stream0)
        buf760 = buf663; del buf663  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf759, _FOLDED_CONST_constant_pad_nd_default_5, out=buf760)
        buf764 = buf623; del buf623  # reuse
        buf768 = buf655; del buf655  # reuse
        # Topologically Sorted Source Nodes: [linear_98, layer_norm_125, sigmoid_30, mul_2005, layer_norm_126], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190.run(buf760, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b, buf764, buf768, s0, 3072, grid=grid(s0), stream=stream0)
        buf769 = buf677; del buf677  # reuse
        # Topologically Sorted Source Nodes: [linear_99], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf768, _FOLDED_CONST_permute_102, buf769, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf773 = buf650; del buf650  # reuse
        buf777 = buf646; del buf646  # reuse
        # Topologically Sorted Source Nodes: [linear_99, layer_norm_127, sigmoid_31, mul_2016, layer_norm_128], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf769, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b, buf773, buf777, s0, 1536, grid=grid(s0), stream=stream0)
        buf778 = buf760; del buf760  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf777, _FOLDED_CONST_permute_103, out=buf778)
        buf782 = buf778; del buf778  # reuse
        buf786 = buf782; del buf782  # reuse
        # Topologically Sorted Source Nodes: [linear_100, layer_norm_129, add_2658, layer_norm_130, sigmoid_32, mul_2031], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193.run(buf786, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias, buf768, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf787 = buf777; del buf777  # reuse
        # Topologically Sorted Source Nodes: [linear_101], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf786, _FOLDED_CONST_permute_104, buf787, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf791 = buf773; del buf773  # reuse
        buf795 = buf769; del buf769  # reuse
        # Topologically Sorted Source Nodes: [linear_101, layer_norm_131, sigmoid_33, mul_2042, layer_norm_132], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf787, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b, buf791, buf795, s0, 1536, grid=grid(s0), stream=stream0)
        buf796 = buf645; del buf645  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf795, _FOLDED_CONST_permute_105, out=buf796)
        buf800 = buf768; del buf768  # reuse
        buf804 = buf800; del buf800  # reuse
        # Topologically Sorted Source Nodes: [linear_102, layer_norm_133, add_2671, add_2696, layer_norm_134, sigmoid_34, mul_2057], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194.run(buf804, buf796, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias, buf786, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf805 = reinterpret_tensor(buf726, (s0, 9216), (9216, 1), 0); del buf726  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_134, sigmoid_34, mul_2057, linear_103], Original ATen: [aten.native_layer_norm, aten.sigmoid, aten.mul, aten.addmm]
        triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_195.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias, buf804, _FOLDED_CONST_permute_106, buf805, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 9216, meta12), stream=stream0)
        buf806 = reinterpret_tensor(buf528, (s0, 19584), (19584, 1), 0); del buf528  # reuse
        buf810 = reinterpret_tensor(buf806, (s0, 102, 192), (19584, 192, 1), 0); del buf806  # reuse
        # Topologically Sorted Source Nodes: [cat_default_1, add_2719, layer_norm_135], Original ATen: [aten.cat, aten.add, aten.native_layer_norm]
        triton_per_fused_add_cat_native_layer_norm_206_xnumel = 102*s0
        triton_per_fused_add_cat_native_layer_norm_206.run(buf810, buf344, buf345, buf346, buf347, buf348, buf349, buf676, buf805, buf669, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b, triton_per_fused_add_cat_native_layer_norm_206_xnumel, 192, grid=grid(triton_per_fused_add_cat_native_layer_norm_206_xnumel), stream=stream0)
        del buf344
        del buf345
        del buf346
        del buf347
        del buf348
        del buf349
        del buf388
        del buf669
        del buf676
        buf811 = empty_strided_cuda((s0, 24, 192), (4608, 192, 1), torch.float16)
        # Topologically Sorted Source Nodes: [matmul_3], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_207.run(_FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w, buf810, buf811, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 24, 192, meta18), stream=stream0)
        buf815 = buf811; del buf811  # reuse
        # Topologically Sorted Source Nodes: [add_2732, layer_norm_136], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_208_xnumel = 24*s0
        triton_per_fused_add_native_layer_norm_208.run(buf815, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b, triton_per_fused_add_native_layer_norm_208_xnumel, 192, grid=grid(triton_per_fused_add_native_layer_norm_208_xnumel), stream=stream0)
        buf816 = buf795; del buf795  # reuse
        # Topologically Sorted Source Nodes: [linear_default_6], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_209.run(buf815, _FOLDED_CONST_permute_108, buf816, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        del buf815
        buf831 = buf715; del buf715  # reuse
        # Topologically Sorted Source Nodes: [contiguous_16, layer_norm_137, sigmoid_35, mul_2095, layer_norm_139], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_172.run(buf816, _FOLDED_CONST_cat_25, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b, buf831, s0, 768, grid=grid(s0), stream=stream0)
        buf836 = buf710; del buf710  # reuse
        # Topologically Sorted Source Nodes: [contiguous_17, layer_norm_138, sigmoid_36, mul_2100, layer_norm_140], Original ATen: [aten.clone, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_clone_mul_native_layer_norm_sigmoid_173.run(buf816, _FOLDED_CONST_cat_25, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b, buf836, s0, 768, grid=grid(s0), stream=stream0)
        buf832 = buf698; del buf698  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_139, linear_106], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf831, _FOLDED_CONST_permute_109, buf832, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf849 = buf831; del buf831  # reuse
        # Topologically Sorted Source Nodes: [linear_106, layer_norm_141, sigmoid_37, mul_2117, layer_norm_143], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf832, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b, buf849, s0, 768, grid=grid(s0), stream=stream0)
        buf837 = buf832; del buf832  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_140, linear_107], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_tem_fused_addmm_native_layer_norm_174.run(buf836, _FOLDED_CONST_permute_110, buf837, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 768, meta12), stream=stream0)
        buf854 = buf836; del buf836  # reuse
        # Topologically Sorted Source Nodes: [linear_107, layer_norm_142, sigmoid_38, mul_2122, layer_norm_144], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_175.run(buf837, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b, buf854, s0, 768, grid=grid(s0), stream=stream0)
        del buf837
        buf850 = buf731; del buf731  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf849, _FOLDED_CONST_permute_111, out=buf850)
        del buf849
        buf859 = buf850; del buf850  # reuse
        # Topologically Sorted Source Nodes: [linear_108, layer_norm_145], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_202.run(buf859, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b, s0, 4896, grid=grid(s0), stream=stream0)
        buf855 = buf805; del buf805  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf854, _FOLDED_CONST_permute_112, out=buf855)
        del buf854
        buf856 = buf757; del buf757  # reuse
        buf857 = buf756; del buf756  # reuse
        # Topologically Sorted Source Nodes: [linear_109, layer_norm_146], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_red_fused_addmm_native_layer_norm_177.run(buf855, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf856, buf857, s0, 9216, grid=grid(s0), stream=stream0)
        buf860 = buf721; del buf721  # reuse
        # Topologically Sorted Source Nodes: [bmm_6], Original ATen: [aten.bmm]
        triton_tem_fused_bmm_203.run(buf810, buf859, buf860, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 192, 48, meta15), stream=stream0)
        buf865 = reinterpret_tensor(buf716, (s0, 192, 48), (9216, 48, 1), 0); del buf716  # reuse
        # Topologically Sorted Source Nodes: [add_2832, layer_norm_147], Original ATen: [aten.add, aten.native_layer_norm]
        triton_per_fused_add_native_layer_norm_179_xnumel = 192*s0
        triton_per_fused_add_native_layer_norm_179.run(buf855, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias, buf856, buf857, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b, buf860, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b, buf865, triton_per_fused_add_native_layer_norm_179_xnumel, 48, grid=grid(triton_per_fused_add_native_layer_norm_179_xnumel), stream=stream0)
        del buf855
        del buf860
        buf866 = reinterpret_tensor(buf859, (s0, 102, 48), (4896, 48, 1), 0); del buf859  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_147, bmm_7], Original ATen: [aten.native_layer_norm, aten.bmm]
        triton_tem_fused_bmm_native_layer_norm_204.run(buf810, buf865, buf866, grid=torch._inductor.kernel.bmm.bmm_grid(s0, 102, 48, meta17), stream=stream0)
        del buf810
        del buf865
        buf870 = reinterpret_tensor(buf866, (s0, 4896), (4896, 1), 0); del buf866  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_148], Original ATen: [aten.native_layer_norm]
        triton_red_fused_native_layer_norm_205.run(buf870, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b, s0, 4896, grid=grid(s0), stream=stream0)
        buf871 = buf732; del buf732  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf870, _FOLDED_CONST_permute_113, out=buf871)
        del buf870
        buf875 = buf736; del buf736  # reuse
        buf876 = buf857; del buf857  # reuse
        buf877 = buf856; del buf856  # reuse
        # Topologically Sorted Source Nodes: [linear_110, layer_norm_149, sigmoid_39, mul_2166, layer_norm_150], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_182.run(buf871, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias, buf875, buf876, buf877, s0, 2048, grid=grid(s0), stream=stream0)
        del buf871
        buf879 = buf751; del buf751  # reuse
        buf883 = buf879; del buf879  # reuse
        # Topologically Sorted Source Nodes: [cat_default, layer_norm_151], Original ATen: [aten.cat, aten.native_layer_norm]
        triton_red_fused_cat_native_layer_norm_183.run(buf883, buf457, buf230, buf875, buf876, buf877, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b, s0, 6354, grid=grid(s0), stream=stream0)
        del buf230
        del buf457
        del buf875
        buf884 = buf759; del buf759  # reuse
        # Topologically Sorted Source Nodes: [linear_111], Original ATen: [aten.addmm]
        triton_poi_fused_addmm_184_xnumel = 6360*s0
        triton_poi_fused_addmm_184.run(buf883, buf884, triton_poi_fused_addmm_184_xnumel, grid=grid(triton_poi_fused_addmm_184_xnumel), stream=stream0)
        buf885 = buf750; del buf750  # reuse
        # Topologically Sorted Source Nodes: [linear_111], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_185.run(buf884, _FOLDED_CONST_constant_pad_nd_default_3, buf885, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 384, meta16), stream=stream0)
        buf889 = buf885; del buf885  # reuse
        # Topologically Sorted Source Nodes: [linear_111, layer_norm_152], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_per_fused_addmm_native_layer_norm_186.run(buf889, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b, s0, 384, grid=grid(s0), stream=stream0)
        buf890 = buf744; del buf744  # reuse
        # Topologically Sorted Source Nodes: [linear_111, layer_norm_152, linear_112], Original ATen: [aten.addmm, aten.native_layer_norm]
        triton_tem_fused_addmm_native_layer_norm_187.run(buf889, _FOLDED_CONST_permute_115, buf890, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 6354, meta10), stream=stream0)
        del buf889
        buf894 = buf755; del buf755  # reuse
        buf895 = buf877; del buf877  # reuse
        buf896 = buf876; del buf876  # reuse
        # Topologically Sorted Source Nodes: [linear_112, layer_norm_153, addcmul_3, layer_norm_154], Original ATen: [aten.addmm, aten.native_layer_norm, aten.addcmul]
        triton_red_fused_addcmul_addmm_native_layer_norm_188.run(buf890, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias, buf883, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b, buf894, buf895, buf896, s0, 6354, grid=grid(s0), stream=stream0)
        del buf883
        del buf890
        buf898 = buf884; del buf884  # reuse
        # Topologically Sorted Source Nodes: [layer_norm_154, linear_113], Original ATen: [aten.native_layer_norm, aten.addmm]
        triton_poi_fused_addmm_native_layer_norm_189_xnumel = 6360*s0
        triton_poi_fused_addmm_native_layer_norm_189.run(buf894, buf895, buf896, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b, buf898, triton_poi_fused_addmm_native_layer_norm_189_xnumel, grid=grid(triton_poi_fused_addmm_native_layer_norm_189_xnumel), stream=stream0)
        del buf894
        del buf895
        buf899 = buf804; del buf804  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf898, _FOLDED_CONST_constant_pad_nd_default_1, out=buf899)
        del buf898
        buf903 = buf764; del buf764  # reuse
        buf907 = buf796; del buf796  # reuse
        # Topologically Sorted Source Nodes: [linear_113, layer_norm_155, sigmoid_40, mul_2193, layer_norm_156], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_190.run(buf899, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b, buf903, buf907, s0, 3072, grid=grid(s0), stream=stream0)
        del buf903
        buf908 = buf816; del buf816  # reuse
        # Topologically Sorted Source Nodes: [linear_114], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf907, _FOLDED_CONST_permute_117, buf908, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf912 = buf791; del buf791  # reuse
        buf916 = buf787; del buf787  # reuse
        # Topologically Sorted Source Nodes: [linear_114, layer_norm_157, sigmoid_41, mul_2204, layer_norm_158], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf908, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b, buf912, buf916, s0, 1536, grid=grid(s0), stream=stream0)
        buf917 = buf899; del buf899  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf916, _FOLDED_CONST_permute_118, out=buf917)
        buf921 = buf917; del buf917  # reuse
        buf925 = buf921; del buf921  # reuse
        # Topologically Sorted Source Nodes: [linear_115, layer_norm_159, add_2926, layer_norm_160, sigmoid_42, mul_2219], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_193.run(buf925, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias, buf907, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf926 = buf916; del buf916  # reuse
        # Topologically Sorted Source Nodes: [linear_116], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_191.run(buf925, _FOLDED_CONST_permute_119, buf926, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1536, meta14), stream=stream0)
        buf930 = buf912; del buf912  # reuse
        buf934 = buf908; del buf908  # reuse
        # Topologically Sorted Source Nodes: [linear_116, layer_norm_161, sigmoid_43, mul_2230, layer_norm_162], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_red_fused_addmm_mul_native_layer_norm_sigmoid_192.run(buf926, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b, buf930, buf934, s0, 1536, grid=grid(s0), stream=stream0)
        del buf926
        del buf930
        buf935 = buf786; del buf786  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf934, _FOLDED_CONST_permute_120, out=buf935)
        del buf934
        buf939 = buf907; del buf907  # reuse
        buf943 = buf939; del buf939  # reuse
        # Topologically Sorted Source Nodes: [linear_117, layer_norm_163, add_2939, add_2964, layer_norm_164, sigmoid_44, mul_2245], Original ATen: [aten.addmm, aten.native_layer_norm, aten.add, aten.sigmoid, aten.mul]
        triton_red_fused_add_addmm_mul_native_layer_norm_sigmoid_194.run(buf943, buf935, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias, buf925, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias, s0, 3072, grid=grid(s0), stream=stream0)
        buf944 = buf170; del buf170  # reuse
        # Topologically Sorted Source Nodes: [linear_118], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_210.run(buf943, _FOLDED_CONST_permute_121, buf944, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 512, meta19), stream=stream0)
        buf945 = buf944; del buf944  # reuse
        # Topologically Sorted Source Nodes: [linear_118, relu_default_1, nan_to_num_default_1], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num]
        triton_poi_fused_addmm_nan_to_num_relu_211_xnumel = 512*s0
        triton_poi_fused_addmm_nan_to_num_relu_211.run(buf945, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b, triton_poi_fused_addmm_nan_to_num_relu_211_xnumel, grid=grid(triton_poi_fused_addmm_nan_to_num_relu_211_xnumel), stream=stream0)
        buf946 = buf935; del buf935  # reuse
        # Topologically Sorted Source Nodes: [linear_118, relu_default_1, nan_to_num_default_1, linear_119], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num]
        triton_tem_fused_addmm_nan_to_num_relu_212.run(buf945, _FOLDED_CONST_permute_122, buf946, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 3072, meta10), stream=stream0)
        buf947 = buf946; del buf946  # reuse
        # Topologically Sorted Source Nodes: [linear_119, relu_default_2, nan_to_num_default_2, add_2989], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num, aten.add]
        triton_poi_fused_add_addmm_nan_to_num_relu_213_xnumel = 3072*s0
        triton_poi_fused_add_addmm_nan_to_num_relu_213.run(buf947, buf943, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b, triton_poi_fused_add_addmm_nan_to_num_relu_213_xnumel, grid=grid(triton_poi_fused_add_addmm_nan_to_num_relu_213_xnumel), stream=stream0)
        buf948 = buf945; del buf945  # reuse
        # Topologically Sorted Source Nodes: [linear_120], Original ATen: [aten.addmm]
        triton_tem_fused_addmm_210.run(buf947, _FOLDED_CONST_permute_123, buf948, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 512, meta19), stream=stream0)
        buf949 = buf948; del buf948  # reuse
        # Topologically Sorted Source Nodes: [linear_120, relu_default_3, nan_to_num_default_3], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num]
        triton_poi_fused_addmm_nan_to_num_relu_211_xnumel = 512*s0
        triton_poi_fused_addmm_nan_to_num_relu_211.run(buf949, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b, triton_poi_fused_addmm_nan_to_num_relu_211_xnumel, grid=grid(triton_poi_fused_addmm_nan_to_num_relu_211_xnumel), stream=stream0)
        buf950 = buf925; del buf925  # reuse
        # Topologically Sorted Source Nodes: [linear_120, relu_default_3, nan_to_num_default_3, linear_121], Original ATen: [aten.addmm, aten.relu, aten.nan_to_num]
        triton_tem_fused_addmm_nan_to_num_relu_212.run(buf949, _FOLDED_CONST_permute_124, buf950, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 3072, meta10), stream=stream0)
        buf951 = buf943; del buf943  # reuse
        # Topologically Sorted Source Nodes: [full_like, mul_2264, add_3008, linear_121, relu_default_4, nan_to_num_default_4, mul_2273, add_3018], Original ATen: [aten.full_like, aten.mul, aten.add, aten.addmm, aten.relu, aten.nan_to_num]
        triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214_xnumel = 3072*s0
        triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214.run(buf951, buf947, buf950, _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b, triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214_xnumel, grid=grid(triton_poi_fused_add_addmm_full_like_mul_nan_to_num_relu_214_xnumel), stream=stream0)
        del buf947
        buf952 = buf950; del buf950  # reuse
        # Unsorted Source Nodes: [], Original ATen: []
        extern_kernels.mm(buf951, _FOLDED_CONST_permute_125, out=buf952)
        buf953 = buf951; del buf951  # reuse
        # Topologically Sorted Source Nodes: [linear_122, sigmoid_45, mul_2282], Original ATen: [aten.addmm, aten.sigmoid, aten.mul]
        triton_poi_fused_addmm_mul_sigmoid_215_xnumel = 3072*s0
        triton_poi_fused_addmm_mul_sigmoid_215.run(buf953, buf952, _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b, triton_poi_fused_addmm_mul_sigmoid_215_xnumel, grid=grid(triton_poi_fused_addmm_mul_sigmoid_215_xnumel), stream=stream0)
        del buf952
        buf954 = buf949; del buf949  # reuse
        # Topologically Sorted Source Nodes: [linear_122, sigmoid_45, mul_2282, linear_123], Original ATen: [aten.addmm, aten.sigmoid, aten.mul]
        triton_tem_fused_addmm_210.run(buf953, _FOLDED_CONST_permute_126, buf954, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 512, meta19), stream=stream0)
        del buf953
        buf958 = buf954; del buf954  # reuse
        # Topologically Sorted Source Nodes: [linear_123, layer_norm_165, sigmoid_46, mul_2291], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_per_fused_addmm_mul_native_layer_norm_sigmoid_216.run(buf958, _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b, _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale, _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias, s0, 512, grid=grid(s0), stream=stream0)
        buf959 = empty_strided_cuda((s0, 1), (1, 1), torch.float16)
        # Topologically Sorted Source Nodes: [linear_123, layer_norm_165, sigmoid_46, mul_2291, linear_124], Original ATen: [aten.addmm, aten.native_layer_norm, aten.sigmoid, aten.mul]
        triton_tem_fused_addmm_mul_native_layer_norm_sigmoid_217.run(buf958, _FOLDED_CONST_permute_127, buf959, s0, grid=torch._inductor.kernel.mm_common.mm_grid(s0, 1, meta20), stream=stream0)
        del buf958
        buf966 = reinterpret_tensor(buf896, (s0, 1), (1, 1), 0); del buf896  # reuse
        # Topologically Sorted Source Nodes: [submod_1, linear_124, sum_3, sum_5, add_605, sum_10, add_708, sum_11, add_712, sum_9, add_722, sum_7, add_735, sum_6, add_742, sum_4, add_749, sum_8, add_762, sum_12, add_772, sum_2, add_789, sum_1, add_804, add_808], Original ATen: [aten.full_like, aten.addmm, aten.add, aten.sigmoid, aten.gt, aten._to_copy, aten.mul, aten.sub, aten.logit, aten.sum]
        triton_poi_fused__to_copy_add_addmm_full_like_gt_logit_mul_sigmoid_sub_sum_218.run(buf2, buf4, buf959, _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b, _FOLDED_CONST_submod_1_main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_positive_weight_calibration_bias, _FOLDED_CONST_submod_0_main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias, _FOLDED_CONST_submod_1__tensor_constant1, buf966, s0, grid=grid(s0), stream=stream0)
        del buf2
        del buf4
        del buf959

    for kernel in globals().values():
        if isinstance(kernel, torch._inductor.runtime.triton_heuristics.CachingAutotuner):
            if not kernel.cuda_kernel_saved:
                if len(kernel.launchers) == 0:
                    kernel.precompile()
                kernel.save_gpu_kernel(
                    grid=(0, 0, 0),   # use dummy grid
                    stream="stream",  # use dummy stream
                    launcher=kernel.launchers[0],
                )
    return (buf966, _FOLDED_CONST__tensor_constant2, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 240), (240, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 240), (240, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_2_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 192), (192, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_1_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 192), (192, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADPUBLISHER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_AD_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_ENTITY_EQUIVALENCE_KEY_0_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_291594492_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_421801413_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_2_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_323876380_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_368273801_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_3_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246015958_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_487599076_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_246272433_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_EARLY_STAGE_FEATURES_EARLY_STAGE_SCALING_USER_MODEL_367428337_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_UDS_UHM_ONSITE_CONVERSION_UHM_ONSITE_CONVERSION_SINGLE_CHANNEL_CTR_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_343512182_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_F3_BULK_EVAL_DAILY_USER_SIDE_EMBEDDING_FEATURE_GRAPH_LEARNING_EMBEDDING_FEATURE_USER_GRAPH_F3_DAILY_BULK_EVAL_2023_H1_2_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 72), (72, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_309862198_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_360324426_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 144), (144, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_0_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 144), (144, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_GRAPH_LEARNING_FLEXIBLE_BATCH_GRAPH_LEARNING_USER_SIDE_EMBEDDING_FEATURE_F3_GRAPH_LEARNING_EMBEDDING_FEATURE_SEPARABLE_ID_1_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_PROD_FEATURES_SCALING_USER_MODEL_346472987_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_POST_MODEL_EVAL_ADS_SCALING_USER_MODEL_REALTIME_CVR_V0_SCALING_USER_MODEL_510272006_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_0_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((192, 96), (96, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_1_scale
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_1_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_F3_ADFINDER_USER_ADS_SCALING_USER_MODEL_SG_SCALE_V0_EARLY_STAGE_SCALING_USER_MODEL_530332232_EMBEDDING_1_AVG_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((214, 6052), (6052, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((214, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_w = rand_strided((42, 6052), (6052, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_0_shards_1_b = rand_strided((42, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_w = rand_strided((3026, 256), (256, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b = rand_strided((3026, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w = rand_strided((256, 3026), (3026, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = rand_strided((3282, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = rand_strided((3282, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_w = rand_strided((192, 3282), (3282, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_w = rand_strided((192, 3282), (3282, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_1_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_w = rand_strided((192, 3282), (3282, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_2_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_w = rand_strided((192, 3282), (3282, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_3_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_w = rand_strided((192, 3282), (3282, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_4_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_w = rand_strided((192, 3282), (3282, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_embedding_archs_0_submodules_dense_embedding_5_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((256, 700), (700, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_w = rand_strided((1024, 256), (256, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_w = rand_strided((256, 700), (700, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_w = rand_strided((960, 256), (256, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_w = rand_strided((1536, 960), (960, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb = rand_strided((1, 200, 64), (12800, 64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb = rand_strided((1, 32, 64), (2048, 64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight = rand_strided((64, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight = rand_strided((64, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_weight = rand_strided((128, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_weight = rand_strided((64, 128), (128, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb = rand_strided((1, 200, 64), (12800, 64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb = rand_strided((1, 32, 64), (2048, 64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_k_proj_weight = rand_strided((64, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_weight = rand_strided((64, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_weight = rand_strided((128, 64), (64, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_weight
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_weight = rand_strided((64, 128), (128, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias
    submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias = rand_strided((64, 1, 192), (192, 192, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight = rand_strided((64, 64, 192), (12288, 192, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w = rand_strided((174, 458), (458, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b = rand_strided((174, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = rand_strided((21984, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = rand_strided((9216, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_weight = rand_strided((2048, 21984), (21984, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = rand_strided((384, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_weight = rand_strided((6354, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_weight = rand_strided((3072, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_weight = rand_strided((9216, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w = rand_strided((72, 102), (102, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b = rand_strided((72, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = rand_strided((4896, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = rand_strided((9216, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_weight = rand_strided((2048, 4896), (4896, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = rand_strided((384, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_weight = rand_strided((6354, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_weight = rand_strided((3072, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_weight = rand_strided((9216, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w = rand_strided((72, 102), (102, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b = rand_strided((72, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = rand_strided((4896, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = rand_strided((9216, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_weight = rand_strided((2048, 4896), (4896, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = rand_strided((384, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_weight = rand_strided((6354, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_weight = rand_strided((3072, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_weight = rand_strided((9216, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w = rand_strided((24, 102), (102, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b = rand_strided((24, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_weight = rand_strided((768, 4608), (4608, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_weight = rand_strided((768, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_weight = rand_strided((4896, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_weight = rand_strided((9216, 768), (768, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_weight = rand_strided((2048, 4896), (4896, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_weight = rand_strided((384, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_weight = rand_strided((6354, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_weight = rand_strided((3072, 6354), (6354, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_weight = rand_strided((1536, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_weight = rand_strided((3072, 1536), (1536, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias
    submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_w = rand_strided((512, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_w = rand_strided((3072, 512), (512, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_w = rand_strided((512, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_w
    submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_w = rand_strided((3072, 512), (512, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b
    submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_dependent_tasks_1_SALR_STANDALONE_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias
    submod_0_main_module_impl_impl_dependent_tasks_1_SALR_STANDALONE_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_w
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_w = rand_strided((3072, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_w = rand_strided((512, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_w
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_w = rand_strided((1, 512), (512, 1), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias
    submod_0_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _tensor_constant2
    _tensor_constant2 = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global submod_0_cat_fusion_gpu__offset_dim_list
    submod_0_cat_fusion_gpu__offset_dim_list = rand_strided((277, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_0_cat_fusion_gpu__permute
    submod_0_cat_fusion_gpu__permute = rand_strided((276, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_0_cat_fusion_gpu__inv_permute
    submod_0_cat_fusion_gpu__inv_permute = rand_strided((276, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_0_cat_fusion_gpu__inv_offset_dim_list
    submod_0_cat_fusion_gpu__inv_offset_dim_list = rand_strided((277, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_0_cat_fusion_cpu__offset_dim_list
    submod_0_cat_fusion_cpu__offset_dim_list = rand_strided((183, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_0_cat_fusion_cpu__permute
    submod_0_cat_fusion_cpu__permute = rand_strided((182, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_0_cat_fusion_cpu__inv_permute
    submod_0_cat_fusion_cpu__inv_permute = rand_strided((182, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_0_cat_fusion_cpu__inv_offset_dim_list
    submod_0_cat_fusion_cpu__inv_offset_dim_list = rand_strided((183, ), (1, ), device='cuda:0', dtype=torch.int64)
    global submod_1__tensor_constant1
    submod_1__tensor_constant1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
    global submod_1_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_calibration_positive_weight_calibration_bias
    submod_1_main_module_impl_impl_task_archs_1_Optimized_prediction_arch_calibration_positive_weight_calibration_bias = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_2_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_1_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adpublisher_ads_graph_learning_flexible_batch_graph_learning_ad_side_embedding_feature_f3_graph_learning_embedding_feature_entity_equivalence_key_0_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_291594492_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_421801413_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_2_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_323876380_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_368273801_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_3_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246015958_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_487599076_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_246272433_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_early_stage_features_early_stage_scaling_user_model_367428337_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_uds_uhm_onsite_conversion_uhm_onsite_conversion_single_channel_ctr_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_343512182_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_f3_bulk_eval_daily_user_side_embedding_feature_graph_learning_embedding_feature_user_graph_f3_daily_bulk_eval_2023_h1_2_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_309862198_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_360324426_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_0_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_graph_learning_flexible_batch_graph_learning_user_side_embedding_feature_f3_graph_learning_embedding_feature_separable_id_1_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_prod_features_scaling_user_model_346472987_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_post_model_eval_ads_scaling_user_model_realtime_cvr_v0_scaling_user_model_510272006_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_0_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_0_arch_submodules_0_submodules_0_shards_0_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_scale = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_feature_arch_embedding_projection_arch_f3_adfinder_user_ads_scaling_user_model_sg_scale_v0_early_stage_scaling_user_model_530332232_embedding_1_avg_3_submodules_1_bias = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_1_shards_0_b = rand_strided((3026, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = rand_strided((3282, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = rand_strided((3282, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_scale = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_0_submodules_1_norm_submodules_0_bias = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_0_shards_0_b = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_scale = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_submodules_0_arch_submodules_1_submodules_1_norm_submodules_0_bias = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_0_arch_submodules_1_submodules_0_shards_0_b = rand_strided((960, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_specialized_arch_specialized_module_list_0_specialized_arch_to_dot_submodules_1_linear_arch_shards_0_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_pos_emb = rand_strided((1, 200, 64), (12800, 64, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_seed_emb = rand_strided((1, 32, 64), (2048, 64, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_y_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_x_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_0_bias = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_iaw_relevance_model_pre_norm_pma_mab_mlps_0_2_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_pos_emb = rand_strided((1, 200, 64), (12800, 64, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_seed_emb = rand_strided((1, 32, 64), (2048, 64, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_y_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_attns_0_q_proj_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_x_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_weight = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_ln_ffn_0_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_0_bias = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_event_submodels_dict_user_conv_ads_event_relevance_model_pre_norm_pma_mab_mlps_0_2_bias = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_bias = rand_strided((64, 1, 192), (192, 192, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_embedding_projection_arch_first_fused_mlp_0_mlp_net_0_weight = rand_strided((64, 64, 192), (12288, 192, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_w = rand_strided((174, 458), (458, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__compression_b = rand_strided((174, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__fused_lce_module__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_w = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_ln__init_b = rand_strided((21984, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__snn_projection_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_0__ln_on_dhen_layer__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_w = rand_strided((72, 102), (102, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__compression_b = rand_strided((72, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__fused_lce_module__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_ln__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__snn_projection_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_1__ln_on_dhen_layer__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_w = rand_strided((72, 102), (102, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__compression_b = rand_strided((72, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__fused_lce_module__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_ln__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__snn_projection_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_2__ln_on_dhen_layer__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_w = rand_strided((24, 102), (102, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__compression_b = rand_strided((24, 1), (1, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_w = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__input_compression__ln_lce__init_b = rand_strided((192, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_1_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_2__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_3_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_weight = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_4_norm_0_bias = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_w = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_mlp_mlp_net_5__init_b = rand_strided((768, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_0_bias = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__weight_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_0_bias = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_w = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp__resnet_arch_post_match_mlp_mlp_net_1__init_b = rand_strided((9216, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_w = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcpp_compressed_tensor_ln__init_b = rand_strided((48, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_w = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_ln__init_b = rand_strided((4896, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_weight = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_1_norm_0_bias = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_w = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcpp_fc__mlps_0_mlp_net_2__init_b = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__ln_on_dsi__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_0_bias = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_w = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_low_rank_mlps_0_mlp_net_1__init_b = rand_strided((384, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_0_bias = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__dcn__dcn_match_mlps_0_mlp_net_1__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_w = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__post_dcn_ln__init_b = rand_strided((6354, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_1_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_0_mlp_net_2__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_1_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_2_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_weight = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_1_norm_0_bias = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_w = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_3_mlp_net_2__init_b = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_w = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__mlps_4_mlp_net_1__init_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_2_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_weight = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_pytorch_dhen_dhen_arch_layers_3__residual_mlp__residual_activation_4_norm_0_bias = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_0_shards_0_b = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_encoder_linear_archs_1_shards_0_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_0_shards_0_b = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_shared_arch_cyclegan_decoder_linear_archs_1_shards_0_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_dependent_tasks_1_salr_standalone_aggregator_module_task_arch_sparse_aggregates_logistic_regression_global_bias = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_gating_archs_0_gn_arch_submodules_0_shards_0_b = rand_strided((3072, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_0_shards_0_b = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b
    _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_linear_archs_1_shards_0_b = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale
    _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_scale = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias
    _FOLDED_CONST_submod_0_main_module_impl_impl_task_archs_1_optimized_prediction_arch_dense_arch_dense_projection_arch_activations_0_norm_submodules_0_bias = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST__tensor_constant2
    _FOLDED_CONST__tensor_constant2 = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_0_cat_fusion_gpu__offset_dim_list
    _FOLDED_CONST_submod_0_cat_fusion_gpu__offset_dim_list = rand_strided((277, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_0_cat_fusion_gpu__permute
    _FOLDED_CONST_submod_0_cat_fusion_gpu__permute = rand_strided((276, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_0_cat_fusion_gpu__inv_permute
    _FOLDED_CONST_submod_0_cat_fusion_gpu__inv_permute = rand_strided((276, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_0_cat_fusion_gpu__inv_offset_dim_list
    _FOLDED_CONST_submod_0_cat_fusion_gpu__inv_offset_dim_list = rand_strided((277, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_0_cat_fusion_cpu__offset_dim_list
    _FOLDED_CONST_submod_0_cat_fusion_cpu__offset_dim_list = rand_strided((183, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_0_cat_fusion_cpu__permute
    _FOLDED_CONST_submod_0_cat_fusion_cpu__permute = rand_strided((182, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_0_cat_fusion_cpu__inv_permute
    _FOLDED_CONST_submod_0_cat_fusion_cpu__inv_permute = rand_strided((182, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_0_cat_fusion_cpu__inv_offset_dim_list
    _FOLDED_CONST_submod_0_cat_fusion_cpu__inv_offset_dim_list = rand_strided((183, ), (1, ), device='cuda:0', dtype=torch.int64)
    global _FOLDED_CONST_submod_1__tensor_constant1
    _FOLDED_CONST_submod_1__tensor_constant1 = rand_strided((), (), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_submod_1_main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_positive_weight_calibration_bias
    _FOLDED_CONST_submod_1_main_module_impl_impl_task_archs_1_optimized_prediction_arch_calibration_positive_weight_calibration_bias = rand_strided((1, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute
    _FOLDED_CONST_permute = rand_strided((240, 192), (1, 240), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_1
    _FOLDED_CONST_permute_1 = rand_strided((240, 192), (1, 240), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_2
    _FOLDED_CONST_permute_2 = rand_strided((192, 192), (1, 192), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_3
    _FOLDED_CONST_permute_3 = rand_strided((192, 192), (1, 192), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_4
    _FOLDED_CONST_permute_4 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_5
    _FOLDED_CONST_permute_5 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_6
    _FOLDED_CONST_permute_6 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_7
    _FOLDED_CONST_permute_7 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_8
    _FOLDED_CONST_permute_8 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_9
    _FOLDED_CONST_permute_9 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_10
    _FOLDED_CONST_permute_10 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_11
    _FOLDED_CONST_permute_11 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_12
    _FOLDED_CONST_permute_12 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_13
    _FOLDED_CONST_permute_13 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_14
    _FOLDED_CONST_permute_14 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_15
    _FOLDED_CONST_permute_15 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_16
    _FOLDED_CONST_permute_16 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_17
    _FOLDED_CONST_permute_17 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_18
    _FOLDED_CONST_permute_18 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_19
    _FOLDED_CONST_permute_19 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_20
    _FOLDED_CONST_permute_20 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_21
    _FOLDED_CONST_permute_21 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_22
    _FOLDED_CONST_permute_22 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_23
    _FOLDED_CONST_permute_23 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_24
    _FOLDED_CONST_permute_24 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_25
    _FOLDED_CONST_permute_25 = rand_strided((72, 192), (1, 72), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_26
    _FOLDED_CONST_permute_26 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_27
    _FOLDED_CONST_permute_27 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_28
    _FOLDED_CONST_permute_28 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_29
    _FOLDED_CONST_permute_29 = rand_strided((144, 192), (1, 144), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_30
    _FOLDED_CONST_permute_30 = rand_strided((144, 192), (1, 144), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_31
    _FOLDED_CONST_permute_31 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_32
    _FOLDED_CONST_permute_32 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_33
    _FOLDED_CONST_permute_33 = rand_strided((64, 192), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_34
    _FOLDED_CONST_permute_34 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_35
    _FOLDED_CONST_permute_35 = rand_strided((96, 192), (1, 96), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_cat_3
    _FOLDED_CONST_cat_3 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_23
    _FOLDED_CONST_constant_pad_nd_default_23 = rand_strided((6056, 256), (256, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_cat_6
    _FOLDED_CONST_cat_6 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_37
    _FOLDED_CONST_permute_37 = rand_strided((700, 512), (1, 700), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_40
    _FOLDED_CONST_permute_40 = rand_strided((256, 3026), (1, 256), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_41
    _FOLDED_CONST_permute_41 = rand_strided((3026, 256), (1, 3026), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_44
    _FOLDED_CONST_permute_44 = rand_strided((256, 1024), (1, 256), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_45
    _FOLDED_CONST_permute_45 = rand_strided((64, 64), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_42
    _FOLDED_CONST_permute_42 = rand_strided((64, 64), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_46
    _FOLDED_CONST_permute_46 = rand_strided((64, 64), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_43
    _FOLDED_CONST_permute_43 = rand_strided((64, 64), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_57
    _FOLDED_CONST_permute_57 = rand_strided((64, 128), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_59
    _FOLDED_CONST_permute_59 = rand_strided((128, 64), (1, 128), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_cat_9
    _FOLDED_CONST_cat_9 = rand_strided((1152, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_21
    _FOLDED_CONST_constant_pad_nd_default_21 = rand_strided((3288, 1152), (1152, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_38
    _FOLDED_CONST_permute_38 = rand_strided((256, 960), (1, 256), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_39
    _FOLDED_CONST_permute_39 = rand_strided((960, 1536), (1, 960), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_56
    _FOLDED_CONST_permute_56 = rand_strided((64, 128), (1, 64), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_58
    _FOLDED_CONST_permute_58 = rand_strided((128, 64), (1, 128), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_cat_13
    _FOLDED_CONST_cat_13 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_63
    _FOLDED_CONST_permute_63 = rand_strided((4608, 1536), (1, 4608), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_64
    _FOLDED_CONST_permute_64 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_65
    _FOLDED_CONST_permute_65 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_66
    _FOLDED_CONST_permute_66 = rand_strided((768, 21984), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_67
    _FOLDED_CONST_permute_67 = rand_strided((768, 9216), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_68
    _FOLDED_CONST_permute_68 = rand_strided((21984, 2048), (1, 21984), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_19
    _FOLDED_CONST_constant_pad_nd_default_19 = rand_strided((6360, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_70
    _FOLDED_CONST_permute_70 = rand_strided((384, 6354), (1, 384), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_17
    _FOLDED_CONST_constant_pad_nd_default_17 = rand_strided((6360, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_72
    _FOLDED_CONST_permute_72 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_73
    _FOLDED_CONST_permute_73 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_74
    _FOLDED_CONST_permute_74 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_75
    _FOLDED_CONST_permute_75 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_76
    _FOLDED_CONST_permute_76 = rand_strided((3072, 9216), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_cat_17
    _FOLDED_CONST_cat_17 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_78
    _FOLDED_CONST_permute_78 = rand_strided((4608, 1536), (1, 4608), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_79
    _FOLDED_CONST_permute_79 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_80
    _FOLDED_CONST_permute_80 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_81
    _FOLDED_CONST_permute_81 = rand_strided((768, 4896), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_82
    _FOLDED_CONST_permute_82 = rand_strided((768, 9216), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_83
    _FOLDED_CONST_permute_83 = rand_strided((4896, 2048), (1, 4896), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_13
    _FOLDED_CONST_constant_pad_nd_default_13 = rand_strided((6360, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_85
    _FOLDED_CONST_permute_85 = rand_strided((384, 6354), (1, 384), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_11
    _FOLDED_CONST_constant_pad_nd_default_11 = rand_strided((6360, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_87
    _FOLDED_CONST_permute_87 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_88
    _FOLDED_CONST_permute_88 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_89
    _FOLDED_CONST_permute_89 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_90
    _FOLDED_CONST_permute_90 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_91
    _FOLDED_CONST_permute_91 = rand_strided((3072, 9216), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_cat_21
    _FOLDED_CONST_cat_21 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_93
    _FOLDED_CONST_permute_93 = rand_strided((4608, 1536), (1, 4608), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_94
    _FOLDED_CONST_permute_94 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_95
    _FOLDED_CONST_permute_95 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_96
    _FOLDED_CONST_permute_96 = rand_strided((768, 4896), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_97
    _FOLDED_CONST_permute_97 = rand_strided((768, 9216), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_98
    _FOLDED_CONST_permute_98 = rand_strided((4896, 2048), (1, 4896), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_7
    _FOLDED_CONST_constant_pad_nd_default_7 = rand_strided((6360, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_100
    _FOLDED_CONST_permute_100 = rand_strided((384, 6354), (1, 384), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_5
    _FOLDED_CONST_constant_pad_nd_default_5 = rand_strided((6360, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_102
    _FOLDED_CONST_permute_102 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_103
    _FOLDED_CONST_permute_103 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_104
    _FOLDED_CONST_permute_104 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_105
    _FOLDED_CONST_permute_105 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_106
    _FOLDED_CONST_permute_106 = rand_strided((3072, 9216), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_cat_25
    _FOLDED_CONST_cat_25 = rand_strided((1536, ), (1, ), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_108
    _FOLDED_CONST_permute_108 = rand_strided((4608, 1536), (1, 4608), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_109
    _FOLDED_CONST_permute_109 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_110
    _FOLDED_CONST_permute_110 = rand_strided((768, 768), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_111
    _FOLDED_CONST_permute_111 = rand_strided((768, 4896), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_112
    _FOLDED_CONST_permute_112 = rand_strided((768, 9216), (1, 768), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_113
    _FOLDED_CONST_permute_113 = rand_strided((4896, 2048), (1, 4896), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_3
    _FOLDED_CONST_constant_pad_nd_default_3 = rand_strided((6360, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_115
    _FOLDED_CONST_permute_115 = rand_strided((384, 6354), (1, 384), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_constant_pad_nd_default_1
    _FOLDED_CONST_constant_pad_nd_default_1 = rand_strided((6360, 3072), (3072, 1), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_117
    _FOLDED_CONST_permute_117 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_118
    _FOLDED_CONST_permute_118 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_119
    _FOLDED_CONST_permute_119 = rand_strided((3072, 1536), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_120
    _FOLDED_CONST_permute_120 = rand_strided((1536, 3072), (1, 1536), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_121
    _FOLDED_CONST_permute_121 = rand_strided((3072, 512), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_122
    _FOLDED_CONST_permute_122 = rand_strided((512, 3072), (1, 512), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_123
    _FOLDED_CONST_permute_123 = rand_strided((3072, 512), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_124
    _FOLDED_CONST_permute_124 = rand_strided((512, 3072), (1, 512), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_125
    _FOLDED_CONST_permute_125 = rand_strided((3072, 3072), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_126
    _FOLDED_CONST_permute_126 = rand_strided((3072, 512), (1, 3072), device='cuda:0', dtype=torch.float16)
    global _FOLDED_CONST_permute_127
    _FOLDED_CONST_permute_127 = rand_strided((512, 1), (1, 512), device='cuda:0', dtype=torch.float16)
    arg606_1 = rand_strided((2048, 3890), (3890, 1), device='cuda:0', dtype=torch.float16)
    arg607_1 = rand_strided((2048, 2688), (2688, 1), device='cuda:0', dtype=torch.float16)
    arg608_1 = rand_strided((2048, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    arg609_1 = rand_strided((2048, 384), (384, 1), device='cuda:0', dtype=torch.float16)
    arg610_1 = rand_strided((2048, 17932), (17932, 1), device='cuda:0', dtype=torch.float16)
    arg611_1 = rand_strided((2048, 17584), (17584, 1), device='cuda:0', dtype=torch.float16)
    arg612_1 = rand_strided((2048, 30516), (30516, 1), device='cuda:0', dtype=torch.float16)
    arg613_1 = rand_strided((2048, 200, 64), (12800, 64, 1), device='cuda:0', dtype=torch.float16)
    arg614_1 = rand_strided((2048, 200, 64), (12800, 64, 1), device='cuda:0', dtype=torch.float16)
    fn = lambda: call([arg606_1, arg607_1, arg608_1, arg609_1, arg610_1, arg611_1, arg612_1, arg613_1, arg614_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
